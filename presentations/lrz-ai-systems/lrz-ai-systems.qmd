---
title: "LRZ AI Systems"
subtitle: "LLM Inference"
author: "Tomas Ruiz"
institute: "Computational Social Sciences - LMU"
format: revealjs
---

# Goals

1. See LRZ AI Systems in Action
2. Squeeze Performance out of Large GPUs

# Part I
::: {.subtitle}
LRZ AI Systems
:::

## LRZ AI Systems
- Cluster with over 160 GPUs ^[https://doku.lrz.de/1-general-description-and-resources-10746641.html]
- Free to Use for LMU & TUM Researchers
- Run code interactively or as batch jobs

## LRZ AI Systems - Overview
<!-- https://drive.google.com/file/d/15mDl0dYuTx-j2GOqr1Oof9F6H0CvzWmG/view?usp=drive_link -->
![LRZ AI Systems Overview](lrz-ai-systems.drawio.png)

# Live Coding

## SLURM Commands

We are now in the **login node**.

SLURM info: `sinfo`

![](imgs/command-sinfo.png)

## SLURM Commands

Get Worker Node (Allocate): `salloc`

![](imgs/command-salloc.png)

## SLURM Commands

Enter Worker Node: `srun`

We are now in the **worker node**, and have a GPU.

![](imgs/command-srun.png)

## Enroot Containers

However, we don't have `sudo`, so we can't install packages.
To get `sudo` we need to use NVIDIA `enroot` containers[^2].
We import a PyTorch docker image, and use it.

![](imgs/command-enroot-import.png)

[^2]: https://github.com/NVIDIA/enroot

## Enroot Containers

After `import` we have to `create` the container.

![](imgs/command-enroot-create.png)

## Enroot Containers

Go into the container: `enroot start`. Finally, we have a GPU, and `sudo`. 

![](imgs/command-enroot-start.png)

## Enroot Containers

Since we imported the PyTorch image, `torch` is installed and CUDA available.

![](imgs/python-torch-cuda-available.png)

# Part II
::: {.subtitle}
Using Large GPUs for LLMs
:::

## LLM Training vs Inference
- **Training:** Updating model parameters (e.g. fine-tuning).
   - Needs at least 3x more memory than inference.
- **Inference:** Generating predictions, text, etc. Focus Today.

## LLM Inference

Our GPUs don't have enough VRAM for large models.

- Our server GPU (RTX 3090) has 24GB VRAM
- Rule of Thumb: VRAM = 3 x Params in B

But fitting the model in VRAM is only the first step.

We also need high **throughput** to process ever larger datasets.

# Live Coding

## LLM Inference

We will compare two LLM runtimes. Code is online ^[https://github.com/tomasruizt/vllm-hf-comparison].

HuggingFace Transformers
```shell
pip install transformers
```
vLLM ^[https://github.com/vllm-project/vllm]

```shell
pip install vllm
```

## LLM Inference

What is the max GPU performance on paper?

![Roofline Model for Performance[^5]](imgs/roofline-model.png)

[^5]: https://en.wikipedia.org/wiki/Roofline_model