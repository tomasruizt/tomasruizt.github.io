---
title: "How Does Tiling Speed Up Matrix Multiplication?"
author:
  - name: Tomas Ruiz
    email: t.ruiz@lmu.de
    affiliations: Ludwig-Maximilians-Universität München
date: "2024-10-26"
categories: [Mathematics, GPUs]
format:
  html:
    toc: true
    code-line-numbers: true
editor:
    render-on-save: true
image: tiled-matrix-multiplication-squared.jpg
engine: jupyter
# draft: true
bibliography: refs.bib
code-annotations: below
citation: true
---

**Summary:** This post is about the concept of **tiling**, which is a technique used to reduce the number of memory accesses performed during matrix multiplication. We will see why this speeds up the matrix multiplication operation, not only on CPUs, but also in GPUs.

# Matrix Multiplication
Assume a matrix multiplication $AB = C$. For instructional purposes, we will assume that $A$ and $B$ are squared with size $n$. A simple formula for each element of $C$ is given by:

$$C_{ij} = \sum_{k=1}^n A_{ik} B_{kj}$$

Below is simple pseudocode for this operation. The total amount of floating-point operations (flops) is approximately $n^3$ (triple triple nested loop).

```python
for i in range(n):
    for j in range(n):
        for k in range(n):
            C[i][j] += A[i][k] * B[k][j]
```

The code is correct, but it is inefficient in terms of its memory access pattern. Let's see why.

# Compute Intensity
Besides the flops counts, the performance of an algorithm is also determined by the memory access pattern. Each operation requires fetching data from main memory into a fast cache, and then returning computed data to main memory. This roundtrip is time-consuming, and the cores can become idle waiting for the data to be fetched from memory. If so, the memory bandwidth becomes the bottleneck of the algorithm.

::: {.callout-tip}
# Remember
To speed up our computations, we need to minimize the number of memory transfers per flop. The ratio of flops to memory transfers is called the **compute intensity**.
:::

The compute intensity can tell us if our algorithm is constrained by the memory bandwidth or the compute power, each requiring a different strategy to speed up the algorithm. This analysis goes back to the Roofline model [@williams2009roofline].

## Naive Memory Access
In the naive matrix multiplication above, to compute each element $C_{ij}$ we fetch $n$ elements from $A$ (a row) and $n$ elements from $B$ (a column). That makes $2n$ memory accesses. Then we perform the dot product of the row and the column, ... On this data we perform a multiplication and an addition $n$ times, which sums to $2n$ flops. This is what happens in line 5:

```{.python}
for k in range(n):
    C[i][j] += A[i][k] * B[k][j]
```

The ratio of flops ($2n$) to memory transfers from memory to cache ($2n$) is $\frac{2n}{2n} = 1$. essentially, we are doing one flop per unit of memory transfer. Can we do better? In this naive implementation, we are doing redundant memory transfers: For example, to compute $C_{11}$, and $C_{12}$, we fetch the same row of $A$ twice from main memory.

## Theoretically Optimal Memory Access
If our cache was large enough, we would transfer all elements of $A$ and $B$ into the cache at once ($2n^2$ transfers) and perform the full matrix multiplication ($2n^3$ flops). The ratio of flops to memory transfers would be $\frac{2n^3}{2n^2} = n$, suggesting that the larger the matrix, the more compute we do per unit of memory transfer.

::: {.callout-note}
# Example
If our matrix size is $n=10000$, the naive approach would do one flop per memory transfer, while the optimal approach would do 10000 flops per memory transfer. This is a 10000x speedup!
:::

## A Middle Ground: Tiling
In practice, we cannot fit the entire matrix into the cache. However, we can fit smaller squared submatrices of size $r$ into the cache. This is the idea behind **tiling**. We divide the matrices $A$ and $B$ into many small blocks, compute the block-wise matrix multiplication, and then sum the results.

To gain some intuition about how block matrix multiplication works, let's break down $AB = C$ by splitting each matrix into 2x2 blocks each. Each of these matrices (e.g. $A_{11}$) has size $n/2$.

$$
\begin{bmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{bmatrix}
\begin{bmatrix}
B_{11} & B_{12} \\
B_{21} & B_{22}
\end{bmatrix} = 
\begin{bmatrix}
C_{11} & C_{12} \\
C_{21} & C_{22}
\end{bmatrix}
$$

To compute an submatrix $C_{ij}$, we multiply the corresponding blocks of $A$ and $B$ and sum the results. For example:

$$
C_{11} = A_{11}B_{11} + A_{12}B_{21}
$$

In pseudocode this translates to the code below.

```python
n_blocks = n // r
for i in range(n_blocks):
    for j in range(n_blocks):
        for k in range(n_blocks):
            C[i][j] += A[i][k] @ B[k][j]
```

::: {.callout-warning}
Note that the entry `C[i][j]` is now a block matrix rather than a scalar, and the `@` operator denotes block matrix multiplication, rather than scalar multiplication.
:::

Line 5 of the code above is loading blocks of size $r^2$ from $A$ and $B$ into the cache. This is a total of $2r^2$ memory transfers. Then, we perform the block matrix multiplication, which requires $2r^3$ flops. The ratio of flops to memory transfers is $\frac{2r^3}{2r^2} = r$. This is a significant improvement over the naive approach, with a ratio of 1 flop per memory transfer.

::: {.callout-note}
# Example
If we use a block size of $r=100$, the naive approach would do one flop per memory transfer, while the tiling approach would do 100 flops per memory transfer. This is a 100x speedup!
:::

In the table below, we compare the performance of the different methods. Note that for the tiled method, the flops and memory transfers are measured per block, rather than per matrix.

| Method | Flops | Memory Transfers | Flops/Memory Transfer |
|----------|-------|-----------------|------------------------|
| Naive    | $2n^3$ | $2n \cdot n^2$ | $\frac{2n^3}{2n \cdot n^2} = 1$ |
| Tiling   | $2r^3$ | $2r^2$          | $\frac{r^3}{r^2} = r$   |
| Theoretical Optimal | $2n^3$ | $2n^2$          | $\frac{n^3}{n^2} = n$   |

# Tiling on GPUs

GPUs have a large number of cores, which can quickly process the data arriving from memory. Therefore, the pressure on the memory bandwidth is higher on GPUs. To achieve maximum performance in matrix multiplication on GPUs, it's necessary to use the tiling technique to reduce the number of memory transfers.

The book by [@kirk2016programming] has a great explanation about implementing tiling on GPUs using CUDA C. What is important to know is that we manage the reads and writes to the cache explicitly, using the concept of **shared memory**. In CUDA C, to define a shared memory array, we use the `__shared__` keyword.

The code block below shows a simplified version of tiled matrix multiplication using CUDA C. Have a look at the structure of the code, rather than the specific threading variables and indices. The variables `row` and `col` indicate what specific element of the matrix $C_{ij}$ the function is computing. Line 13 shows that we loop over blocks. Lines 15-16 load the data from main memory into the cache, and lines 22-24 perform the dot product needed, and accumulate the result. Finally in line 31, we write the result to global memory. The function `__syncthreads()` is a CUDA synchronization primitive to avoid a race condition between threads.

```{.c code-line-numbers="6-8"}
__global__ void matrixMul(float *A, float *B, float *C, int n) {
    // CUDA thread variables
    int row = threadIdx.x + blockIdx.x * BLOCK_SIZE;
    int col = threadIdx.y + blockIdx.y * BLOCK_SIZE;

    // Define shared memory arrays (cache)
    __shared__ float A_block[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float B_block[BLOCK_SIZE][BLOCK_SIZE];

    float C_value = 0.0;

    // Loop over blocks in the grid
    for (int block = 0; block < n / BLOCK_SIZE; block++) {
        // Load data from main memory into the cache
        A_block[threadIdx.x][threadIdx.y] = A[row * n + (block * BLOCK_SIZE + threadIdx.y)];
        B_block[threadIdx.x][threadIdx.y] = B[(block * BLOCK_SIZE + threadIdx.x) * n + col];

        // Ensure loading is complete before proceeding
        __syncthreads();

        // Perform the block matrix multiplication
        for (int k = 0; k < BLOCK_SIZE; k++) {
            C_value += A_block[threadIdx.x][k] * B_block[k][threadIdx.y];
        }

        // Synchronize threads before the next iteration
        __syncthreads();
    }

    // Write the result to global memory
    C[row * n + col] = C_value;
}
```
The CUDA C code looks much more complicated than the pseudocode, but it is just making explicit what happened under the hood in the CPU pseudocode: That we needed a cache to store the data and avoid redundant memory transfers.

# Conclusion
In this post, we saw why memory access patterns are important for the performance of matrix multiplication, calculated the theoretically optimal memory access, and saw how tiling can be used to speed up matrix multiplication on both CPUs and GPUs. We also saw an example implementation of tiling using CUDA C.

**Acknowledgement:** The author is funded by the Bavarian Research Institute for Digital Transformation (bidt), and employed by the Ludwig-Maximilians-Universität Munich, in the Computational Social Science group of Prof. Carsten Schwemmer.

# References
