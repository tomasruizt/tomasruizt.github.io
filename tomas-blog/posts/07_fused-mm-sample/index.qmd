---
title: "FMMS: A Fused Matmul-Sample Kernel"
subtitle: "An Efficient and Exact LLM Sampling Algorithm"
author:
  - name: Tomas Ruiz
    email: t.ruiz@lmu.de
    affiliations: Ludwig-Maximilians-Universität München
date: "2025-10-31"
categories: [GPUs, Triton, Mathematics]
format:
  html:
    toc: true
    code-line-numbers: true
editor:
    render-on-save: true
image: "imgs/Fused MM sample.png"
engine: jupyter
bibliography: refs.bib
code-annotations: below
citation: true
draft: true
---

<!-- TODO
- Consider removing A100 from tables and GPU tabsets (was it removed from README?)
- Add Qwen3-8B vLLM results once available
- Verify B300 outlier in top_k_top_p comparison (identical specs to B200 but 1.5-1.7x vs 1.1-1.3x)
- Update date before publishing
- Make FMMS visually distinct from baselines in barplots (e.g. different color palette) so it's obvious which one is my algorithm
- Introduce a "Learnings" section and move content there (e.g. swizzling bug, profiling insights, NCU tips)
- Lead with strong benchmark results up front. Separate that from the longer story motivating the kernel (Background, Overview, Tiling the Computation, Incremental Sampling, Gumbel-Max Trick). Readers should see the payoff before the explanation.
- Improve or remove the absolute performance plots (batch-scaling). They're hard to read and the point is unclear. The absolute runtime table also doesn't communicate much on its own. Needs better storytelling or should be cut in favor of the relative performance plots which tell a clearer story.
- The method/explanation sections might be less informative than in the README. Compare and check.
- Table columns seem sometimes too wide. See if they can be made narrower with markdown/Quarto styling.
- The text says compute-bound starts at N=128, but roofline plots show the ridge at 153 to 295 depending on GPU. Reconcile the discussion with the actual per-GPU ridge points.
- At low batch sizes FMMS reaches ~90% of peak memory bandwidth (H200, H100). Deserves a mention and explanation of what 100% would mean: load weights + hidden states from HBM, write back only the samples (a few ints) to GMEM. Almost no wasted traffic.
- The roofline plot must be larger to be readable (currently width="60%").
- The memory throughput plot says "Inference batch size" on the x-axis. Harmonize to "Batch Size" everywhere across all plots.
- Add discussion on the vLLM speedup plot's large per-run variability (the scattered dots), especially at higher concurrency levels.
- The torch.multinomial bf16 subtlety might fit better in the Learnings section than in Correctness.
- A side-by-side diagram comparing FMMS to the PyTorch baseline would be nice (show the fused single-pass vs the unfused matmul-write-read-sample pipeline).
-->

## Background

The paper _Cut Your Losses in Large-Vocabulary Language Models_ by @wijmans2025cut presents a method to massively reduce the VRAM footprint of training LLMs.
The key idea is to avoid materializing the full logits during the loss computation.
Instead, they compute the loss incrementally on SRAM.
They report reducing the memory footprint computing the cross-entropy loss for Gemma 2 (2B) from 24GB to 1MB on a batch of size 8,192.

This made me wonder: 
Can we use this technique to reduce the VRAM footprint of inference on LLMs?
There is an important difference between training and inference.
During training, we never sample from the distribution given by the logits.
Instead, a method called teacher-forcing is used:
The logits are compared to the ground truth and the loss is computed by the cross-entropy loss function.
In constrast, during inference, the logits must be converted into a distribution, e.g. using the softmax function,
and then sampled from.

The kernels in [Liger Kernel](https://github.com/linkedin/Liger-Kernel) by @hsu2025ligerkernelefficienttriton are also aimed at training-time, and include no sampling primitives.
The [FlashInfer](https://github.com/flashinfer-ai/flashinfer) library by @ye2025flashinfer is aimed at inference, but all the sampling kernels take `logits` or `probs` as input, which means that you still need to materialize full logits in VRAM before calling them. Below are two examples of the FlashInfer [sampling API](https://docs.flashinfer.ai/api/sampling.html) for reference.

```python
# Sampling method taking probs
sampling_from_probs(probs: Tensor, ...) -> Tensor

# Sampling method taking logits
top_k_top_p_sampling_from_logits(logits: Tensor, ...) -> Tensor
```

While FlashInfer is the backbone of inference engines like vLLM [@DBLP:conf/sosp/KwonLZ0ZY0ZS23], or SGLang [@zheng2024sglang],
it still assumed that logits must be materialized in VRAM.
But I was looking for a method that incrementally computed the logits and sampled from them in a single pass.
I knew that incrementally computing the softmax is possible, as described by @milakov2018onlinenormalizercalculationsoftmax (_online-softmax_).
However, they still materialize the full probabilities in VRAM.
The online-softmax method is also used in _FlashAttention_ [@dao2022flashattention], but they never sample from the probabilities,
because they are used in attention, rather than in the language modeling (LM) head.
All the building blocks exist (online-softmax, incremental logit computation), but have never been put together in a single method for efficient sampling. I set out to implement this myself.

::: {.callout-note}
# Note
I later found the paper **FlashSample**, which presents a conceptually similar algorithm [@qin2025flashsampling].
The paper was submitted to ICLR 25, but it was rejected due to a weak justification and experimental evidence.
In this post I present comprehensive experimental results, compare the method against the best baselines for LLM sampling, and dive deep into the real bottlenecks of LLM sampling.
:::

## Overview
Let's restate the goals:

* We want to sample from the probability determined by the logits of the LM head
* We want to avoid materializing the full logits in VRAM

This means that the inputs of the kernel are the hidden states $H$ of the last hidden layer and last position, and the weights $W$ of the LM head.
Multiplying them yield the logits $L$, from which the probabilities $P$ are computed using the softmax. The sizes are as follows:

* $W$ has shape $(V, d)$, where $V$ is the vocabulary size and $d$ is the hidden dimension
* $H$ has shape $(d, N)$, where $N$ is the number of hidden states. E.g. during batched inference, $N$ is the batch size.
* $L$ has shape $(V, N)$
* $P$ has shape $(V, N)$

Generally speaking, the largest among these is the vocabulary size $V$, which can range up to 256k, followed by the hidden dimension $d$, which be around 8k, and finally the batch size $N$, which can be 256 in batched inference.

Below is a simple PyTorch function that implements the sampling by computing the logits and probabilities.
This is just to get an idea of the steps involved, but we want to avoid computing the logits in full.
For simplicity, I'm sampling a single sample, and not applying any temperature scaling.

```python
def sample(
    weights: torch.Tensor,       # [V, d]
    hidden_states: torch.Tensor, # [N, d]
):
    logits = hidden_states @ weights.T  # [N, V]
    probs = logits.softmax(dim=-1)      # [N, V]
    samples = torch.multinomial(probs, num_samples=1)
    return samples  # [N, 1]
```

## Tiling the Computation
To break down the computation into an incremental fashion, I used Triton, which supports tiling the computation.
The logits are the result of a matrix multiplication, which can be tiled in across the $V$, $d$, and $H$ dimensions as shown in @fig-tiled-mm.
The tiles of $W$ have shape $(T_V, T_d)$ while the tiles of $H$ have shape $(T_d, T_N)$.
The resulting tiles of the logits have shape $(T_V, T_N)$.

![Tiling the computation of the logits.](imgs/Fused%20MM%20sample.png){width="80%" text-align="left" fig-alt="Tiling the computation of the logits" .lightbox #fig-tiled-mm}

## Incremental Sampling

The next part of the puzzle is how to incrementally sample based on a single tile of logits, rather than on the full vector of logits.
The problem is not strictly the softmax normalization factor, which can be computed incrementally (online-softmax).
Rather, the problem is that we don't know how likely each token is in comparison to the others until we have computed the last (bottom) tile of logits.

To understand this, consider the logits case below, where the vocabulary size $V=8$, the tile size $T_V=2$ (so, 8/2 = 4 tiles).
The horizontal lines separate the tiles.
Notice how its impossible to know and sample from the highest probability logit (50.0) until the last tile is computed.

$$
\begin{aligned}
L &= \begin{bmatrix}
1.0 \\
1.2 \\
\hline
2.0 \\
1.4 \\
\hline
1.0 \\
2.1 \\
\hline
1.1 \\
50.0
\end{bmatrix}
\end{aligned}
$$

However, there is one statistical trick to circumvent this problem: The Gumbel-Max trick.

## Gumbel-Max Trick

The Gumbel-max trick lets us sample from a categorical distribution using only logits, completely bypassing the softmax function. The key identity is:

$$
\text{argmax}_i \left( \log \pi_i + G_i \right) \sim \text{Categorical}(\pi_1, \ldots, \pi_V)
$$

where $G_i \sim \text{Gumbel}(0, 1)$ are i.i.d. Gumbel random variables, and $\pi_i$ are the (unnormalized) probabilities.
Since logits are $\ell_i = \log \pi_i$ (up to a constant that cancels in the argmax), we can equivalently write:

$$
\text{sample} = \text{argmax}_i \left( \ell_i + G_i \right)
$$

A Gumbel(0,1) random variable can be generated from a uniform random variable $U \sim \text{Uniform}(0, 1)$ as $G = -\log(-\log(U))$.

### Why this is useful for tiled computation

The crucial property is that **argmax decomposes over tiles**. If we split the vocabulary into tiles $T_1, T_2, \ldots, T_K$, we can compute the local maximum in each tile independently:

$$
m_k = \max_{i \in T_k} \left( \ell_i + G_i \right), \quad a_k = \text{argmax}_{i \in T_k} \left( \ell_i + G_i \right)
$$

and then the global sample is simply:

$$
\text{sample} = a_{k^*} \quad \text{where} \quad k^* = \text{argmax}_k \, m_k
$$

Each tile only needs to output two values: its local max $m_k$ and the corresponding vocabulary index $a_k$. The full logit vector is never materialized -- each tile computes its logits, adds Gumbel noise, finds the local winner, and discards the rest. The final reduction over $K$ tiles is trivially cheap.

Compare this with the standard approach: softmax requires a full pass to compute the normalizer $Z = \sum_i e^{\ell_i}$, then another pass to compute probabilities, then sampling via inverse CDF. None of these operations decompose over tiles without cross-tile communication.

## Two-Stage Architecture

Putting the tiled matmul and the Gumbel-max trick together, the kernel has a two-stage design:

**Stage 1 (GPU kernel):** The Triton kernel launches a 2D grid of programs indexed by `(v, h)`, where `v` ranges over vocabulary tiles and `h` over batch tiles. Each program:

1. Computes its tile of logits via the tiled matmul: `logits_blk = W_tile @ H_tile.T`
2. Scales by temperature: `logits_blk /= temperature`
3. Generates Gumbel noise and adds it to the logits
4. Finds the local maximum and argmax over the vocabulary dimension
5. Writes the local max value and the corresponding global vocabulary index to main memory

Each program outputs just two values per hidden state: the local max and its index. The intermediate output is a tensor of shape `[num_V_tiles, N, num_samples]`.

**Stage 2 (host-side reduction):** A simple PyTorch reduction finds which tile had the global maximum for each hidden state, then gathers the corresponding vocabulary index:

```python
best_tiles = tile_maxs.argmax(dim=0)   # which tile won?
samples = tile_indices.gather(0, best_tiles)  # what index did it have?
```

This stage operates on `num_V_tiles` elements per hidden state (e.g. 512 tiles for V=128K with a tile size of 256) and is negligibly cheap compared to stage 1.

## Profiling

NVIDIA Nsight Compute (NCU) was invaluable for identifying optimization opportunities. The profiler provides detailed metrics on warp stalls, memory utilization, and cache behavior. Here are the key insights it surfaced:

**Warp stalls and memory access patterns.** The initial version showed significant warp stalls due to suboptimal memory access. NCU suggested looking for opportunities to improve memory access patterns, which led me to transpose the hidden states for coalesced reads. However, this change had only a marginal effect.

**Increasing tile size for data reuse.** A much better improvement was to increase the tile size along the batch dimension ($T_N$) from 4 to 32. This means each tile of weights is reused 8x more often to compute logits, directly improving the arithmetic intensity. This is a classic compute-vs-memory optimization: at small batch sizes the kernel is memory-bound, so making each weight load do more useful work is the primary lever.

**Shared memory bank conflicts.** The profiler also flagged bank conflicts when storing data to shared memory. In CUDA, this is relatively easy to fix by padding the shared memory array by +1 element per row, breaking the stride alignment. In Triton, we don't have direct control over shared memory layout, so I left this optimization on the table.

## Swizzling Bug

The Triton documentation for matrix multiplication suggests to swizzle the program ids to improve the data reuse. @fig-loading-blocks-in-grouped-order shows the difference between loading data blocks in row-major order vs loading them in a "grouped" order.
The expected benefit of this optimization is a "more than 10%" speedup in the matmul performance. 
The reason is that tiles of data are not loaded from VRAM but rather from the faster L2 cache.

![Loading Blocks in Grouped Order. Source: [Triton Docs](https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html#l2-cache-optimizations)](imgs/grouped_vs_row_major_ordering.png){width="80%" text-align="left" fig-alt="Loading Blocks in Grouped Order" .lightbox #fig-loading-blocks-in-grouped-order}

I implemented this optimization with the existing Triton function `triton.language.swizzle_2d` ([docs](https://triton-lang.org/main/python-api/generated/triton.language.swizzle2d.html)), and saw speed increase massively. The Nsight Compute profiler showed that the L2 cache utilization increased from 50% to 90%, a steep jump.

However, I had made a mistake and passed wrong arguments to `swizzle_2d`.
This resulted in the same tiles being reloaded again and again, destroying the correctness of the computation, and explaning the excessive L2 cache utilization.
This bug was nevertheless difficult to catch, because the output of the kernel is a stochastic sample, rather than a deterministic result.
What made me suspicious was that my kernel, which includes a matmul, was 2x faster than the cuBLAS matmul, which should be close to optimal.
The performance was _too good to be true_, so I dug deeper and found the bug.

::: {.callout-note}
If you don't understand why your algorithm is running faster than expected, something might be wrong with the code.
:::

## Performance

I compare the performance of FMMS against strong baselines in LLM sampling. All benchmarks use the "large" configuration (V=128,256, d=8,192), representative of Llama 3 70B.

### Baselines

Both FlashInfer kernels require the full logits, so I include in their runtime the matmul to compute the logits. This is a fair comparison because FMMS also computes a matmul -- the logits are just never written to VRAM.

(1) **Naive PyTorch Compiled**: torch compiled matmul + softmax + `torch.multinomial`.
(2) **FlashInfer `top_k_top_p_sampling_from_logits`**: A dual-pivot rejection sampling kernel used in vLLM for top-k/top-p sampling.
(3) **FlashInfer `sampling_from_logits`**: FlashInfer's fastest sampling kernel. It also uses the Gumbel-max trick internally, but operates on pre-materialized logits.

### H100 Results

@fig-batch-scaling shows execution time (left) and throughput in samples/ms (right) across batch sizes.
FMMS (blue) is the fastest method up to batch size 64, after which `sampling_from_logits` (red) takes the lead.

::: {.panel-tabset group="gpu"}
# B300
![Batch scaling on B300 (V=128,256, d=8,192).](imgs/batch-scaling-large-b300.png){width="100%" #fig-batch-scaling}

# B200
![Batch scaling on B200.](imgs/batch-scaling-large-b200.png){width="100%"}

# H200
![Batch scaling on H200.](imgs/batch-scaling-large-h200.png){width="100%"}

# H100
![Batch scaling on H100.](imgs/batch-scaling-large-h100.png){width="100%"}

# A100-80GB
![Batch scaling on A100-80GB.](imgs/batch-scaling-large-a100.png){width="100%"}
:::

The following table shows absolute execution times (in milliseconds) on an H100 GPU.

| Method / Batch Size                         | 1     | 2     | 4     | 8     | 16    | 32    | 64    | 128   | 256   |
| ------------------------------------------- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| **FMMS (Triton)**                           | 0.71 | 0.71 | 0.72 | 0.72 | 0.73 | 0.74 | 0.78 | 0.96 | 1.55 |
| Naive PyTorch Compiled                      | 0.86 | 0.86 | 0.86 | 0.87 | 0.88 | 0.90 | 0.99 | 1.12 | 1.40 |
| FlashInfer `top_k_top_p`                    | 0.85 | 0.86 | 0.86 | 0.86 | 0.87 | 0.88 | 0.93 | 0.99 | 1.27 |
| FlashInfer `sampling_from_logits`           | 0.80 | 0.80 | 0.80 | 0.81 | 0.81 | 0.82 | 0.86 | 0.90 | 1.09 |

*All benchmarks: PyTorch 2.10.0, CUDA 13.0, run on Modal. Data as of 2026-02-11.*

### Relative Performance Across GPUs

The relative speedup tables show `baseline_time / fmms_time`, so values > 1.0 mean FMMS is faster.

:::: {.panel-tabset group="baseline"}
# PyTorch Compiled

This is the sampling path used in vLLM when top-k and top-p are unset (torch compiled matmul + softmax + `torch.multinomial`).

::: {.panel-tabset group="gpu-pytorch"}
## B300
![Relative performance vs PyTorch Compiled on B300.](imgs/relative-perf-vs-pytorch-b300.png){width="80%" #fig-relative-perf-vs-pytorch}

## B200
![Relative performance vs PyTorch Compiled on B200.](imgs/relative-perf-vs-pytorch-b200.png){width="80%"}

## H200
![Relative performance vs PyTorch Compiled on H200.](imgs/relative-perf-vs-pytorch-h200.png){width="80%"}

## H100
![Relative performance vs PyTorch Compiled on H100.](imgs/relative-perf-vs-pytorch-h100.png){width="80%"}

## A100-80GB
![Relative performance vs PyTorch Compiled on A100-80GB.](imgs/relative-perf-vs-pytorch-a100-80gb.png){width="80%"}
:::

| GPU / Batch Size | 1    | 2    | 4    | 8    | 16   | 32   | 64   | 128  | 256  |
| ---------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| B300             | 1.34 | 1.31 | 1.26 | 1.25 | 1.27 | 1.27 | 1.26 | 0.97 | 0.71 |
| B200             | 1.37 | 1.31 | 1.27 | 1.25 | 1.26 | 1.26 | 1.25 | 0.97 | 0.71 |
| H200             | 1.23 | 1.21 | 1.19 | 1.20 | 1.20 | 1.24 | 1.27 | 0.94 | 0.78 |
| H100             | 1.22 | 1.21 | 1.20 | 1.20 | 1.20 | 1.21 | 1.27 | 1.17 | 0.90 |
| A100-80GB        | 1.20 | 1.11 | 1.09 | 1.10 | 1.12 | 1.20 | 1.27 | 0.98 | 0.84 |

FMMS is faster across all GPUs and all batch sizes from 1 to 64, with speedups of 9-37%.
H100 retains its advantage at larger batch sizes than other GPUs: at N=128, it still achieves 1.17x, while other GPUs drop to ~1.0 or below.
This is likely because H100 has the highest ops:byte ratio (295), keeping the matmul memory-bound longer.

# FlashInfer `top_k_top_p`

This is the FlashInfer sampling function used in vLLM when top-k or top-p is set.
I set top-k=-1 and top-p=1.0 to disable filtering and compare pure sampling runtimes.

::: {.panel-tabset group="gpu-flashinfer"}
## B300
![Relative performance vs FlashInfer baselines on B300.](imgs/relative-perf-vs-flashinfer-b300.png){width="80%" #fig-relative-perf-vs-flashinfer}

## B200
![Relative performance vs FlashInfer baselines on B200.](imgs/relative-perf-vs-flashinfer-b200.png){width="80%"}

## H200
![Relative performance vs FlashInfer baselines on H200.](imgs/relative-perf-vs-flashinfer-h200.png){width="80%"}

## H100
![Relative performance vs FlashInfer baselines on H100.](imgs/relative-perf-vs-flashinfer-h100.png){width="80%"}

## A100-80GB
![Relative performance vs FlashInfer baselines on A100-80GB.](imgs/relative-perf-vs-flashinfer-a100-80gb.png){width="80%"}
:::

| GPU / Batch Size | 1    | 2    | 4    | 8    | 16   | 32   | 64   | 128  | 256  |
| ---------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| B300             | 1.63 | 1.56 | 1.62 | 1.62 | 1.65 | 1.64 | 1.53 | 1.09 | 0.88 |
| B200             | 1.25 | 1.26 | 1.22 | 1.22 | 1.22 | 1.19 | 1.14 | 0.83 | 0.64 |
| H200             | 1.20 | 1.21 | 1.20 | 1.20 | 1.20 | 1.21 | 1.20 | 0.83 | 0.74 |
| H100             | 1.20 | 1.21 | 1.19 | 1.19 | 1.20 | 1.19 | 1.19 | 1.03 | 0.82 |
| A100-80GB        | 1.21 | 1.10 | 1.10 | 1.10 | 1.12 | 1.18 | 1.18 | 0.92 | 0.79 |

FMMS is up to **1.65x faster** (B300, N=16).
Across typical decode batch sizes (1 to 64), FMMS is 10-65% faster.
B300 is a striking outlier: 1.5-1.7x speedups while A100-B200 show 1.1-1.3x.
B200 and B300 have identical bandwidth and compute specs, so the cause of this gap is unclear.

# FlashInfer `sampling_from_logits`

`sampling_from_logits` is not used in vLLM, but it is the fastest FlashInfer sampling function benchmarked.
It also uses a Gumbel-max trick internally, but operates on pre-materialized logits.

::: {.panel-tabset group="gpu-flashinfer2"}
## B300
![Relative performance vs FlashInfer baselines on B300.](imgs/relative-perf-vs-flashinfer-b300.png){width="80%"}

## B200
![Relative performance vs FlashInfer baselines on B200.](imgs/relative-perf-vs-flashinfer-b200.png){width="80%"}

## H200
![Relative performance vs FlashInfer baselines on H200.](imgs/relative-perf-vs-flashinfer-h200.png){width="80%"}

## H100
![Relative performance vs FlashInfer baselines on H100.](imgs/relative-perf-vs-flashinfer-h100.png){width="80%"}

## A100-80GB
![Relative performance vs FlashInfer baselines on A100-80GB.](imgs/relative-perf-vs-flashinfer-a100-80gb.png){width="80%"}
:::

| GPU / Batch Size | 1    | 2    | 4    | 8    | 16   | 32   | 64   | 128  | 256  |
| ---------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| B300             | 1.14 | 1.14 | 1.08 | 1.08 | 1.07 | 1.07 | 1.03 | 0.75 | 0.55 |
| B200             | 1.15 | 1.15 | 1.08 | 1.07 | 1.06 | 1.05 | 1.02 | 0.74 | 0.57 |
| H200             | 1.10 | 1.10 | 1.08 | 1.08 | 1.08 | 1.09 | 1.07 | 0.72 | 0.62 |
| H100             | 1.13 | 1.13 | 1.12 | 1.12 | 1.11 | 1.10 | 1.10 | 0.93 | 0.70 |
| A100-80GB        | 1.12 | 1.04 | 1.03 | 1.04 | 1.05 | 1.11 | 1.10 | 0.81 | 0.71 |

A clear pattern emerges: **FMMS is 3-15% faster at small batch sizes (N=1 to 64) but regresses at large batch sizes (N=128+)**.
The crossover point varies by GPU, but is consistently in the N=64--128 range.
::::

### Why the Crossover? Arithmetic Intensity

The explanation lies in the **arithmetic intensity** of the matmul. The matmul $W_{[V, d]} \times H_{[N, d]}^T$ has:

$$
\text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{Bytes}} = \frac{2 \cdot V \cdot d \cdot N}{2 \cdot V \cdot d} = N
$$

The weight matrix $W$ dominates the memory traffic ($V \cdot d$ elements, each read once), and the hidden states are negligible when $N \ll V$. So the arithmetic intensity is simply $N$: each byte of weights loaded produces $N$ multiply-adds.

A GPU becomes compute-bound when the arithmetic intensity exceeds its **ops:byte ratio** (peak compute / peak memory bandwidth). For the H100:

$$
\text{ops:byte} = \frac{989 \text{ TFLOP/s (BF16)}}{3.35 \text{ TB/s (HBM3)}} \approx 295
$$

The roofline plot in @fig-roofline confirms this: all methods track the memory-bound slope up to N=64, then curve toward the compute ceiling.
Each point is labeled with its batch size.
FMMS sits slightly above the baselines in the memory-bound region, reflecting its bandwidth savings from avoiding the logits round-trip.

::: {.panel-tabset group="gpu"}
# B300
![Roofline model on B300 (V=128,256, d=8,192). The ridge point marks the transition from memory-bound to compute-bound.](imgs/roofline-large-b300.png){width="60%" #fig-roofline}

# B200
![Roofline model on B200.](imgs/roofline-large-b200.png){width="60%"}

# H200
![Roofline model on H200.](imgs/roofline-large-h200.png){width="60%"}

# H100
![Roofline model on H100.](imgs/roofline-large-h100.png){width="60%"}

# A100-80GB
![Roofline model on A100-80GB.](imgs/roofline-large-a100.png){width="60%"}
:::

@fig-memory-throughput shows the achieved memory throughput (GB/s) as a function of batch size.
In the memory-bound region (N=1 to 64), FMMS consistently achieves higher throughput than all baselines, reaching ~90% of peak HBM bandwidth on H100.
This is the direct mechanism behind the speedups: by fusing the matmul and sampling, FMMS uses the available memory bandwidth more efficiently.

::: {.panel-tabset group="gpu"}
# B300
![Memory throughput on B300 (V=128,256, d=8,192). The dashed line shows peak HBM bandwidth.](imgs/memory-throughput-large-b300.png){width="60%" #fig-memory-throughput}

# B200
![Memory throughput on B200.](imgs/memory-throughput-large-b200.png){width="60%"}

# H200
![Memory throughput on H200.](imgs/memory-throughput-large-h200.png){width="60%"}

# H100
![Memory throughput on H100.](imgs/memory-throughput-large-h100.png){width="60%"}

# A100-80GB
![Memory throughput on A100-80GB.](imgs/memory-throughput-large-a100.png){width="60%"}
:::

| Batch size $N$ | Arithmetic intensity | Regime on H100 |
| -------------- | -------------------- | --------------- |
| 1              | 1                    | Deeply memory-bound (295x below threshold) |
| 32             | 32                   | Memory-bound (9x below) |
| 64             | 64                   | Memory-bound (5x below) |
| 295            | 295                  | Crossover to compute-bound |

**When memory-bound (N < ~128)**, the kernel's throughput is determined by how fast it reads the weight matrix from HBM. FMMS wins here because it reads the weights **once** and performs both the matmul and sampling in-kernel. The unfused baselines read the weights for the matmul, write logits to HBM, then read them back for sampling -- this extra round-trip wastes bandwidth on the bottleneck path.

**When compute-bound (N > ~128)**, the matmul dominates runtime and the cost of the extra HBM traffic for logits becomes negligible relative to the computation. The baselines can use highly optimized matmul implementations (cuBLAS via `torch.compile`), while FMMS carries the overhead of Gumbel noise generation and the max-reduction fused into the same kernel.

## End-to-End vLLM Benchmarks

The kernel microbenchmarks above measure the FMMS kernel in isolation.
To measure end-to-end impact, I integrated FMMS into vLLM ([branch](https://github.com/tomasruizt/vllm/tree/feature/fmms-sampler)) and benchmarked median time per output token (TPOT) across concurrency levels using `vllm bench sweep`.
Each configuration is run 5 times; the table shows the median TPOT (lower is better) and the TPOT reduction.
All results are on a B200 GPU with PyTorch 2.10.0 and CUDA 13.0, run on Modal.

![Median TPOT vs concurrency for both models. The gap between baseline and FMMS is clearly visible on Qwen3-1.7B (left) and smaller but consistent on gpt-oss-120b (right).](imgs/tpot_vs_concurrency.png){width="100%" #fig-tpot-vs-concurrency}

![TPOT speedup (%) per run. Each light dot is one of 5 runs; the line connects the medians. Qwen3-1.7B shows 10-19% speedups across all concurrency levels.](imgs/speedup_vs_concurrency.png){width="100%" #fig-speedup-vs-concurrency}

::: {.panel-tabset group="model"}
# Qwen3-1.7B (V=151,936, d=2,048)

| Concurrency | Baseline (ms) | FMMS Triton (ms) | TPOT Reduction |
| ----------- | ------------- | ---------------- | -------------- |
| 1           | 2.25          | 2.00             | -10.8%         |
| 2           | 2.14          | 1.87             | -12.6%         |
| 4           | 2.15          | 1.84             | -14.5%         |
| 8           | 2.19          | 1.86             | -15.2%         |
| 16          | 2.24          | 1.85             | -17.7%         |
| 32          | 2.52          | 2.28             | -10.0%         |
| 64          | 3.47          | 2.82             | -18.7%         |
| 128         | 4.21          | 4.05             | -4.4%          |
| 256         | 15.43         | 14.28            | -7.6%          |

FMMS reduces TPOT by 10-19% across all concurrency levels on Qwen3-1.7B.
The small hidden dimension (d=2,048) makes the LM head matmul strongly memory-bound, so fusion has a large impact.
Peak improvement is -18.7% at concurrency 64.

# gpt-oss-120b (V=201,088, d=2,880)

| Concurrency | Baseline (ms) | FMMS Triton (ms) | TPOT Reduction |
| ----------- | ------------- | ---------------- | -------------- |
| 1           | 3.45          | 3.29             | -4.3%          |
| 2           | 4.14          | 3.99             | -3.6%          |
| 4           | 5.28          | 5.11             | -3.1%          |
| 8           | 7.30          | 7.12             | -2.5%          |
| 16          | 9.94          | 9.77             | -1.8%          |
| 32          | 13.34         | 13.39            | +0.6%          |
| 64          | 17.80         | 17.50            | -1.7%          |
| 128         | 23.39         | 23.25            | -0.6%          |
| 256         | 27.43         | 27.06            | -1.4%          |

On gpt-oss-120b, FMMS reduces TPOT by 2-4% at low concurrency (1-8).
The improvement is smaller than Qwen3-1.7B because the LM head matmul is a smaller fraction of the total decode step in a 120B-parameter model.
:::

## Correctness

FMMS uses the Gumbel-max trick, which produces **exact** samples from the categorical distribution -- it is not an approximation. To verify this, I use a chi-squared goodness-of-fit test that compares 10,000 empirical samples against the theoretical softmax probabilities. The test is parametrized over multiple vocabulary sizes and batch sizes to catch tile-boundary edge cases.

One subtlety: `torch.multinomial` (used in the naive baseline) produces incorrect distributions when given bfloat16 probabilities. The internal CDF accumulation loses precision with bfloat16's 7-bit mantissa. The fix is to upcast to float32 before softmax: `probs = (logits.float() / temperature).softmax(dim=-1)`. This is a real bug that affects production code, and PyTorch's documentation does not mention this precision requirement.

As an end-to-end quality check, I integrated FMMS into vLLM and ran the GSM8K benchmark (1,319 questions, 0-shot CoT) via [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) on Qwen3-1.7B, with answers graded by an LLM judge.

| Variant                 | Accuracy | 95% CI         |
| ----------------------- | -------- | -------------- |
| Baseline (vLLM default) | 89.6%    | [87.9%, 91.2%] |
| FMMS Triton             | 89.4%    | [87.7%, 91.0%] |

The difference is +0.2 percentage points (p=0.776, paired bootstrap), not statistically significant, confirming that FMMS does not degrade model accuracy.

## Conclusion

FMMS demonstrates that fusing matrix multiplication with sampling via the Gumbel-max trick is both correct and practical.
In kernel microbenchmarks, FMMS is 3-15% faster than the fastest baseline (FlashInfer `sampling_from_logits`) and up to 37% faster than PyTorch compiled sampling at the batch sizes that matter most for LLM decode (N=1 to 64), where the operation is memory-bound and eliminating the extra HBM round-trip for logits directly improves throughput.

These gains translate to real-world speedups: integrated into vLLM, FMMS reduces median TPOT by 10-19% on Qwen3-1.7B and 2-4% on gpt-oss-120b, with no degradation in model accuracy (verified on GSM8K).

The approach has clear limitations: at large batch sizes where the matmul becomes compute-bound, baseline approaches with optimized cuBLAS matmuls are faster.
This is a fundamental trade-off, as fusion saves memory bandwidth at the cost of matmul efficiency.

The code is open-source and available at [github.com/tomasruizt/fused-mm-sample](https://github.com/tomasruizt/fused-mm-sample).

# References
