---
title: "FMS: A Fused Matmul-Sample Kernel"
subtitle: "An Efficient and Exact LLM Sampling Algorithm"
author:
  - name: Tomas Ruiz
    email: t.ruiz@lmu.de
    affiliations: Ludwig-Maximilians-Universität München
date: "2025-10-31"
categories: [GPUs, Triton, Mathematics]
format:
  html:
    toc: true
    code-line-numbers: true
editor:
    render-on-save: true
image: "imgs/Fused MM sample.png"
engine: jupyter
bibliography: refs.bib
code-annotations: below
citation: true
draft: true
---

## Background

The paper _Cut Your Losses in Large-Vocabulary Language Models_ by @wijmans2025cut presents a method to massively reduce the VRAM footprint of training LLMs.
The key idea is to avoid materializing the full logits during the loss computation.
Instead, they compute the loss incrementally on SRAM.
They report reducing the memory footprint computing the cross-entropy loss for Gemma 2 (2B) from 24GB to 1MB on a batch of size 8,192.

This made me wonder: 
Can we use this technique to reduce the VRAM footprint of inference on LLMs?
There is an important difference between training and inference.
During training, we never sample from the distribution given by the logits.
Instead, a method called teacher-forcing is used:
The logits are compared to the ground truth and the loss is computed by the cross-entropy loss function.
In constrast, during inference, the logits must be converted into a distribution, e.g. using the softmax function,
and then sampled from.

The kernels in [Liger Kernel](https://github.com/linkedin/Liger-Kernel) by @hsu2025ligerkernelefficienttriton are also aimed at training-time, and include no sampling primitives.
The [FlashInfer](https://github.com/flashinfer-ai/flashinfer) library by @ye2025flashinfer is aimed at inference, but all the sampling kernels take `logits` or `probs` as input, which means that you still need to materialize full logits in VRAM before calling them. Below are two examples of the FlashInfer [sampling API](https://docs.flashinfer.ai/api/sampling.html) for reference.

```python
# Sampling method taking probs
sampling_from_probs(probs: Tensor, ...) -> Tensor

# Sampling method taking logits
top_k_top_p_sampling_from_logits(logits: Tensor, ...) -> Tensor
```

While FlashInfer is the backbone of inference engines like vLLM [@DBLP:conf/sosp/KwonLZ0ZY0ZS23], or SGLang [@zheng2024sglang],
it still assumed that logits must be materialized in VRAM.
But I was looking for a method that incrementally computed the logits and sampled from them in a single pass.
I knew that incrementally computing the softmax is possible, as described by @milakov2018onlinenormalizercalculationsoftmax (_online-softmax_).
However, they still materialize the full probabilities in VRAM.
The online-softmax method is also used in _FlashAttention_ [@dao2022flashattention], but they never sample from the probabilities,
because they are used in attention, rather than in the language modeling (LM) head.
All the building blocks exist (online-softmax, incremental logit computation), but have never been put together in a single method for efficient sampling. I set out to implement this myself.

::: {.callout-note}
# Note
I later found the paper **FlashSample**, which presents a conceptually similar algorithm [@qin2025flashsampling].
The paper was submitted to ICLR 25, but it was rejected due to a weak justification and experimental evidence.
In this post I present comprehensive experimental results, compare the method against the best baselines for LLM sampling, and dive deep into the real bottlenecks of LLM sampling.
:::

## Overview
Let's restate the goals:

* We want to sample from the probability determined by the logits of the LM head
* We want to avoid materializing the full logits in VRAM

This means that the inputs of the kernel are the hidden states $H$ of the last hidden layer and last position, and the weights $W$ of the LM head.
Multiplying them yield the logits $L$, from which the probabilities $P$ are computed using the softmax. The sizes are as follows:

* $W$ has shape $(V, d)$, where $V$ is the vocabulary size and $d$ is the hidden dimension
* $H$ has shape $(d, N)$, where $N$ is the number of hidden states. E.g. during batched inference, $N$ is the batch size.
* $L$ has shape $(V, N)$
* $P$ has shape $(V, N)$

Generally speaking, the largest among these is the vocabulary size $V$, which can range up to 256k, followed by the hidden dimension $d$, which be around 8k, and finally the batch size $N$, which can be 256 in batched inference.

Below is a simple PyTorch function that implements the sampling by computing the logits and probabilities.
This is just to get an idea of the steps involved, but we want to avoid computing the logits in full.
For simplicity, I'm sampling a single sample, and not applying any temperature scaling.

<!-- TODO: Update this code with the new transposed shapes -->
```python
def sample(
    weights: torch.Tensor,  # [V, d]
    hidden_states: torch.Tensor,  # [d, N]
):
    logits = weights @ hidden_states  # [V, N]
    probs = logits.softmax(dim=0)  # [V, N]
    samples = torch.multinomial(probs.T, num_samples=1)
    return samples # [N, 1]
```

## Tiling the Computation
To break down the computation into an incremental fashion, I used Triton, which supports tiling the computation.
The logits are the result of a matrix multiplication, which can be tiled in across the $V$, $d$, and $H$ dimensions as shown in @fig-tiled-mm.
The tiles of $W$ have shape $(T_V, T_d)$ while the tiles of $H$ have shape $(T_d, T_N)$.
The resulting tiles of the logits have shape $(T_V, T_N)$.

![Tiling the computation of the logits.](imgs/Fused%20MM%20sample.png){width="80%" text-align="left" fig-alt="Tiling the computation of the logits" .lightbox #fig-tiled-mm}

## Incremental Sampling

The next part of the puzzle is how to incrementally sample based on a single tile of logits, rather than on the full vector of logits.
The problem is not strictly the softmax normalization factor, which can be computed incrementally (online-softmax).
Rather, the problem is that we don't know how likely each token is in comparison to the others until we have computed the last (bottom) tile of logits.

To understand this, consider the logits case below, where the vocabulary size $V=8$, the tile size $T_V=2$ (so, 8/2 = 4 tiles).
The horizontal lines separate the tiles.
Notice how its impossible to know and sample from the highest probability logit (50.0) until the last tile is computed.

$$
\begin{aligned}
L &= \begin{bmatrix}
1.0 \\
1.2 \\
\hline
2.0 \\
1.4 \\
\hline
1.0 \\
2.1 \\
\hline
1.1 \\
50.0
\end{bmatrix}
\end{aligned}
$$

However, there is one statistical trick to circumvent this problem: The Gumbel-Max trick.

## Gumbel-Max Trick
This sampling method allows us to sample from a categorical distribution by only using logits, and avoiding the softmax function.

## Profiling

Nsight Compute can give you pointers about what to optimize.

E.g. in the baseline it a lot of warp stalls and recommended looking for opportunities to improve memory access patterns.
This lead me to transpose the hidden_states (but this had marginal effect)

A much better improvement was to increase the computational intensity by increasing the tile size of H from 4 to 32, meaning at each tile of the weights would be reused 8x as often to compute logits.

Finally, the profiler shows bank conflicts that occur when saving to the shared memory.
This is relatively easy to fix in CUDA by padding the shared memory array with +1, since
we directly control the shared memory size.
However, we don't have this control as easily in Triton, so I skipped this optimization.

## Swizzling Bug

The Triton documentation for matrix multiplication suggests to swizzle the program ids to improve the data reuse. @fig-loading-blocks-in-grouped-order shows the difference between loading data blocks in row-major order vs loading them in a "grouped" order.
The expected benefit of this optimization is a "more than 10%" speedup in the matmul performance. 
The reason is that tiles of data are not loaded from VRAM but rather from the faster L2 cache.

![Loading Blocks in Grouped Order. Source: [Triton Docs](https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html#l2-cache-optimizations)](imgs/grouped_vs_row_major_ordering.png){width="80%" text-align="left" fig-alt="Loading Blocks in Grouped Order" .lightbox #fig-loading-blocks-in-grouped-order}

I implemented this optimization with the existing Triton function `triton.language.swizzle_2d` ([docs](https://triton-lang.org/main/python-api/generated/triton.language.swizzle2d.html)), and saw speed increase massively. The Nsight Compute profiler showed that the L2 cache utilization increased from 50% to 90%, a steep jump.

However, I had made a mistake and passed wrong arguments to `swizzle_2d`.
This resulted in the same tiles being reloaded again and again, destroying the correctness of the computation, and explaning the excessive L2 cache utilization.
This bug was nevertheless difficult to catch, because the output of the kernel is a stochastic sample, rather than a deterministic result.
What made me suspicious was that my kernel, which includes a matmul, was 2x faster than the cuBLAS matmul, which should be close to optimal.
The performance was _too good to be true_, so I dug deeper and found the bug.

::: {.callout-note}
If you don't understand why your algorithm is running faster than expected, something might be wrong with the code.
:::

## Performance

I compare the performance of FMS against strong baselines in LLM sampling. These are: first, a PyTorch + torch.compile baseline. Second, two FlashInfer kernels that sample from the logits, rather than from normalized probabilities.

Both FlashInfer kernels require the full logits, so I include in their runtime the time required by the matmul to compute the logits.
This is a fair comparison because FMS must also perform a matmul to compute logits in SRAM. They are just not written back to VRAM.

In summary, the baselines are:

(1) Naive sampling implemented in PyTorch and compiled using `torch.compile`.
(2) **`flashinfer:top_k_top_p_sampling_from_logits`**, which uses a dual-pivot algorithm that leverages rejection sampling to sample from the top-k and top-p tokens. This kernel is used for fast sampling in vLLM.
(3) **`flashinfer:sample_from_logits`**. This kernel is the fastest FlashInfer kernel for sampling. It also uses Gumbel-Max trick to sample from the logits, but still materializes the full logits in VRAM.

# References
