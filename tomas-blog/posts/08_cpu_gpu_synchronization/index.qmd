---
title: "PyTorch and CPU-GPU Synchronizations"
subtitle: "Writing Fast PyTorch Code"
author:
  - name: Tomas Ruiz
    email: t.ruiz@lmu.de
    affiliations: Ludwig-Maximilians-Universität München
date: "2026-01-03"
categories: [GPUs, PyTorch, Triton]
format:
  html:
    toc: true
    code-line-numbers: true
editor:
    render-on-save: true
image: "imgs/example-code-do-print-annotated.jpg"
engine: jupyter
bibliography: refs.bib
code-annotations: below
citation: true
# draft: true
---
# Summary
CPU-GPU synchronizations (also knowns as _host-device_ synchronizations) are a subtle but important mechanism to write fast and efficient PyTorch programs.
The CPU-GPU synchronization is a blocking operation that prevents the CPU from scheduling new work (PyTorch ops) on the GPU.
The [PyTorch Tuning Guide](https://docs.pytorch.org/tutorials/recipes/recipes/tuning_guide.html#avoid-unnecessary-cpu-gpu-synchronization) makes some basic suggestions about how to avoid CPU-GPU synchronizations, e.g. by avoiding calls to `tensor.item()`, printing a device tensor, or calling `tensor.cpu()` / `tensor.cuda()`, but doesn't explain in depth what is happening under the hood.
Furthermore, there are other subtle ways to run into CPU-GPU synchronizations.
In this post I dive deeper into synchronizations and explain precisely how they slow down a PyTorch program.
I show how to use the NVIDIA **Nsight Systems** profiler, to identify and diagnose CPU-GPU synchronization issues.
Finally, I also show how to use the experimental PyTorch API `torch.cuda.set_sync_debug_mode` in **unit tests** to verify the absence of CPU-GPU synchronizations in production code.


![Profile showing CPU-GPU synchronization issues. A deep dive into the profile details is in @sec-profiling-analysis.](./imgs/example-code-do-print.png){#fig-profile-no-print}

# Background
To fully take advantage of the speed of a GPU, it must be kept busy (high GPU utilization).
This depends on the CPU scheduling enough work to keep the GPU busy. 
Typically, in a well-performing PyTorch program, the CPU schedules many instructions quickly, which are executed by the GPU.
The CPU is said to *run ahead* of the GPU, because it issues instructions without waiting for the previous ones to complete.
This is why PyTorch is said to be *asynchronous*.
If the CPU fails to schedule enough work, the GPU will sit idle waiting for work.

**The intuition:** As an analogy to understand CPU-GPU synchronizations, think of a restaurant where a chef (CPU) schedules all the steps (PyTorch ops) for a big dinner party in advance.
A CPU-GPU synchronization would be like the chef waiting to observe how a specific step in a specific dish turned out *during the dinner*, before scheduling the rest of the dishes and telling the cooks what to do.
Obviously, this latency in communication will lead to the cooks being idle, waiting for the chef to schedule the dishes. Instead, the chef should schedule the entire dinner plan in advance (*run ahead*).

# Example
Let's look at a concrete example.
Below we have a PyTorch progam that executes a `slow_operation()` on the GPU, followed by a `quick_operation()` on the GPU. 
The quick operation just counts the number of 1s in the result tensor.
The result of the quick operation is gathered in `results` tensor.
Both functions are executed in a loop, to simulate a long-running program like LLM inference, where the slow operation could be the LLM forward pass, and the quick operation could be bookkeeping with the sampled tokens^[This entire example is inspired by the vLLM inference loop].
To warm up the torch code, I run both the `slow_operation()` and `quick_operation()` once before the hot loop.
I annotated regions of code with `nvtx.annotate()`, to keep track of GPU and CPU runtime during profiling.

<script src="https://gist.github.com/tomasruizt/4281c61bfaead3e61211d052d3971c90.js?file=cpu_gpu_sync_example.py"></script>

# Profiling Analysis {#sec-profiling-analysis}

We can run the NVIDIA Nsight Systems profiler with the command below to produce the timeline shown in @fig-profile-no-print.
The script takes a flag `--do-print` to include a CPU-GPU synchronization in the quick operation: a `print`of a gpu tensor, which forces data back from the GPU to the CPU.

**Without** CPU-GPU synchronization:
```shell
nsys profile \
  -o profile-no-print.nsys-rep \
  python cpu_gpu_sync_example.py
```
**With** CPU-GPU synchronization:
```shell
nsys profile \
  -o profile-do-print.nsys-rep \
  python cpu_gpu_sync_example.py --do-print
```

The resulting profiles are shown below and annotated:

### Run With CPU-GPU Synchronization

![Profile With CPU-GPU Synchronization. ](./imgs/example-code-do-print-annotated.jpg){#fig-profile-no-print}

- We see **gaps** in the GPU utilization (first blue bar from top to bottom). These gaps indicate that the GPU has idle times and waits for work.
- We observe that the runtimes of the GPU and CPU are similarly long, as seen both timelines being equal in length in the horizontal axis. In a well-performing program, the CPU runtime should be shorter than the GPU runtime (CPU _runs ahead_ of the GPU).
- In the CPU ops timeline, we see long green bars (`cudaStreamSyncrhonize`), which mean that the CPU is **blocked** waiting for the GPU to return some data.
- On the GPU ops timeline we observe that the slow operations takes longer than the quick operation, which just means that the quick operation is the one blocking the CPU.


### Healthy Run

![Profile Without CPU-GPU Synchronization. ](./imgs/example-code-no-print.png){#fig-profile-do-print}

- We observe that the CPU **runs ahead** of the GPU, so it dispatches all the work quickly and finishes way before the GPU.
- The GPU utilization does not have any gaps. The blue line showing utilization is continuous.
- In the GPU ops timeline, the slow operation is slower than the quick operation.
- The full region `interleaved-code` runs faster than without the CPU-GPU synchronization (4.434 ms vs 4.827 ms).

# More Synchronization Triggers
Besides the code mentioned in the [PyTorch Tuning Guide](https://docs.pytorch.org/tutorials/recipes/recipes/tuning_guide.html#avoid-unnecessary-cpu-gpu-synchronization) there is another common pattern that triggers CPU-GPU synchronizations, namely **dynamic shapes**.
One example of dynamic shapes is boolean indexing `x = t[bool_mask]`, where `bool_mask` is a boolean tensor on GPU.
The size of the tensor `x` cannot be determined by the CPU alone, because it depends on the number of true values in `bool_mask`.
Therefore, the CPU needs to fetch data from the GPU to determine the amount of memory to allocate for `x`.
This creates a CPU-GPU synchronization.

Another example of this same problem is slicing `x = t[:gpu_index]`, where `gpu_index` is an scalar integer tensor on GPU.
Once again, the size of the tensor `x` cannot be determined by the CPU alone, because it depends on the value of `gpu_index`.

# Unit Testing for CPU-GPU Synchronizations

So we need to prevent syncrhonization in our code.
How can we do this without running a profiler on each code section we are interested in?
PyTorch provides an experimental feature that can raise warnings or errors when CPU-GPU synchronizations occur: The `torch.cuda.set_sync_debug_mode()` function [(link)](https://docs.pytorch.org/docs/stable/generated/torch.cuda.set_sync_debug_mode.html).
Since it raises errors immediately, it can be used in unit tests to verify that the code is free of CPU-GPU synchronizations. For example it can be used in a context manager:

```python
@contextmanager
def fail_if_gpu_cpu_synchronization(fail: bool):
    """Within this context, GPU-CPU synchronization raises an error if `fail` is True."""

    new_mode = 2 if fail else 0
    old_mode = torch.cuda.get_sync_debug_mode()
    torch.cuda.set_sync_debug_mode(new_mode)
    try:
        yield
    finally:
        torch.cuda.set_sync_debug_mode(old_mode)


x = torch.arange(10, device="cuda")
with fail_if_gpu_cpu_synchronization(fail=True):
    print(x)  # Raises an error
```

The context manager can be used to in a decorator as well, to decorate functions that should fail if they contain CPU-GPU synchronizations.
```python
def on_gpu_cpu_synchronization(fail: bool):
    """
    Wrap a function to raise an error on GPU-CPU synchronization if `fail` is True.
    """

    def decorator(fn):
        @functools.wraps(fn)
        def wrapper(*args, **kwargs):
            with fail_if_gpu_cpu_synchronization(fail):
                return fn(*args, **kwargs)

        return wrapper

    return decorator


@on_gpu_cpu_synchronization(fail=True)
def test_should_not_sync():
    x = torch.arange(10, device="cuda")
    y = x ** 2
```

Note that the decorator is used on test functions, meaning that the PyTorch API to raise synchronization errors is never applied to production code, but only to the test code.
This keeps this experimental feature isolated from prod code while still providing quick and valuable feedback on CPU-GPU synchronizations in prod code. 
I used this pattern to verify the absence of synchronizations in my Github Gist about [vLLM TokenGroup](https://gist.github.com/tomasruizt/4281c61bfaead3e61211d052d3971c90#file-test_tokengroup-py).
The mechanism itself of raising errors is also unit tested in the [test_helpers.py file](https://gist.github.com/tomasruizt/4281c61bfaead3e61211d052d3971c90#file-test_helpers-py) in the Github Gist.

::: {.callout-warning}
This PyTorch API is experimental as of today, and does not cover *all* CPU-GPU synchronizations. For example, it does not cover the `torch.distributed` and `torch.sparse` namespaces (see [docs](https://docs.pytorch.org/docs/stable/generated/torch.cuda.set_sync_debug_mode.html)).
:::

# Further References
For a deep dive into kernel benchmarking in practice, I recommend this [YouTube lecture](https://www.youtube.com/watch?v=CtrqBmYtSEk) by NVIDIA engineering manager [Georgii Evtushenko](https://www.linkedin.com/in/g-evtushenko/).

<!-- 
  - I learned a lot about synchronizing operations while developing the vLLM TokenGroup abstraction in this Github gist: https://gist.github.com/tomasruizt/4281c61bfaead3e61211d052d3971c90

  - `torch.tensor([1, 2, 3], device=”cuda”)` syncs, while `torch.tensor([1, 2, 3]).to(”cuda”, non_blocking=True)` does not.
  - Dynamic shapes sync. E.g. boolean indexing `t[bool_mask]` or `t[:gpu_index]` because torch needs to know how large the tensor is to issue the mem-alloc.

  - Scalar ops can be very easy to write in Triton, not only vectorized programs. In particular, multiple torch ops can be fused into a single Triton op. This is useful to reduce the dispatching overhead on the CPU-side, and the kernel launch overhead on the GPU-side.
-->

# References
