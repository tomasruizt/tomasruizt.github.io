---
title: "vLLM Speculative Decoding"
author:
  - name: Tomas Ruiz
    email: t.ruiz@lmu.de
    affiliations: Ludwig-Maximilians-Universität München
# date: "2024-12-23"
# categories: [Mathematics, GPUs]
format:
  html:
    toc: true
    code-line-numbers: true
editor:
    render-on-save: true
# image: tiled-matrix-multiplication-squared.jpg
engine: jupyter
bibliography: refs.bib
code-annotations: below
citation: true
# draft: true
---
I recently contributed speculative decoding with draft models to vLLM V1 [PR #24322](https://github.com/vllm-project/vllm/pull/24322).
In this post, I benchmark the performance of this implementation and compare it against vanilla decoding and EAGLE-3, a modern speculative decoding method already supported by vLLM.
The results show that draft_model achieves 1.5x to 2x speedups, matching EAGLE-3's performance while offering the advantage of requiring no specialized training.

## Background

Speculative decoding is a technique that accelerates LLM inference by using a smaller "draft" model to predict multiple tokens ahead, which are then verified in parallel by the larger target model.
This approach can significantly improve throughput, especially in memory-bound regimes with small batch sizes.

**A bit of history:** Speculative decoding with draft models was previously available in vLLM V0.
There's an excellent [GPU Mode lecture](https://www.youtube.com/watch?v=9wNAgpX6z_4) by [Daniel Cade](https://www.linkedin.com/in/cade-daniel/) from June 2024 that outlines the original implementation.
However, during the rearchitecting to V1, this feature was removed and hadn't been reimplemented until now.

## Using Speculative Decoding

To use speculative decoding with vLLM, simply pass additional arguments to the `vllm serve` command.
You'll need to specify the draft model and the number of speculative tokens to generate.
I like passing the `--max-model-len` as well, to leave more memory for the KV cache.

```shell
vllm serve Qwen/Qwen3-32B \
    --speculative_config.method=draft_model \
    --speculative_config.model=Qwen/Qwen3-1.7B \
    --speculative_config.num_speculative_tokens=4 \
    --speculative_config.max_model_len=5000 \
    --max-model-len 5000
```

## Experimental Setup

### Models
As a target model, I used **`Qwen3/Qwen-32B`**^[<https://huggingface.co/Qwen/Qwen3-32B>].
Vanilla decoding uses this model directly.
For draft_model, I evaluated two different draft models: **`Qwen3/Qwen-1.7B`**^[<https://huggingface.co/Qwen/Qwen3-1.7B>], and **`Qwen3/Qwen-4B`**^[<https://huggingface.co/Qwen/Qwen3-4B>].
The 0.6B model threw a runtime exception (probable bug).
For EAGLE-3, I evaluated the **`RedHatAI/Qwen3-32B-speculator.eagle3`** draft model^[<https://huggingface.co/RedHatAI/Qwen3-32B-speculator.eagle3>].

### Datasets

I evaluated performance on two datasets:

- **MT-Bench**^[<https://huggingface.co/datasets/philschmid/mt-bench>]: A dataset of 80 multi-turn questions used to benchmark speedups in the EAGLE-3 paper by @li2025eagle.
This dataset is too small to test batch sizes larger than 80 without repeating prompts.

- **InstructCoder**^[<https://huggingface.co/datasets/likaixin/InstructCoder>]: A dataset of 114k prompts about programming tasks.
This larger dataset enables testing at higher batch sizes.
Programming tasks are thought to contain substantial repetition between input and output tokens, which may be beneficial for speculative decoding.

### Benchmarking Methodology

I benchmarked performance using `vllm bench serve` to measure token throughput metrics.
All experiments were run with temperature set to 0.0 (greedy sampling) because vLLM does not yet support lossless sampling with temperature > 0.0 (this feature has an [open PR](https://github.com/vllm-project/vllm/pull/20459)).
All experiments were run on an NVIDIA H100 96GB GPU.

<details>
<summary>Benchmarking Command</summary>
```shell
vllm bench serve \
  --model Qwen/Qwen3-32B \
  --dataset-name hf \
  --dataset-path likaixin/InstructCoder \
  --max-concurrency 32 \
  --num-prompts 320 \
  --temperature 0.0 \
  --top-p 1.0 \
  --ready-check-timeout-sec 600
```
For `--num-prompts`, I use 10 times `--max-concurrency` or at least 50.
</details>

<details>
<summary>Benchmarking Output</summary>
```shell
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Benchmark duration (s):                  47.20     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              6.78      
Output token throughput (tok/s):         1355.72   
Peak output token throughput (tok/s):    576.00    
Peak concurrent requests:                56.00     
Total token throughput (tok/s):          2378.30   
---------------Time to First Token----------------
Mean TTFT (ms):                          192.97    
Median TTFT (ms):                        140.48    
P99 TTFT (ms):                           629.06    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.75     
Median TPOT (ms):                        21.67     
P99 TPOT (ms):                           26.55     
---------------Inter-token Latency----------------
Mean ITL (ms):                           61.50     
Median ITL (ms):                         58.23     
P99 ITL (ms):                            114.37    
---------------Speculative Decoding---------------
Acceptance rate (%):                     46.13     
Acceptance length:                       2.85      
Drafts:                                  22519     
Draft tokens:                            90076     
Accepted tokens:                         41554     
Per-position acceptance (%):
  Position 0:                            71.44     
  Position 1:                            49.82     
  Position 2:                            36.32     
  Position 3:                            26.96     
==================================================
```
</details>

## Results

### Performance vs. Vanilla Decoding

I ran benchmarks across different concurrency levels ranging from 1 to 64.
The resulting throughput metrics are shown in @fig-total-token-throughput-short.
The x-axis is the inference batch size, implemented as concurrent requests sent to the server (`--max-concurrency`).
We observe that using speculative decoding **nearly doubles** token throughput compared to vanilla decoding.
In the next section we see what happens when we continue increasing the batch size.

::: {.panel-tabset group="dataset"}
# MT-Bench
![Token Throughput Metrics.](imgs/mt-bench/total_token_throughput_short.png){width="60%" text-align="left" fig-alt="Token throughput metrics" #fig-total-token-throughput-short}

# InstructCoder
![Token Throughput Metrics.](imgs/InstructCoder/total_token_throughput_short.png){width="60%" text-align="left" fig-alt="Token throughput metrics"}
:::

### Optimal Number of Speculative Tokens $K$

For the benchmarks shown in @fig-total-token-throughput-short, I set the number of speculative tokens $K=4$.
However, the optimal value of $K$ depends on the acceptance rate of draft tokens, which varies with the data distribution.
If $K$ is set too high, the model wastes time predicting draft tokens that will be rejected.
If $K$ is set too low, we miss potential speedup.

To understand these dynamics, I ran benchmarks across different values of $K$ and evaluated both the 1.7B and 4B models as draft models.
The results are shown in @fig-draft-model-ratios.
The y-axis shows the **speedup ratio** (or simply **speedup**), which measures how much faster speculative decoding is compared to vanilla decoding (in wall-clock time).
A speedup of 2 means speculative decoding finishes the workload in half the time of vanilla decoding.

::: {.panel-tabset group="dataset"}
# MT-Bench
![Throughput as a function of $K$, evaluating both the 1.7 and the 4B models as draft models.](imgs/mt-bench/draft_model_ratios.png){#fig-draft-model-ratios}

# InstructCoder
![Throughput as a function of $K$, evaluating both the 1.7 and the 4B models as draft models.](imgs/InstructCoder/draft_model_ratios.png){#fig-draft-model-ratios}
:::

The results show several key patterns:

1. **Draft model size trade-off**: The 1.7B model achieves generally higher speedups than the 4B model.
The 4B model generates better draft tokens (higher acceptance rate @fig-acceptance-lengths-vs-num-spec-toks), but it is also slower, so the speedup is hampered.

2. **Batch size dependency**: Speedup is maximal for small batch sizes (concurrency 1-4) and decays for larger batch sizes.
At batch size 128 or larger, speculative decoding becomes slower than vanilla decoding.
This aligns with research from the speculative decoding literature [@li2025eagle; @tang2025efficientspeculativedecodingllama], which shows that speculative decoding is most effective in the **memory-bound regime** (small batches and thin matrix multiplications).
In the **compute-bound regime** (large batches and regular matrix multiplications), speculative decoding adds extra compute work to an already saturated system.

3. **Low $K$ behavior**: For low $K$ values (e.g., $K=1$), speedups are lower at small batch sizes, but performance degradation at larger batch sizes is less pronounced.

4. **Saturation effect**: Increasing $K$ beyond 4 does not consistently improve speedups, indicating a saturation point.

### Comparison with EAGLE-3

EAGLE-3 is a modern speculative decoding method already supported by vLLM that uses lightweight draft models trained specifically for speculative decoding.
The EAGLE-3 paper by @li2025eagle reports speedup ratios on SGLang^[<https://github.com/sgl-project/sglang>] between 1.38x and 1.82x at batch sizes ranging from 2 to 64 (table 3, GPU=H100).
My benchmark results in @fig-eagle3-ratios demonstrate even **greater speedups on vLLM**: around 2.0x or better at batch sizes 1 to 4.
This confirms that EAGLE-3 works very well on vLLM and provides a strong baseline to compare my draft_model implementation against.

::: {.panel-tabset group="dataset"}
# MT-Bench
![Speedup ratios of EAGLE-3](imgs/mt-bench/eagle3_ratios.png){width="60%" text-align="left" fig-alt="Speedup ratios of EAGLE-3" #fig-eagle3-ratios}

# InstructCoder
![Speedup ratios of EAGLE-3](imgs/InstructCoder/eagle3_ratios.png){width="60%" text-align="left" fig-alt="Speedup ratios of EAGLE-3"}
:::

@fig-draft-model-vs-eagle3-ratios compares the speedup ratios of both speculative decoding methods.
It uses $K=4$ for both speculative decoding methods.
In the MT-Bench dataset, the draft_model speedups outperform EAGLE-3 in every batch size.
In the InstructCoder dataset, both methods have comparable speedups, with EAGLE-3 sligthly outperforming draft_model in the largest batch sizes.

::: {.panel-tabset group="dataset"}
# MT-Bench
![Comparison of Speedups of Draft Model and EAGLE-3](imgs/mt-bench/draft_model_vs_eagle3_ratios.png){width="60%" text-align="left" fig-alt="Comparison of Speedups of Draft Model and EAGLE-3" #fig-draft-model-vs-eagle3-ratios}

# InstructCoder
![Comparison of Speedups of Draft Model and EAGLE-3](imgs/InstructCoder/draft_model_vs_eagle3_ratios.png){width="60%" text-align="left" fig-alt="Comparison of Speedups of Draft Model and EAGLE-3"}
:::

The often comparable performance of both methods is remarkable given their very different **acceptance lengths** (AL).
@fig-acceptance-lengths-vs-num-spec-toks shows the AL for both methods.
Draft models achieve better AL than EAGLE-3 because they generate more tokens that are likely to be accepted by the target model.
We observe a saturation effect on the AL for EAGLE-3: The AL line starts to plateau around AL=2.5 (MT-Bench).
This means that predicting more tokens leads to diminishing returns.
In contrast, the AL curve for draft models continues to climb with more speculative tokens.
Should we increase $K$? No, as we saw in @fig-draft-model-ratios, increasing $K$ has diminishing returns.

::: {.panel-tabset group="dataset"}
# MT-Bench
![Acceptance Lengths for draft_model and EAGLE-3. For draft_model, the acceptance lengths of the 4B model are higher than for the 1.7B model.](imgs/mt-bench/acceptance_length_vs_num_spec_toks.png){width="90%" text-align="left" fig-alt="Acceptance Lengths for draft_model and EAGLE-3" #fig-acceptance-lengths-vs-num-spec-toks}

# InstructCoder
![Acceptance Lengths for draft_model and EAGLE-3. For draft_model, the acceptance lengths of the 4B model are higher than for the 1.7B model.](imgs/InstructCoder/acceptance_length_vs_num_spec_toks.png){width="90%" text-align="left" fig-alt="Acceptance Lengths for draft_model and EAGLE-3"}
:::

The fact that EAGLE-3 achieves similar speedups despite having lower AL demonstrates that its draft models are very **lightweight** and add minimal computational overhead.
This is the opposite behaviour of what we observed for the 4B draft_model in @fig-draft-model-ratios: Despite it having higher AL than the 1.7B model, it is too slow to achieve major speedups.

### Total Token Throughput

@fig-total-token-throughput-full shows the Total Token Throughput (toks/s) metric for both speculative decoding methods and vanilla decoding (SD=None).
Using a speculative decoding method outperforms vanilla decoding up to batch size 64.
Beyond this point, the throughput of speculative decoding methods plateaus at aroun 4000 toks/s, while vanilla decoding continues to scale with larger batch sizes, reaching over 5600 toks/s.

::: {.panel-tabset group="dataset"}
# MT-Bench
![Token Throughputs of all Methods](imgs/mt-bench/total_token_throughput_full.png){width="60%" text-align="left" fig-alt="Token Throughputs of all Methods" #fig-total-token-throughput-full}

# InstructCoder
![Token Throughputs of all Methods](imgs/InstructCoder/total_token_throughput_full.png){width="60%" text-align="left" fig-alt="Token Throughputs of all Methods"}
:::

### Inference Metrics

The `vllm bench serve` command reports several request-level inference metrics (lower is better for all):

- **TTFT**: Time to first token
- **TPOT**: Time per output token (excluding the first token)
- **ITL**: Inter-token latency

The benchmark reports means, medians, and 99th percentiles for these metrics.
@fig-ttft-itl-tpot shows the 99th percentile values for both speculative decoding methods and vanilla decoding.

::: {.panel-tabset group="dataset"}
# MT-Bench
![Inference Metrics. 99th percentile of TPOT, ITL, and TTFT. Lower is better.](imgs/mt-bench/ttft_itl_tpot.png){width="100%" text-align="left" fig-alt="Inference Metrics" #fig-ttft-itl-tpot}

# InstructCoder
![Inference Metrics. 99th percentile of TPOT, ITL, and TTFT. Lower is better.](imgs/InstructCoder/ttft_itl_tpot.png){width="100%" text-align="left" fig-alt="Inference Metrics"}
:::

All metrics generally increase (worsen) with batch size. Note that the graph shows worst case statistics (99th percentile). We observe the following patterns

- **TTFT**: The TTFT is higher for speculative decoding methods because the server must prefill with both the draft and target models, rather than just the target model.
It is not clear to me why the TTFT of EAGLE-3 is consistently higher than for draft_model, since the EAGLE-3 drafters are lighter.
- **TPOT**: Both speculative decoding methods significantly reduce TPOT compared to vanilla decoding from 35-40ms to 19-30ms.
EAGLE-3 has slightly better TPOT, since the drafters are lighter.
- **ITL**: The inter-token latency (ITL) differs from TPOT in a subtle way: it measures the time between batches of tokens as seen by the client.
During speculative decoding, the client receives batches of multiple tokens, so ITL will be higher than TPOT.
In vanilla decoding, ITL and TPOT should be equal or very similar.
In the plot, vanilla decoding (SD=None) has the lowest ITL, followed by EAGLE-3 and finally draft_model.

## Discussion

### When to Use Draft Models vs. EAGLE-3

Both draft_model and EAGLE-3 achieve similar speedups, but they have different trade-offs:

Advantages of draft_model:

- *No training required*: You can use a smaller model from the same family without training a specialized draft model.
This is particularly effective when the smaller model is trained by distillation from the larger model, meaning they were explicitly trained to mimick the logits of the larger model.
- *Better acceptance rates*: Draft models from the same family tend to have higher acceptance lengths, as shown in @fig-acceptance-lengths-vs-num-spec-toks.

Advantages of EAGLE-3:

- *Lower computational overhead*: EAGLE-3's lightweight draft models add minimal overhead, allowing get away with lower acceptance rates and to maintain speedups at larger batch sizes.

**Recommendation**: Use draft_model when you have access to a smaller model from the same family and want to get started quickly without training a specialized draft model.
Compare the performance with EAGLE-3 on your on workload if a corresponding draft model already exists on HuggingFace, or when you can and want to train a draft model yourself.
You can use the vLLM speculators library^[<https://github.com/vllm-project/speculators>] to train draft models.

## Lessons Learned

In this section I begin by sharing some lessons learned implementing speculative decoding in vLLM, and then in @sec-start-high-level I share tips
about how to approach the vLLM codebase as a new contributor.

### Side-Effects
The vLLM project uses a lot of side effects, which can make it difficult to understand what the inputs to the model really are.
The most important example to understand is the `forward_context`, which is a global variable that contains information about every layer in the model,
and references it KV cache.
The forward context is manipulated at runtime with `set_forward_context()` function, [here](https://github.com/vllm-project/vllm/blob/5da4c7d789fe0c4ca2c49913441f99df24715a97/vllm/v1/worker/gpu_model_runner.py#L3287) in the main loop of the `gpu_model_runner.py` file.
It looks similar to @lst-set-forward-context, and its important to understand that it also determines what goes into the model.
The forward context contains e.g. `AttentionMetadata.slot_mapping` object containing **pointers to the KV cache**.
If these pointers are incorrect, the KV cache will be corrupted at runtime, and your outputs will be garbage.

```{#lst-set-forward-context .python lst-cap="Set Forward Context"}
with set_forward_context(
  ... # A bunch of params that determine the inputs to the model
):
    self._model_forward(
      ...  # What you THINK are the only inputs to the model
    )
```

The forward context also includes the `batch_descriptor` and `cudagraph_runtime_mode` objects, which together determine which **CUDA graph
mode** the model will run in.
CUDA graphs are a complex topic by themselves, but in short they make the code within the graph run impressively fast.
There is an introduction to CUDA graphs in the vLLM docs^[<https://docs.vllm.ai/en/stable/design/cuda_graphs/>].
What's important to understand is that CUDA graphs require the input and output tensors (*buffers*) to be allocated before running the code (i.e. no allocating PyTorch tensors during the graph's execution).
All operations on buffers must be done in-place, rather than creating new tensors, so the code becomes heavy in side-effects.
I found that if you run CUDA graphs, but don't reuse the buffers, you will not get hard exceptions, but rather your outputs will be garbage, again.
This is why you find comments like @lst-use-persistent-buffers everywhere in the codebase.

```{#lst-use-persistent-buffers .python lst-cap="Example Comment About CUDA Graphs"}
# Run the model.
# Use persistent buffers for CUDA graphs.
with set_forward_context(...):
```

### Start High-Level {#sec-start-high-level}
Introduce one or more unit tests for your desired API (e.g. `method=draft_model` for speculative decoding)
Think about how as a user you would observe the outputs to identify if API is correct (e.g. `acceptance_rate=100%` in greedy decoding).
Then go ahead and run the tests. 
You will find yourself waiting for vLLM to spin up and compile the model, so you want to deactivate all features that make you wait for your code to execute
(e.g. set `enforce-eager=True`).
The reason is that slow **feedback loops** sap your focus and kill your flow.
You will obviously hit errors that prevent your feature from working, but that is the goal.
You are working from the inside out, catching and solving problems locally one at a time without having to understand the whole codebase at once.
Set a debugger and try to understand only one error at a time, interactively and with the local context.
vLLM is a very complex system, and trying to understand the whole codebase at once would be overwhelming and lead to frustration.

### Use The Debugger
Most features require you to manipulate tensors, and change their shape or content.
This type of PyTorch code is often not simple to read or write, because its often more imperative than declarative.
Good contributors comment their code to communicate the intent of it, without repeating the code itself, and without making the code 
so verbose that its buried in noisy comments.
For these code regions heavy in PyTorch code, I suggest to factor out a pure Python function that you can test independently.
Then you create a unit test for it and can easily set breakpoints into the function without having to restart the envire vLLM server 
(which is painfully slow).

### Test extensively
I mentioned that you speed up your feedback cycles by setting e.g. `enforce-eager=True`, but that leaves the most important paths of the codebase untested.
In production, you never run in eager mode, so you want to test these paths as well.
I recommend the `pytest.mark.parametrize` decorator to test your code with different inputs, e.g. as shown in @lst-pytest-mark-parametrize to test both eager and compiled mode.
If you look at my [draft_model PR](https://github.com/vllm-project/vllm/pull/24322/changes#diff-51dd765a6984b59836ab7ec3c6c12c904afe4c325d72659fb82611b40521d008R658-R659), you will find that I used parametrizations almost like a paranoid to test many combinations.
E.g. Is the target model in FP8 or in FP16? 
Is the draft model in FP8/FP16? 
Are we using greedy sampling or temperature >= 0.0?
Is the target model using tensor parallelism of 1 or more?
What happens if we pass invalid combinations of these parameters?
Obviously, these are specific cases for speculative decoding, and the code paths that are relevant to your feature will become apparent as you understand the code from the outside in.

```{#lst-pytest-mark-parametrize .python lst-cap="Pytest Parametrization"}
@pytest.mark.parametrize("enforce_eager", [True, False])
def test_draft_model_correctness(enforce_eager):
  ...
```


<!-- 
# TODO: 
Report MT-Bench first since they are great.
Make sure the numbers in the text don't refer to InstructCoder only.

Mention that the tokenizers must be the same. models speak the same language. This is the case for smaller models in the same family.

**Lessons Learned**:

* pass `--request-ratio` to prevent statistical artifacts in the TTFT for non-SD (vanilla) decoding benchmarks.
* Unit tests are great, but often don't capture the full picture
* You need quick access to your sanity-check evaluation suite
* These scripts should live outside the git repository to avoid being wiped.
* Long lived branches can experience a lot of merge conflicts.
* Show example of `Eagle.propose()` method.
* Its not always clear whom to ask for a review and who can actually accept your PR. -->

# References