---
title: "Up to 2.24x Faster: Contributing Speculative Decoding with Draft Models to vLLM V1"
subtitle: "Benchmarks and Key Learnings"
author:
  - name: Tomas Ruiz
    email: t.ruiz@lmu.de
    affiliations: Ludwig-Maximilians-Universität München
date: "2026-01-25"
categories: [vLLM, PyTorch, Triton, Transformers]
format:
  html:
    toc: true
    code-line-numbers: true
editor:
    render-on-save: true
# image: tiled-matrix-multiplication-squared.jpg
engine: jupyter
bibliography: refs.bib
code-annotations: below
citation: true
# draft: true
---
I recently contributed speculative decoding with draft models to vLLM V1 ([PR #24322](https://github.com/vllm-project/vllm/pull/24322)).
In this post, I benchmark the performance of my implementation (draft_model) and compare it against vanilla decoding and EAGLE-3, a modern speculative decoding technique supported by vLLM.
My benchmarks show that draft_model can achieve speedups of up to 2.24× on MT-Bench, outperforming EAGLE-3, while requiring no specialized training.
At the end, I share key learnings from working in the vLLM codebase and a few practical tips for approaching and navigating it.

::: {.callout-note}
This is **not a tutorial** on speculative decoding.
I assume you are already familiar with the technique and its basic trade-offs.
The focus here is on benchmarking the new code and sharing key learnings.
:::

## Background

Speculative decoding is a technique that accelerates LLM inference by using a smaller "draft" model to predict multiple tokens ahead, which are then verified in parallel by the larger target model.
This approach can significantly improve throughput, especially in memory-bound regimes with small batch sizes.

**A bit of history:** Speculative decoding with draft models was previously available in vLLM V0.
There's an excellent [GPU Mode lecture](https://www.youtube.com/watch?v=9wNAgpX6z_4) by [Daniel Cade](https://www.linkedin.com/in/cade-daniel/) from June 2024 that outlines the original implementation.
However, during the rearchitecting to V1, this feature was removed and hadn't been reimplemented until now.

## Using Speculative Decoding

To use speculative decoding with vLLM, simply pass additional arguments to the `vllm serve` command.
You'll need to specify the draft model and the number of speculative tokens to generate.
I also recommend setting `--max-model-len` to leave more memory for the KV cache.

```shell
vllm serve Qwen/Qwen3-32B \
    --speculative_config.method=draft_model \
    --speculative_config.model=Qwen/Qwen3-1.7B \
    --speculative_config.num_speculative_tokens=4 \
    --speculative_config.max_model_len=5000 \
    --max-model-len 5000
```

## Benchmarking Setup

### Models & Datasets
As a target model, I used **`Qwen3/Qwen-32B`**^[<https://huggingface.co/Qwen/Qwen3-32B>].
Vanilla decoding uses this model directly.
For draft_model, I evaluated two different draft models: **`Qwen3/Qwen-1.7B`**^[<https://huggingface.co/Qwen/Qwen3-1.7B>] and **`Qwen3/Qwen-4B`**^[<https://huggingface.co/Qwen/Qwen3-4B>].
The 0.6B model encountered a runtime exception (likely a bug).
For EAGLE-3, I evaluated the **`RedHatAI/Qwen3-32B-speculator.eagle3`** draft model^[<https://huggingface.co/RedHatAI/Qwen3-32B-speculator.eagle3>].

I evaluated performance on two datasets:

- **MT-Bench**^[<https://huggingface.co/datasets/philschmid/mt-bench>]: A dataset of 80 multi-turn questions used to benchmark speedups in the EAGLE-3 paper by @li2025eagle.
This dataset is too small to test batch sizes larger than 80 without repeating prompts.

- **InstructCoder**^[<https://huggingface.co/datasets/likaixin/InstructCoder>]: A dataset of 114k prompts about programming tasks.
This larger dataset enables testing at higher batch sizes.
Programming tasks typically contain substantial repetition between input and output tokens, which may be beneficial for speculative decoding.

### Benchmarking Methodology

I benchmarked performance using `vllm bench serve` to measure token throughput metrics.
All experiments were run with temperature set to 0.0 (greedy sampling) because vLLM does not yet support lossless sampling with temperature > 0.0.
This feature has an [open PR](https://github.com/vllm-project/vllm/pull/20459).
All experiments were run on an NVIDIA H100 96GB GPU, within the SLURM cluster of the [LRZ AI Systems](https://doku.lrz.de/lrz-ai-systems-11484278.html).
The benchmarking code can be found on [Github](https://github.com/tomasruizt/vllm-scripts/blob/3564bb62538d75b2337e3aa65f502a17f5408dc7/README.md).
All benchmark results are available in a single [Parquet file](https://github.com/tomasruizt/vllm-scripts/blob/fccb861997697293835325cdc551c820aa1aa290/results/data-2026-01-23.parquet) for download.

<details>
<summary>Example Benchmarking Command</summary>
```shell
vllm bench serve \
  --model Qwen/Qwen3-32B \
  --dataset-name hf \
  --dataset-path likaixin/InstructCoder \
  --max-concurrency 32 \
  --request-rate 32 \
  --num-prompts 320 \
  --temperature 0.0 \
  --top-p 1.0 \
  --ready-check-timeout-sec 600
```
For `--num-prompts`, I use 10 times `--max-concurrency` or at least 50 prompts.
</details>

<details>
<summary>Example Benchmarking Output</summary>
```shell
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Benchmark duration (s):                  47.20     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              6.78      
Output token throughput (tok/s):         1355.72   
Peak output token throughput (tok/s):    576.00    
Peak concurrent requests:                56.00     
Total token throughput (tok/s):          2378.30   
---------------Time to First Token----------------
Mean TTFT (ms):                          192.97    
Median TTFT (ms):                        140.48    
P99 TTFT (ms):                           629.06    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.75     
Median TPOT (ms):                        21.67     
P99 TPOT (ms):                           26.55     
---------------Inter-token Latency----------------
Mean ITL (ms):                           61.50     
Median ITL (ms):                         58.23     
P99 ITL (ms):                            114.37    
---------------Speculative Decoding---------------
Acceptance rate (%):                     46.13     
Acceptance length:                       2.85      
Drafts:                                  22519     
Draft tokens:                            90076     
Accepted tokens:                         41554     
Per-position acceptance (%):
  Position 0:                            71.44     
  Position 1:                            49.82     
  Position 2:                            36.32     
  Position 3:                            26.96     
==================================================
```
</details>

## Results

### Performance vs. Vanilla Decoding

I ran benchmarks across different concurrency levels ranging from 1 to 64.
The resulting throughput metrics are shown in @fig-total-token-throughput-short.
The x-axis represents the inference batch size, implemented as concurrent requests sent to the server (`--max-concurrency`).
We observe that using speculative decoding **nearly doubles** token throughput compared to vanilla decoding.
I set the number of speculative tokens to $K=4$.
In the next section, we see what happens when we change $K$.

::: {.panel-tabset group="dataset"}
# MT-Bench
![Token Throughput Metrics.](imgs/mt-bench/total_token_throughput_short.png){width="60%" text-align="left" fig-alt="Token throughput metrics" #fig-total-token-throughput-short}

# InstructCoder
![Token Throughput Metrics.](imgs/InstructCoder/total_token_throughput_short.png){width="60%" text-align="left" fig-alt="Token throughput metrics"}
:::

### Optimal Number of Speculative Tokens $K$

The optimal value of $K$ depends on the acceptance rate of draft tokens, which varies with the data distribution.
If $K$ is set too high, the model wastes time predicting draft tokens that will be rejected.
If $K$ is set too low, we miss potential speedups.

To understand these dynamics, I ran benchmarks across different values of $K$ and evaluated both the 1.7B and 4B models as draft models.
The results are shown in @fig-draft-model-ratios.
The y-axis shows the **speedup ratio** (or simply **speedup**), which measures how much faster speculative decoding is compared to vanilla decoding (in wall-clock time).
A speedup of 2× means speculative decoding finishes the workload in half the time of vanilla decoding.

::: {.panel-tabset group="dataset"}
# MT-Bench
![Throughput as a function of $K$, evaluating both the 1.7 and the 4B models as draft models.](imgs/mt-bench/draft_model_ratios.png){#fig-draft-model-ratios}

# InstructCoder
![Throughput as a function of $K$, evaluating both the 1.7 and the 4B models as draft models.](imgs/InstructCoder/draft_model_ratios.png)
:::

The results show several key patterns:

1. **Draft model size trade-off**: The 1.7B model generally achieves higher speedups than the 4B model.
The 4B model generates better draft tokens (higher acceptance rate @fig-acceptance-lengths-vs-num-spec-toks), but it is also slower, which limits the overall speedup.

2. **Batch size dependency**: Speedup is maximal for small batch sizes (concurrency 1-4) and decays for larger batch sizes.
At batch size 128 or larger, speculative decoding becomes slower than vanilla decoding.
This aligns with research from the speculative decoding literature [@li2025eagle; @tang2025efficientspeculativedecodingllama], which shows that speculative decoding is most effective in the **memory-bound regime** (small batches and thin matrix multiplications).
In the **compute-bound regime** (large batches and regular matrix multiplications), speculative decoding adds extra compute work to an already saturated system.

3. **Low $K$ behavior**: For low $K$ values (e.g., $K=1$), speedups are lower at small batch sizes, but performance degradation at larger batch sizes is less pronounced.

4. **Saturation effect**: Increasing $K$ beyond 4 does not consistently improve speedups, indicating a saturation point.

### Comparison with EAGLE-3

EAGLE-3 is a modern speculative decoding technique supported by vLLM that uses lightweight draft models trained specifically for speculative decoding.
The EAGLE-3 paper by @li2025eagle reports speedup ratios on SGLang^[<https://github.com/sgl-project/sglang>] between 1.38× and 1.82× at batch sizes ranging from 2 to 64 (table 3, GPU=H100).
My benchmark results in @fig-eagle3-ratios demonstrate even **greater speedups on vLLM**: around 2.0× or better at batch sizes 1 to 4, although we used different models (LLaMA-Instruct-3.1-8B vs Qwen3-32B, respectively).
This confirms that EAGLE-3 works very well on vLLM and provides a strong baseline against which to compare my draft_model implementation.

::: {.panel-tabset group="dataset"}
# MT-Bench
![Speedup ratios of EAGLE-3](imgs/mt-bench/eagle3_ratios.png){width="60%" text-align="left" fig-alt="Speedup ratios of EAGLE-3" #fig-eagle3-ratios}

# InstructCoder
![Speedup ratios of EAGLE-3](imgs/InstructCoder/eagle3_ratios.png){width="60%" text-align="left" fig-alt="Speedup ratios of EAGLE-3"}
:::

@fig-draft-model-vs-eagle3-ratios compares the speedup ratios of both speculative decoding methods.
It uses $K=4$ for both speculative decoding methods.
In the MT-Bench dataset, the draft_model speedups outperform EAGLE-3 in every batch size.
In the InstructCoder dataset, both methods have comparable speedups, with EAGLE-3 slightly outperforming draft_model at the largest batch sizes.

::: {.panel-tabset group="dataset"}
# MT-Bench
![Comparison of Speedups of Draft Model and EAGLE-3](imgs/mt-bench/draft_model_vs_eagle3_ratios.png){width="60%" text-align="left" fig-alt="Comparison of Speedups of Draft Model and EAGLE-3" #fig-draft-model-vs-eagle3-ratios}

# InstructCoder
![Comparison of Speedups of Draft Model and EAGLE-3](imgs/InstructCoder/draft_model_vs_eagle3_ratios.png){width="60%" text-align="left" fig-alt="Comparison of Speedups of Draft Model and EAGLE-3"}
:::

The often comparable performance of both methods is remarkable given their very different **acceptance lengths** (AL).
@fig-acceptance-lengths-vs-num-spec-toks shows the AL for both methods.
Draft models achieve better AL than EAGLE-3 because they generate more tokens that are likely to be accepted by the target model.
We observe a saturation effect on the AL for EAGLE-3: the AL line starts to plateau around AL=2.5 (MT-Bench).
This means that predicting more tokens leads to diminishing returns.
In contrast, the AL curve for draft models continues to climb with more speculative tokens.
Should we increase $K$? No—as we saw in @fig-draft-model-ratios, increasing $K$ has diminishing returns.

::: {.panel-tabset group="dataset"}
# MT-Bench
![Acceptance Lengths for draft_model and EAGLE-3. For draft_model, the acceptance lengths of the 4B model are higher than for the 1.7B model.](imgs/mt-bench/acceptance_length_vs_num_spec_toks.png){width="90%" text-align="left" fig-alt="Acceptance Lengths for draft_model and EAGLE-3" #fig-acceptance-lengths-vs-num-spec-toks}

# InstructCoder
![Acceptance Lengths for draft_model and EAGLE-3. For draft_model, the acceptance lengths of the 4B model are higher than for the 1.7B model.](imgs/InstructCoder/acceptance_length_vs_num_spec_toks.png){width="90%" text-align="left" fig-alt="Acceptance Lengths for draft_model and EAGLE-3"}
:::

The fact that EAGLE-3 achieves similar speedups despite having lower AL demonstrates that its draft models are very **lightweight** and add minimal computational overhead.
This contrasts with what we observed for the 4B draft_model in @fig-draft-model-ratios: despite having higher AL than the 1.7B model, it is too slow to achieve major speedups.

### Total Token Throughput

@fig-total-token-throughput-full shows the Total Token Throughput (toks/s) metric for both speculative decoding methods and vanilla decoding (SD=None).
Using a speculative decoding method outperforms vanilla decoding up to batch size 64.
Beyond this point, the throughput of speculative decoding methods plateaus at around 4000 toks/s, while vanilla decoding continues to scale with larger batch sizes, reaching over 5600 toks/s.

::: {.panel-tabset group="dataset"}
# MT-Bench
![Token Throughputs of all Methods](imgs/mt-bench/total_token_throughput_full.png){width="60%" text-align="left" fig-alt="Token Throughputs of all Methods" #fig-total-token-throughput-full}

# InstructCoder
![Token Throughputs of all Methods](imgs/InstructCoder/total_token_throughput_full.png){width="60%" text-align="left" fig-alt="Token Throughputs of all Methods"}
:::

### Inference Metrics

The `vllm bench serve` command reports several request-level inference metrics (lower is better for all):

- **TTFT**: Time to first token
- **TPOT**: Time per output token (excluding the first token)
- **ITL**: Inter-token latency

The benchmark reports means, medians, and 99th percentiles for these metrics.
@fig-ttft-itl-tpot shows the 99th percentile values for both speculative decoding methods and vanilla decoding.

::: {.panel-tabset group="dataset"}
# MT-Bench
![Inference Metrics. 99th percentile of TPOT, ITL, and TTFT. Lower is better.](imgs/mt-bench/ttft_itl_tpot.png){width="100%" text-align="left" fig-alt="Inference Metrics" #fig-ttft-itl-tpot}

# InstructCoder
![Inference Metrics. 99th percentile of TPOT, ITL, and TTFT. Lower is better.](imgs/InstructCoder/ttft_itl_tpot.png){width="100%" text-align="left" fig-alt="Inference Metrics"}
:::

All metrics generally increase (worsen) with batch size. Note that the graph shows worst-case statistics (99th percentile). We observe the following patterns:

- **TTFT**: The TTFT is higher for speculative decoding methods because the server must prefill with both the draft and target models, rather than just the target model.
It is unclear to me why the TTFT of EAGLE-3 is consistently higher than for draft_model, since the EAGLE-3 drafters are lighter.
- **TPOT**: Both speculative decoding methods significantly reduce TPOT compared to vanilla decoding from 35-40ms to 19-30ms.
EAGLE-3 has slightly better TPOT, since the drafters are lighter.
- **ITL**: The inter-token latency (ITL) differs from TPOT in a subtle way: it measures the time between batches of tokens as seen by the client.
During speculative decoding, the client receives batches of multiple tokens, so ITL will be higher than TPOT.
In vanilla decoding, ITL and TPOT should be equal or very similar.
In the plot, vanilla decoding (SD=None) has the lowest ITL, followed by EAGLE-3, and finally draft_model.

## Discussion

### When to Use Draft Models vs. EAGLE-3

Both draft_model and EAGLE-3 achieve similar speedups, but they have different trade-offs:

Advantages of draft_model:

- *No training required*: You can use a smaller model from the same family without training a specialized draft model.
This is particularly effective when the smaller model is trained by distillation from the larger model, meaning they were explicitly trained to mimic the logits of the larger model.
- *Better acceptance rates*: Draft models from the same family tend to have higher acceptance lengths, as shown in @fig-acceptance-lengths-vs-num-spec-toks.

Advantages of EAGLE-3:

- *Lower computational overhead*: EAGLE-3's lightweight draft models add minimal overhead, allowing it to maintain speedups at larger batch sizes despite lower acceptance rates.

### Recommendation
Use draft_model when you have access to a smaller model from the same family and want to get started quickly without training a specialized draft model.
Compare the performance with EAGLE-3 on your workload if a corresponding draft model already exists on HuggingFace, or when you can and want to train a draft model yourself.
You can use the vLLM speculators library^[<https://github.com/vllm-project/speculators>] to train draft models.

In any case, the draft model must have the same tokenizer as the target model, i.e., both models must "speak the same language."
This is typically the case when models belong to the same family.

## Key Learnings

In this section, I share key lessons I learned about vLLM while implementing speculative decoding.

### Side-Effects
The vLLM project uses a lot of side effects, which can make it difficult to understand what the inputs to the model really are.
The most important example to understand is the `forward_context` object, which is a global variable that contains information about every layer in the model and references its KV cache.
The forward context is manipulated at runtime with the `set_forward_context()` function, [here](https://github.com/vllm-project/vllm/blob/5da4c7d789fe0c4ca2c49913441f99df24715a97/vllm/v1/worker/gpu_model_runner.py#L3287) in the main loop of the `gpu_model_runner.py` file.
It looks similar to @lst-set-forward-context, and it's important to understand that it partially determines the inputs to the model.

```{#lst-set-forward-context .python lst-cap="Set Forward Context"}
with set_forward_context(
  ... # A bunch of params that determine the inputs to the model
):
    self._model_forward(
      ...  # What you THINK are the only inputs to the model
    )
```

The forward context contains, for example, the `AttentionMetadata.slot_mapping` object containing **pointers to the KV cache**.
If these pointers are incorrect, the KV cache will be corrupted at runtime, and your outputs will be garbage.
Saving to the KV cache happens concretely in the function `reshape_and_cache_flash()` (FlashAttention code path: [permalink](https://github.com/vllm-project/vllm/blob/13d8746c545576bcb6e0771ee4dc0b6fae694fa1/vllm/v1/attention/backends/flash_attn.py#L670)).
The function directly takes this `slot_mapping` tensor as an input.

### Performance Optimizations
There were three noteworthy drivers of performance I encountered when implementing speculative decoding:

1. **CUDA Graphs**
2. **CPU-GPU synchronizations**
3. **Triton Kernels**

#### CUDA Graphs

CUDA graphs are a complex topic by themselves, but in short: they make the code within the graph run impressively fast.
There is an introduction to CUDA graphs in the vLLM docs^[<https://docs.vllm.ai/en/stable/design/cuda_graphs/>].
What's important to understand is that CUDA graphs require the input and output tensors (**buffers**) to be allocated before running the code (i.e. no dynamic-shape allocation of PyTorch tensors during the graph's execution).
All operations on buffers must be done in-place, rather than creating new tensors, so the code becomes heavy in side-effects.
I found that if you run CUDA graphs, but don't reuse the buffers, you will not get hard exceptions, but rather your outputs will be garbage, again.
The forward context I mentioned earlier includes the `batch_descriptor` and `cudagraph_runtime_mode` objects, which together determine which *CUDA graph mode* the model will dispatch to.
This is why you find comments like @lst-use-persistent-buffers everywhere in the codebase.

```{#lst-use-persistent-buffers .python lst-cap="Example Comment About CUDA Graphs"}
# Run the model.
# Use persistent buffers for CUDA graphs.
with set_forward_context(...):
```

#### CPU-GPU synchronizations
To write fast and efficient PyTorch code, it's important to avoid CPU-GPU synchronizations.
Synchronizations prevent the CPU from scheduling enough work for the GPU to keep it busy, leading to low GPU utilization.
I wrote a full-length [separate post](https://tomasruizt.github.io/posts/08_cpu_gpu_synchronization/) about them, which I recommend reading for a deeper explanation including PyTorch examples.

#### Triton Kernels
Preparing the input tokens for the speculative decoding draft model required a lot of small PyTorch operations.
Launching many small kernels incurs unnecessary overhead (2-3µs per kernel).
To fuse all these operations into a single one, I implemented a custom [Triton kernel](https://github.com/vllm-project/vllm/blob/1209b784f2ba976eff2ea24bc33c61f35c6eb213/vllm/v1/spec_decode/draft_model.py#L234).
The kernel is quite simple, but it replaced around 8 calls to PyTorch functions, avoiding their launch overhead.
If you are interested in learning about Triton kernels, I recommend looking at the [tutorial examples](https://triton-lang.org/main/getting-started/tutorials/index.html).
The langauge is easy to learn and it gets you 85% of the way to speed-of-light performance.

### Dive Deep
While creating @fig-ttft-itl-tpot, I ran into a strange phenomenon:
The mean TTFT of the SD methods was consistently lower than for vanilla decoding.
This was contrary to the expected behavior, since the SD methods have to perform the prefill with both the draft and target models, so their TTFT should be higher.
What was going on here?

![Statistical Artifact in TTFT between SD methods and vanilla decoding.](imgs/statistical-artifact.png){width="50%" text-align="left" fig-alt="Statistical Artifact in TTFT" #fig-statistical-artifact}

The root cause of the problem was the default `--request-rate` parameter of the `vllm bench serve` command, which is infinite, meaning that all requests were sent to the server *exactly* at the same time.
This meant that the server was batch processing all prefills at once in synchrony, which inflated the TTFT for all requests of vanilla decoding.
It did not affect the SD methods, because each request finished at a different step due to the different number of accepted tokens, which broke the batch synchrony.
To prevent this artifact, I now always set a request rate (e.g., `--request-rate` equals `--max-concurrency`), so that the requests are sent on average over the first second.

We sometimes find that our empirical results don't match our expectations or intuitions.
In these cases, we need to dive deep and understand what is driving the disagreement.
Otherwise, we would miss an opportunity to learn something about our system.

## Contributing to vLLM
In this section, I share some tips about how to approach the vLLM codebase as a new contributor.

### Start High-Level {#sec-start-high-level}
Introduce one or more unit tests for your desired API (e.g., `method=draft_model` for speculative decoding).
Think about how, as a user, you would observe the outputs to identify if the API is correct (e.g., `acceptance_rate=100%` in greedy decoding).
Then go ahead and run the tests.
You will find yourself waiting for vLLM to spin up and compile the model, so you want to deactivate all features that make you wait for your code to execute
(e.g., pass `--enforce-eager`).
The reason is that slow **feedback loops** sap your focus and kill your flow.
You will obviously hit errors that prevent your feature from working, but that is the goal.
You are working from the inside out, catching and solving problems locally one at a time without having to understand the whole codebase at once.
Set a debugger and try to understand only one error at a time, interactively and with the local context.
vLLM is a very complex system, and trying to understand the whole codebase at once would be overwhelming and lead to frustration.
Reading about the big picture of the codebase can be useful after getting some exposure to the codebase, e.g., [in this post](https://www.aleksagordic.com/blog/vllm) by Aleksa Gordić.

### Use the Debugger
Most features require you to manipulate tensors and change their shape or content.
This type of PyTorch code is often not simple to read or write, because it's often more imperative than declarative.
Good contributors comment their code to communicate the intent without repeating the code itself, and without making the code 
so verbose that it's buried in noisy comments.
For code regions heavy in PyTorch code, I suggest factoring out a pure Python function that you can test independently.
Then you can create a unit test for it and easily set breakpoints in the function without having to restart the entire vLLM server 
(which is very slow).

### Test Extensively
I mentioned that you speed up your feedback cycles by passing, e.g., `--enforce-eager`, but that leaves the most important paths of the codebase untested.
In production, you never run in eager mode, so you want to test these paths as well.
I recommend the `pytest.mark.parametrize` decorator to test your code with different inputs, e.g., as shown in @lst-pytest-mark-parametrize to test both eager and compiled mode.
If you look at my [draft_model PR](https://github.com/vllm-project/vllm/pull/24322/changes#diff-51dd765a6984b59836ab7ec3c6c12c904afe4c325d72659fb82611b40521d008R658-R659), you will find that I used parametrizations almost obsessively to test many combinations.
For example: Is the *target* model in FP8 or FP16? 
Is the *draft* model in FP8/FP16? 
Are we using greedy sampling or temperature >= 0.0?
Is the target model using tensor parallelism of 1 or more?
What happens if we pass invalid combinations of these parameters?
Obviously, these are specific cases for speculative decoding, and the code paths that are relevant to your feature will become apparent as you understand the code from the outside in.

```{#lst-pytest-mark-parametrize .python lst-cap="Pytest Parametrization"}
@pytest.mark.parametrize("enforce_eager", [True, False])
def test_draft_model_correctness(enforce_eager):
  ...
```

### Code Evolution
The vLLM project has far more contributors than reviewers, so many PRs sit unreviewed for extended periods.
My [PR](https://github.com/vllm-project/vllm/pull/24322) was open from 2025-09-05 and merged on 2026-01-19, which is 136 days, or about **4.5 months**, despite having relatively engaged reviewers.
With the vLLM codebase evolving so quickly, this long review cycle meant I had to resolve merge conflicts at least 10 times.

After resolving several painful conflicts, I learned that certain types of changes and patterns are more likely to cause conflicts.
For example, moving a function to a new file _and_ changing its body creates painful merge conflicts, because the function continues to evolve on the main branch in its old location.
The same applies to extracting a superclass or child class, changing its body, and then moving it to a new file.

While we shouldn't plan for PRs to stay open this long, we should be prepared for the main branch to evolve in parallel to our PR.
One strategy is to modify functions and classes in-place (within the same file) and defer moving them to other files in a subsequent PR.
This might sound counterintuitive to experienced engineers, but it proved effective to minimize merge conflicts.

## Future Outlook

### Full CUDA Graphs
Currently, vLLM supports two CUDA graph modes: `piecewise` and `full`.
The `full` mode is typically fastest, but it comes with stricter constraints and isn't compatible with all execution paths.
At the moment, draft models (both EAGLE-3 and `draft_model`) use `piecewise`.
In this section, I estimate the end-to-end speedups we could achieve by supporting `full` CUDA graphs for draft-model speculative decoding.
My results suggest that the speedup ratios over vanilla decoding would grow **from 2.26× to 2.37×**.

#### Method
I break the question into two measurements:

1. **Standalone (full CUDA graphs):**
   I disable speculative decoding, run each model (Qwen3-32B and Qwen3-1.7B) separately on a single InstructCoder request with 1000 output tokens, and measure ITL.
   This estimates the per-token runtime under full graphs.

2. **Within speculative decoding (piecewise CUDA graphs):**
   I enable speculative decoding (`draft_model`), run draft + target, and measure ITL while varying the number of speculative tokens \(K\).
   Because each additional speculative token triggers one extra draft forward pass, a linear fit of ITL vs K is a good approximation.
   The regression in @fig-itl-vs-num-spec-toks can be interpreted as follows:

   - *The Intercept*: the target-model cost inside the SD loop (piecewise).
   - *The Slope*: the incremental draft-model cost per additional speculative token (piecewise).

![Regression of ITL over number of speculative tokens. The linear fit is good.](imgs/itl_k_comparison.png){width="50%" text-align="left" fig-alt="Regression of ITL over number of speculative tokens." #fig-itl-vs-num-spec-toks}

With those two measurements, we can compare per-token runtimes across graph modes in @tbl-speed-comparison-full-vs-piecewise.
"SD Runtime" comes from the ITL regression (piecewise, inside SD), and "Standalone Runtime" is the median ITL from `vllm bench serve` without SD (full graphs).
For this setup, the draft model is **9.10% faster** under full graphs, while the target model is **2.91% faster**.

| Model        | SD Runtime (piecewise CG) | Standalone Runtime (full CG) | Runtime Reduction |
|--------------|------------------|----------------|----------------|
| Qwen3-1.7B  | 2.50 ms | 2.27 ms | 9.10 % |
| Qwen3-32B   | 24.36 ms | 23.65 ms | 2.91 % |  

:Table: Speed comparison of full and piecewise CUDA graphs. {#tbl-speed-comparison-full-vs-piecewise}

#### Forecasted End-to-end Impact
Assuming we could realize those per-token reductions inside speculative decoding, what does that translate to in terms of TPOT?
I use the formula below to forecast the speedups for full CUDA graphs.
$$
\text{TPOT} = \frac{\text{ITL}}{\text{AL}} = \frac{T_{d} \cdot K + T_{t}}{\text{AL}}
$$

where $T_{d}$ is the runtime of the draft model, $T_{t}$ is the runtime of the target model, $K$ is the number of speculative tokens, and $\text{AL}$ is the acceptance length.
@fig-tpot-comparison-full-vs-piecewise shows the TPOT values under piecewise CUDA graphs (Current), and full CUDA graphs (Expected).
The minimum (best) TPOT values for each curve are marked with a cross.
At $K=5$, the TPOT improves from 10.43ms to 9.91ms (4.9% improvement).
**The speedup ratios over vanilla decoding grow from 2.26× to 2.37×.**
When I started this analysis, I expected larger improvements (closer to 20%), but the target model dominates overall ITL, so draft-side gains translate to single-digit TPOT gains.
Nevertheless, this improvement would positively impact the performance of draft_model across all workloads.

![TPOT Comparison between piecewise and full CUDA graphs (forecast).](imgs/tpot_k_comparison.png){width="50%" text-align="left" fig-alt="TPOT calculation" #fig-tpot-comparison-full-vs-piecewise}

::: {.callout-note}
In this experiment, I used only a single request from the InstructCoder dataset, with `--output-len=1000`.
The best TPOT values were achieved at $K=5$ rather than the $K=4$ for the larger benchmark,
but the TPOT differences are insignificant.
The code for this analysis is available on [Github](https://github.com/tomasruizt/vllm-scripts/blob/4c80edd944e7c71d89e343ecc742cab22fae349b/profiling/analysis.ipynb)
:::

## Summary
In this post, I benchmarked the implementation of speculative decoding with draft models I contributed to vLLM V1 ([PR #24322](https://github.com/vllm-project/vllm/pull/24322)) and compared it against vanilla decoding and EAGLE-3. 
On MT-Bench and InstructCoder, it delivers substantial speedups (up to 2.24×) without specialized training.
Speedups are most pronounced at smaller batch sizes and fade at larger batch sizes.
The performance numbers are meant as a reference as of today (2026-01-25, commit `e1a34c3`), and can vary with model, workload, and deployment.
Finally, I hope the key learnings and contributor tips I shared here help other new vLLM contributors get productive faster.

## Acknowledgments
I thank the reviewers and collaborators of the vLLM project who engaged with my PR and provided valuable feedback: [Benjamin Chislett](https://www.linkedin.com/in/benjamin-chislett-05502818a/) from NVIDIA, [Ekagra Ranjan](https://www.linkedin.com/in/ekagra-ranjan/) from Cohere, [Lily Liu](https://www.linkedin.com/in/lily-xiaoxuan-liu-170bb4b1/) from OpenAI, [Wentao Ye](https://www.linkedin.com/in/yewentao/) from Red Hat, [Harry Mellor](https://www.linkedin.com/in/harrymellor/) from Hugging Face.

## References