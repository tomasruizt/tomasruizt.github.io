---
title: "vLLM Speculative Decoding"
author:
  - name: Tomas Ruiz
    email: t.ruiz@lmu.de
    affiliations: Ludwig-Maximilians-Universität München
# date: "2024-12-23"
# categories: [Mathematics, GPUs]
format:
  html:
    toc: true
    code-line-numbers: true
editor:
    render-on-save: true
# image: tiled-matrix-multiplication-squared.jpg
engine: jupyter
bibliography: refs.bib
code-annotations: below
citation: true
# draft: true
---
I recently contributed speculative decoding with draft models to vLLM V1 [PR #24322](https://github.com/vllm-project/vllm/pull/24322).
In this post I benchmark the performance of speculative decoding and compare it against vanilla decoding, and against EAGLE-3, which is a modern method for speculative decoding that already existed in vLLM.

First let's start with the basics.
How would you run speculative decoding with vLLM?
Simply pass some extra arguments to the `vllm serve` command.

<details>
<summary>Serve Command</summary>
```shell
vllm serve Qwen/Qwen3-32B \
    --speculative_config.method=draft_model \
    --speculative_config.model=Qwen/Qwen3-1.7B \
    --speculative_config.num_speculative_tokens=4 \
    --speculative_config.max_model_len=5000 \
    --max-model-len 5000
```
</details>

## Datasets
In terms of datasets, I use multiple ones:

- `philschmid/mt-dataset`: 80 multi-turn questions. Used to benchmark speedups in the EAGLE-3 paper. It is too small to test batch sizes larger than 80 without repeating prompts.
- `likaixin/InstructCoder`: 114k prompts about programming tasks. It is useful to run larger batch sizes. Also, programming is thought to contain lots of repetition between input and output tokens, which is potentially beneficial for SD.

## Performance Benchmarks
Let's compare the performance of the draft_model implementation first against vanilla decoding.
We can run a benchmark using `vllm bench serve` to get the token throughput metrics.
I also set the temperature to 0.0 (greedy sampling) because vLLM does not yet support lossless sampling with temperature > 0.0 (that feature has an [open PR](https://github.com/vllm-project/vllm/pull/20459)).

<details>
<summary>Benchmarking Command</summary>
```shell
vllm bench serve \
  --model Qwen/Qwen3-32B \
  --dataset-name hf \
  --dataset-path likaixin/InstructCoder \
  --max-concurrency 32 \
  --num-prompts 320 \
  --temperature 0.0 \
  --top-p 1.0 \
  --ready-check-timeout-sec 600
```
For `--num-prompts`, I use 10 times `--max-concurrency` or at least 50.
</details>

<details>
<summary>Benchmarking Output</summary>
```shell
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Benchmark duration (s):                  47.20     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              6.78      
Output token throughput (tok/s):         1355.72   
Peak output token throughput (tok/s):    576.00    
Peak concurrent requests:                56.00     
Total token throughput (tok/s):          2378.30   
---------------Time to First Token----------------
Mean TTFT (ms):                          192.97    
Median TTFT (ms):                        140.48    
P99 TTFT (ms):                           629.06    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.75     
Median TPOT (ms):                        21.67     
P99 TPOT (ms):                           26.55     
---------------Inter-token Latency----------------
Mean ITL (ms):                           61.50     
Median ITL (ms):                         58.23     
P99 ITL (ms):                            114.37    
---------------Speculative Decoding---------------
Acceptance rate (%):                     46.13     
Acceptance length:                       2.85      
Drafts:                                  22519     
Draft tokens:                            90076     
Accepted tokens:                         41554     
Per-position acceptance (%):
  Position 0:                            71.44     
  Position 1:                            49.82     
  Position 2:                            36.32     
  Position 3:                            26.96     
==================================================
```
</details>

I run the benchmark over different concurrency levels ranging from 1 to 64.
The resulting throughput metrics are shown in @fig-total-token-throughput-short below.
The x-axis is the number of concurrent requests sent to the server (`--max-concurrency`).
We observe that using SD greatly increases the token throughput by a factor of 1.5x to 2x.
Later we will see that further increasing the concurrency beyond 64 leads to a breakdown in throughput.

::: {.panel-tabset group="dataset"}
# InstructCoder
![Token Throughput Metrics. Experiment run on an NVIDIA H100 96GB GPU.](imgs/InstructCoder/total_token_throughput_short.png){width="60%" text-align="left" fig-alt="Token throughput metrics" #fig-total-token-throughput-short}

# MT-Bench
![Token Throughput Metrics. Experiment run on an NVIDIA H100 96GB GPU.](imgs/mt-bench/total_token_throughput_short.png){width="60%" text-align="left" fig-alt="Token throughput metrics"}
:::

## Optimal Number of Speculative Tokens $K$
For @fig-total-token-throughput-short, I set the number of speculative tokens $K=4$.
But what is the optimal number of speculative tokens $K$?
If we set it too high, the model will waste a lot of time predicting draft tokens that will be rejected.
If we set it too low, we miss potential speedup.
The optimal $K$ depends on the acceptance rate of draft tokens, which depends on the data.
But to get sense of the dynamics, I run a benchmark over different values of $K$ and plot the throughput metrics.
@fig-draft-model-ratios show the results evaluating both the 1.7B and the 4B models as draft models.

The y-axis is the "Speedup Ratio" or **speedup** in short, which measures how much faster using speculative decoding is compared to vanilla decoding.
If the speedup is 2, then using speculative decoding finishes the workload in half the time needed by vanilla decoding.

::: {.panel-tabset group="dataset"}
# InstructCoder
![Throughput as a function of $K$ for the dataset `likaixin/InstructCoder`, evaluating both the 1.7 and the 4B models as draft models.](imgs/InstructCoder/draft_model_ratios.png){#fig-draft-model-ratios}

# MT-Bench
![Throughput as a function of $K$ for the dataset `likaixin/InstructCoder`, evaluating both the 1.7 and the 4B models as draft models.](imgs/mt-bench/draft_model_ratios.png){#fig-draft-model-ratios}
:::

We observe that the speedups of the 1.7B model are generally higher than the speedups of the 4B model.
This shows that while the 4B model generates better draft tokens, the slower runtime compared to the 1.7B model slows down the overall speedup.
The speedup is maximal for small batch sizes (concurrency) ranging from 1 to 4, and decays for larger batch sizes.
At around batch size 128 or larger, SD becomes slower than vanilla decoding.
This is in line with research results from the SD literature [@li2025eagle; @tang2025efficientspeculativedecodingllama], which show that SD is most effective in the **memory-bound regime** (small batches and thin matmuls),
while in the compute-bound regime (large batches and squarish matmuls) SD is only adding extra compute work to an already saturated compute system.
Interestingly, the speedup for low $K$ values (e.g. 1) are lower in small batch sizes, but the performance degradation in larger batch sizes is less pronounced.
We also observe a **saturation** effect, where increasing $K$ beyond 4 does not lead to consistent speedups.

## Comparison against EAGLE-3
Let's now compare SD with the draft_model implementation against EAGLE-3, which is a modern SD method supported by vLLM.
The EAGLE-3 paper by @li2025eagle shows speedup ratios on SGLang^[https://github.com/sgl-project/sglang] of between 1.38x and 1.82x, at batch sizes ranging from 2 to 64 (table 3, GPU=H100).
My results on @fig-eagle3-ratios show an even **greater speedup on vLLM**: 2.0x speedup at batch size 1, and 1.43x at batch size 64.
In summary: EAGLE-3 works very well on vLLM, and is a strong baseline to compare the draft_model implementation against.

::: {.panel-tabset group="dataset"}
# InstructCoder
![Speedup ratios of EAGLE-3](imgs/InstructCoder/eagle3_ratios.png){width="60%" text-align="left" fig-alt="Speedup ratios of EAGLE-3" #fig-eagle3-ratios}

# MT-Bench
![Speedup ratios of EAGLE-3](imgs/mt-bench/eagle3_ratios.png){width="60%" text-align="left" fig-alt="Speedup ratios of EAGLE-3"}
:::

Now let's compare the speedups of the draft_model implementation against EAGLE-3. 
@fig-draft-model-vs-eagle3-ratios shows the speedups.
The speedups of draft models are also between 1.39x and 2.0x for batch sizes up to 64, matching those of EAGLE-3 up until batch size 16.
For larger batch sizes, the speedups of draft models are slightly lower than those of EAGLE-3.

::: {.panel-tabset group="dataset"}
# InstructCoder
![Comparison of Speedups of Draft Model and EAGLE-3](imgs/InstructCoder/draft_model_vs_eagle3_ratios.png){width="60%" text-align="left" fig-alt="Comparison of Speedups of Draft Model and EAGLE-3" #fig-draft-model-vs-eagle3-ratios}

# MT-Bench
![Comparison of Speedups of Draft Model and EAGLE-3](imgs/mt-bench/draft_model_vs_eagle3_ratios.png){width="60%" text-align="left" fig-alt="Comparison of Speedups of Draft Model and EAGLE-3"}
:::

Both methods performing similarly is remarkable, given that both methods have very **different acceptance lengths** (AL).
@fig-acceptance-lengths-vs-num-spec-toks shows the AL for both methods.
We observe that the AL starts at different levels for draft_model and for EAGLE-3, with draft_model having better AL.
The reason is that the models used for draft_model SD generate tokens more likely to be accepted by the target model.
We also observe a saturation effect for EAGLE-3, meaning that predicting more tokens leads to diminishing returns.
In contrast, the AL for draft_model continues to improve.

::: {.panel-tabset group="dataset"}
# InstructCoder
![Acceptance Lengths for draft_model and EAGLE-3. For draft_model, the acceptance lenghts of the 4B model are higher than for the 1.7B model.](imgs/InstructCoder/acceptance_length_vs_num_spec_toks.png){width="90%" text-align="left" fig-alt="Acceptance Lengths for draft_model and EAGLE-3" #fig-acceptance-lengths-vs-num-spec-toks}

# MT-Bench
![Acceptance Lengths for draft_model and EAGLE-3. For draft_model, the acceptance lenghts of the 4B model are higher than for the 1.7B model.](imgs/mt-bench/acceptance_length_vs_num_spec_toks.png){width="90%" text-align="left" fig-alt="Acceptance Lengths for draft_model and EAGLE-3"}
:::

The fact that EAGLE-3 achieves the same speedups despite having lower AL shows that the draft models used by EAGLE-3 are very lightweight, and add very little overhead to the computation.
In @fig-draft-model-ratios, we saw that using the 4B model in draft_model SD lead to lower speedups than using the 1.7B model, despite having better AL. 
This means that the improved AL of the 4B model is not enough to compensate for its slower runtime.

In terms of total token throughput, going for higher batch sizes is still beneficial.
@fig-total-token-throughput-full shows the total token throughput for both SD methods and vanilla decoding (SD=None).
Using a SD method outperforms vanilla decoding up to batch size 64.
Afterward, the throughput of SD methods plateaus, while the throughput of vanilla decoding continues to increase.

::: {.panel-tabset group="dataset"}
# InstructCoder
![Token Throughputs of all Methods](imgs/InstructCoder/total_token_throughput_full.png){width="60%" text-align="left" fig-alt="Token Throughputs of all Methods" #fig-total-token-throughput-full}

# MT-Bench
![Token Throughputs of all Methods](imgs/mt-bench/total_token_throughput_full.png){width="60%" text-align="left" fig-alt="Token Throughputs of all Methods"}
:::

## Inference Metrics
Common metrics reported by `vllm bench serve` below. Lower is better for all of them.

- **TPOT**: Time per output token (excluding the first token)
- **ITL**: Inter-token latency
- **TTFT**: Time to first token

The benchmark reports their means, medians, and 99th percentiles.
@fig-ttft-itl-tpot shows the 99th percentile of these metrics for both SD methods and vanilla decoding.

::: {.panel-tabset group="dataset"}
# InstructCoder
![Inference Metrics. 99th percentile of TPOT, ITL, and TTFT. Lower is better.](imgs/InstructCoder/ttft_itl_tpot.png){width="100%" text-align="left" fig-alt="Inference Metrics" #fig-ttft-itl-tpot}

# MT-Bench
![Inference Metrics. 99th percentile of TPOT, ITL, and TTFT. Lower is better.](imgs/mt-bench/ttft_itl_tpot.png){width="100%" text-align="left" fig-alt="Inference Metrics"}
:::

We observe a general trend of all metrics going up as the batch size increases.
In terms of the TPOT metric, both SD methods cut the time per output token significantly.
The inter-token latency differs from the TPOT in a subtle way: It measure the time between batches of tokens as seen by the client. 
During SD, the client receives batches of multiple tokens, which means that the ITL will be higher than the TPOT, while in vanilla decoding both ITL and TPOT should be equal or very similar.
We see this effect in the center plot, where vanilla decoding has the lowest ITL.
The TTFT is higher for SD methods than for vanilla decoding, because the server must prefill with both the draft and the target model, rather than just with the target model.

**Why use draft_model over EAGLE-3?**
The first advantage of draft_model is that you don't need to train a separate draft model.
Instead, you can pick a smaller model from the same family.


## History of SD With Draft Models on vLLM
SD with draft models is a feature that used to exist in vLLM V0.
There is an amazing [GPU Mode lecture](https://www.youtube.com/watch?v=9wNAgpX6z_4) on YouTube by [Daniel Cade](https://www.linkedin.com/in/cade-daniel/) from June 2024 where he outlines the speculative decoding implementation on vLLM.
However, during the rearchiteuring towards V1, the code for the feature was removed.
Since then, nobody had implemented it in V1.

**Lessons Learned**:

* pass `--request-ratio` to prevent statistical artifacts in the TTFT for non-SD (vanilla) decoding benchmarks.
* Unit tests are great, but often don't capture the full picture
* You need quick access to your sanity-check evaluation suite
* These scripts should live outside the git repository to avoid being wiped.
* Long lived branches can experience a lot of merge conflicts.
* Show example of `Eagle.propose()` method.
* Its not always clear whom to ask for a review and who can actually accept your PR.

# References