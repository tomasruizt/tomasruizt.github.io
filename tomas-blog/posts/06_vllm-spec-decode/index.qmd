---
title: "vLLM Speculative Decoding"
author:
  - name: Tomas Ruiz
    email: t.ruiz@lmu.de
    affiliations: Ludwig-Maximilians-Universität München
# date: "2024-12-23"
# categories: [Mathematics, GPUs]
format:
  html:
    toc: true
    code-line-numbers: true
editor:
    render-on-save: true
# image: tiled-matrix-multiplication-squared.jpg
engine: jupyter
bibliography: refs.bib
code-annotations: below
citation: true
# draft: true
---
I recently contributed speculative decoding with draft models to vLLM V1 [PR #24322](https://github.com/vllm-project/vllm/pull/24322).
In this post, I benchmark the performance of this implementation and compare it against vanilla decoding and EAGLE-3, a modern speculative decoding method already supported by vLLM.
The results show that draft_model achieves 1.5x to 2x speedups, matching EAGLE-3's performance while offering the advantage of requiring no specialized training.

## Background

Speculative decoding is a technique that accelerates LLM inference by using a smaller "draft" model to predict multiple tokens ahead, which are then verified in parallel by the larger target model.
This approach can significantly improve throughput, especially in memory-bound regimes with small batch sizes.

**A bit of history:** Speculative decoding with draft models was previously available in vLLM V0.
There's an excellent [GPU Mode lecture](https://www.youtube.com/watch?v=9wNAgpX6z_4) by [Daniel Cade](https://www.linkedin.com/in/cade-daniel/) from June 2024 that outlines the original implementation.
However, during the rearchitecting to V1, this feature was removed and hadn't been reimplemented until now.

## Using Speculative Decoding

To use speculative decoding with vLLM, simply pass additional arguments to the `vllm serve` command.
You'll need to specify the draft model and the number of speculative tokens to generate.
I like passing the `--max-model-len` as well, to leave more memory for the KV cache.

```shell
vllm serve Qwen/Qwen3-32B \
    --speculative_config.method=draft_model \
    --speculative_config.model=Qwen/Qwen3-1.7B \
    --speculative_config.num_speculative_tokens=4 \
    --speculative_config.max_model_len=5000 \
    --max-model-len 5000
```

## Experimental Setup

### Datasets

I evaluated performance on two datasets:

- **`philschmid/mt-dataset`**: A dataset of 80 multi-turn questions used to benchmark speedups in the EAGLE-3 paper.
  This dataset is too small to test batch sizes larger than 80 without repeating prompts.

- **`likaixin/InstructCoder`**: A dataset of 114k prompts about programming tasks.
  This larger dataset enables testing at higher batch sizes.
  Programming tasks are thought to contain substantial repetition between input and output tokens, which may be beneficial for speculative decoding.

### Benchmarking Methodology

I benchmarked performance using `vllm bench serve` to measure token throughput metrics.
All experiments were run with temperature set to 0.0 (greedy sampling) because vLLM does not yet support lossless sampling with temperature > 0.0 (this feature has an [open PR](https://github.com/vllm-project/vllm/pull/20459)).

<details>
<summary>Benchmarking Command</summary>
```shell
vllm bench serve \
  --model Qwen/Qwen3-32B \
  --dataset-name hf \
  --dataset-path likaixin/InstructCoder \
  --max-concurrency 32 \
  --num-prompts 320 \
  --temperature 0.0 \
  --top-p 1.0 \
  --ready-check-timeout-sec 600
```
For `--num-prompts`, I use 10 times `--max-concurrency` or at least 50.
</details>

<details>
<summary>Benchmarking Output</summary>
```shell
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Benchmark duration (s):                  47.20     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              6.78      
Output token throughput (tok/s):         1355.72   
Peak output token throughput (tok/s):    576.00    
Peak concurrent requests:                56.00     
Total token throughput (tok/s):          2378.30   
---------------Time to First Token----------------
Mean TTFT (ms):                          192.97    
Median TTFT (ms):                        140.48    
P99 TTFT (ms):                           629.06    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.75     
Median TPOT (ms):                        21.67     
P99 TPOT (ms):                           26.55     
---------------Inter-token Latency----------------
Mean ITL (ms):                           61.50     
Median ITL (ms):                         58.23     
P99 ITL (ms):                            114.37    
---------------Speculative Decoding---------------
Acceptance rate (%):                     46.13     
Acceptance length:                       2.85      
Drafts:                                  22519     
Draft tokens:                            90076     
Accepted tokens:                         41554     
Per-position acceptance (%):
  Position 0:                            71.44     
  Position 1:                            49.82     
  Position 2:                            36.32     
  Position 3:                            26.96     
==================================================
```
</details>

## Results

### Performance vs. Vanilla Decoding

I ran benchmarks across different concurrency levels ranging from 1 to 64.
The resulting throughput metrics are shown in @fig-total-token-throughput-short.
The x-axis represents the number of concurrent requests sent to the server (`--max-concurrency`).
We observe that using speculative decoding increases token throughput by a factor of 1.5x to 2x compared to vanilla decoding.
As we'll see later, increasing concurrency beyond 64 leads to a breakdown in throughput for speculative decoding.

::: {.panel-tabset group="dataset"}
# InstructCoder
![Token Throughput Metrics. Experiment run on an NVIDIA H100 96GB GPU.](imgs/InstructCoder/total_token_throughput_short.png){width="60%" text-align="left" fig-alt="Token throughput metrics" #fig-total-token-throughput-short}

# MT-Bench
![Token Throughput Metrics. Experiment run on an NVIDIA H100 96GB GPU.](imgs/mt-bench/total_token_throughput_short.png){width="60%" text-align="left" fig-alt="Token throughput metrics"}
:::

### Optimal Number of Speculative Tokens $K$

For the benchmarks shown in @fig-total-token-throughput-short, I set the number of speculative tokens $K=4$.
However, the optimal value of $K$ depends on the acceptance rate of draft tokens, which varies with the data distribution.
If $K$ is set too high, the model wastes time predicting draft tokens that will be rejected.
If $K$ is set too low, we miss potential speedup.

To understand these dynamics, I ran benchmarks across different values of $K$ and evaluated both the 1.7B and 4B models as draft models.
The results are shown in @fig-draft-model-ratios.
The y-axis shows the **speedup ratio** (or simply **speedup**), which measures how much faster speculative decoding is compared to vanilla decoding.
A speedup of 2 means speculative decoding finishes the workload in half the time of vanilla decoding.

::: {.panel-tabset group="dataset"}
# InstructCoder
![Throughput as a function of $K$ for the dataset `likaixin/InstructCoder`, evaluating both the 1.7 and the 4B models as draft models.](imgs/InstructCoder/draft_model_ratios.png){#fig-draft-model-ratios}

# MT-Bench
![Throughput as a function of $K$ for the dataset `likaixin/InstructCoder`, evaluating both the 1.7 and the 4B models as draft models.](imgs/mt-bench/draft_model_ratios.png){#fig-draft-model-ratios}
:::

The results show several key patterns:

1. **Draft model size trade-off**: The 1.7B model achieves generally higher speedups than the 4B model.
   While the 4B model generates better draft tokens (higher acceptance rate), its slower runtime outweighs this benefit.

2. **Batch size dependency**: Speedup is maximal for small batch sizes (concurrency 1-4) and decays for larger batch sizes.
   At batch size 128 or larger, speculative decoding becomes slower than vanilla decoding.
   This aligns with research from the speculative decoding literature [@li2025eagle; @tang2025efficientspeculativedecodingllama], which shows that speculative decoding is most effective in the **memory-bound regime** (small batches and thin matrix multiplications).
   In the **compute-bound regime** (large batches and squarish matrix multiplications), speculative decoding adds extra compute work to an already saturated system.

3. **Low $K$ behavior**: For low $K$ values (e.g., $K=1$), speedups are lower at small batch sizes, but performance degradation at larger batch sizes is less pronounced.

4. **Saturation effect**: Increasing $K$ beyond 4 does not consistently improve speedups, indicating a saturation point.

### Comparison with EAGLE-3

EAGLE-3 is a modern speculative decoding method already supported by vLLM that uses lightweight draft models trained specifically for speculative decoding.
The EAGLE-3 paper by @li2025eagle reports speedup ratios on SGLang^[https://github.com/sgl-project/sglang] between 1.38x and 1.82x at batch sizes ranging from 2 to 64 (table 3, GPU=H100).
My results shown in @fig-eagle3-ratios demonstrate even **greater speedups on vLLM**: 2.0x at batch size 1 and 1.43x at batch size 64.
This confirms that EAGLE-3 works very well on vLLM and provides a strong baseline for comparison.

::: {.panel-tabset group="dataset"}
# InstructCoder
![Speedup ratios of EAGLE-3](imgs/InstructCoder/eagle3_ratios.png){width="60%" text-align="left" fig-alt="Speedup ratios of EAGLE-3" #fig-eagle3-ratios}

# MT-Bench
![Speedup ratios of EAGLE-3](imgs/mt-bench/eagle3_ratios.png){width="60%" text-align="left" fig-alt="Speedup ratios of EAGLE-3"}
:::

Comparing the draft_model implementation against EAGLE-3, @fig-draft-model-vs-eagle3-ratios shows that draft models achieve speedups between 1.39x and 2.0x for batch sizes up to 64, matching EAGLE-3 up to batch size 16.
For larger batch sizes, draft models show slightly lower speedups than EAGLE-3.

::: {.panel-tabset group="dataset"}
# InstructCoder
![Comparison of Speedups of Draft Model and EAGLE-3](imgs/InstructCoder/draft_model_vs_eagle3_ratios.png){width="60%" text-align="left" fig-alt="Comparison of Speedups of Draft Model and EAGLE-3" #fig-draft-model-vs-eagle3-ratios}

# MT-Bench
![Comparison of Speedups of Draft Model and EAGLE-3](imgs/mt-bench/draft_model_vs_eagle3_ratios.png){width="60%" text-align="left" fig-alt="Comparison of Speedups of Draft Model and EAGLE-3"}
:::

The similar performance of both methods is remarkable given their very different **acceptance lengths** (AL).
@fig-acceptance-lengths-vs-num-spec-toks shows the AL for both methods.
Draft models start with better AL than EAGLE-3 because the smaller models used for draft_model speculative decoding generate tokens that are more likely to be accepted by the target model.
We observe a saturation effect for EAGLE-3, where predicting more tokens leads to diminishing returns.
In contrast, the AL for draft models continues to improve with more speculative tokens.

::: {.panel-tabset group="dataset"}
# InstructCoder
![Acceptance Lengths for draft_model and EAGLE-3. For draft_model, the acceptance lengths of the 4B model are higher than for the 1.7B model.](imgs/InstructCoder/acceptance_length_vs_num_spec_toks.png){width="90%" text-align="left" fig-alt="Acceptance Lengths for draft_model and EAGLE-3" #fig-acceptance-lengths-vs-num-spec-toks}

# MT-Bench
![Acceptance Lengths for draft_model and EAGLE-3. For draft_model, the acceptance lengths of the 4B model are higher than for the 1.7B model.](imgs/mt-bench/acceptance_length_vs_num_spec_toks.png){width="90%" text-align="left" fig-alt="Acceptance Lengths for draft_model and EAGLE-3"}
:::

The fact that EAGLE-3 achieves similar speedups despite having lower AL demonstrates that its draft models are very lightweight and add minimal computational overhead.
This aligns with our earlier observation in @fig-draft-model-ratios: using the 4B model in draft_model speculative decoding led to lower speedups than the 1.7B model, despite having better AL.
The improved AL of the 4B model is not sufficient to compensate for its slower runtime.

### Total Token Throughput

@fig-total-token-throughput-full shows the total token throughput for both speculative decoding methods and vanilla decoding (SD=None).
Using a speculative decoding method outperforms vanilla decoding up to batch size 64.
Beyond this point, the throughput of speculative decoding methods plateaus, while vanilla decoding continues to scale with larger batch sizes.

::: {.panel-tabset group="dataset"}
# InstructCoder
![Token Throughputs of all Methods](imgs/InstructCoder/total_token_throughput_full.png){width="60%" text-align="left" fig-alt="Token Throughputs of all Methods" #fig-total-token-throughput-full}

# MT-Bench
![Token Throughputs of all Methods](imgs/mt-bench/total_token_throughput_full.png){width="60%" text-align="left" fig-alt="Token Throughputs of all Methods"}
:::

### Inference Metrics

The `vllm bench serve` command reports several inference metrics (lower is better for all):

- **TPOT**: Time per output token (excluding the first token)
- **ITL**: Inter-token latency
- **TTFT**: Time to first token

The benchmark reports means, medians, and 99th percentiles for these metrics.
@fig-ttft-itl-tpot shows the 99th percentile values for both speculative decoding methods and vanilla decoding.

::: {.panel-tabset group="dataset"}
# InstructCoder
![Inference Metrics. 99th percentile of TPOT, ITL, and TTFT. Lower is better.](imgs/InstructCoder/ttft_itl_tpot.png){width="100%" text-align="left" fig-alt="Inference Metrics" #fig-ttft-itl-tpot}

# MT-Bench
![Inference Metrics. 99th percentile of TPOT, ITL, and TTFT. Lower is better.](imgs/mt-bench/ttft_itl_tpot.png){width="100%" text-align="left" fig-alt="Inference Metrics"}
:::

All metrics generally increase with batch size.
Both speculative decoding methods significantly reduce TPOT compared to vanilla decoding.
The inter-token latency (ITL) differs from TPOT in a subtle way: it measures the time between batches of tokens as seen by the client.
During speculative decoding, the client receives batches of multiple tokens, so ITL will be higher than TPOT.
In vanilla decoding, ITL and TPOT should be equal or very similar.
This effect is visible in the center plot, where vanilla decoding has the lowest ITL.
The TTFT is higher for speculative decoding methods because the server must prefill with both the draft and target models, rather than just the target model.

## Discussion

### When to Use Draft Models vs. EAGLE-3

Both draft_model and EAGLE-3 achieve similar speedups, but they have different trade-offs:

Advantages of draft_model:

- **No training required**: You can use a smaller model from the same family without training a specialized draft model.
- **Better acceptance rates**: Draft models from the same family tend to have higher acceptance lengths, as shown in @fig-acceptance-lengths-vs-num-spec-toks.

Advantages of EAGLE-3:

- **Lower computational overhead**: EAGLE-3's lightweight draft models add minimal overhead, allowing it to maintain speedups at larger batch sizes.
- **Better scaling**: EAGLE-3 shows slightly better performance at batch sizes above 16.

**Recommendation**: Use draft_model when you have access to a smaller model from the same family and want to get started quickly without training.
Use EAGLE-3 when you need optimal performance at larger batch sizes or are willing to train specialized draft models.


<!-- 
# TODO: 
Report MT-Bench first since they are great.
Make sure the numbers in the text don't refer to InstructCoder only.

**Lessons Learned**:

* pass `--request-ratio` to prevent statistical artifacts in the TTFT for non-SD (vanilla) decoding benchmarks.
* Unit tests are great, but often don't capture the full picture
* You need quick access to your sanity-check evaluation suite
* These scripts should live outside the git repository to avoid being wiped.
* Long lived branches can experience a lot of merge conflicts.
* Show example of `Eagle.propose()` method.
* Its not always clear whom to ask for a review and who can actually accept your PR. -->

# References