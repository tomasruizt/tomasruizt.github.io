**New post: Up to 3.55Ã— Faster. Speculative Decoding with Draft Models in vLLM V1 ðŸš€**

I contributed speculative decoding with draft models to vLLM V1 (PR #24322), a feature long missing since the rearchitecting to V1. A small draft model predicts multiple tokens ahead, which are then verified in parallel by a larger model.

In this post I cover:
* Performance benchmarks vs vanilla decoding and EAGLE-3: Draft model reaches speedups of up to 3.55Ã—, beating EAGLE-3 in 2/3 settings.
* Key learnings from the vLLM codebase: forward context and side effects, CUDA graphs and buffers, and practical tips for new contributors.

Post: https://tomasruizt.github.io/posts/06_vllm-spec-decode/

#vLLM #speculativedecoding #LLM #inference #PyTorch
