---
title: "Drilling Down into Multimodal Attention"
author:
  - name: Tomas Ruiz
    email: t.ruiz@lmu.de
    affiliations: Ludwig-Maximilians-Universität München
date: "2025-01-29"
categories: [Transformers, Attention]
format:
  html:
    toc: true
    code-line-numbers: true
editor:
    render-on-save: true
engine: jupyter
bibliography: refs.bib
code-annotations: below
citation: true
---

![Visualizing Multimodal Attention Patterns](images/multimodal-attn.png){width="70%" text-align="left" fig-alt="Visualizing Multimodal Attention" .lightbox}

This post explains how to inspect the attention patterns of vision-language models (VLMs) like `LLaVa`, or `PaliGemma2` using a new module I created for the `circuitsviz` library [link here](https://github.com/tomasruizt/CircuitsVis). My preliminary hypothesis suggests that the PaliGemma2 model, which uses a prefix-attention mask, has trained its `<bos>` token to be a "broker" token for visual information.

# Mechanistic Interpretability
Large language models (LLMs) are notoriously difficult to interpret (black-box). One approach to shed light on LLMs is mechanistic interpretability, which aims to understand the inner workings of the model by breaking down its components. The [distill.pub journal](https://distill.pub/) hosted early works on this topic, the team at [Anthropic](https://transformer-circuits.pub/) continued the tradition, and today researchers actively contribute to the field.

# Attention Patterns
The central component of the Transformer architecture is the attention mechanism, which allows the LLM to focus on different parts of the input sequence. Most interpretability research on attention has focused on text-only models, finding e.g. "induction heads" for in-context-learning [@olsson2022incontextlearninginductionheads]. To find such attention patterns, an essential tool is to visualize them, and the `circuitsviz` librarydo so. The examples below show that each token in the input sequence attends to other tokens with different "intensities", as determined by the attention mechanism.

::: {layout-ncol=2}

![Example 1: Induction Head Pattern](images/attn-pattern-induction-head.png){#fig-induction-head width="80%"}

![Example 2: Multiple Attention Heads](images/many-txt-attention-heads.png){width="120%"}

Text-Only Attention Patterns
:::

# Multimodal Tokens
But how are images turned into tokens? In contrast to text-only LLMs, VLMs can also process images. In this post, we will focus on two exemplary VLMs: LLaVA [@liu2023visualinstructiontuning] and PaliGemma [@beyer2024paligemmaversatile3bvlm]. A VLM consists of a vision encoder, an LLM and an adapter to combine both. The vision encoder is a vision transformer (ViT) [@dosovitskiy2021imageworth16x16words] that has been pre-trained with (image, text) pairs, like CLIP [@radford2021learningtransferablevisualmodels] or SigLIP [@zhai2023sigmoid]. The VLM converts the image into a sequence of image tokens in two steps: 

![1. First, The image is cropped into a square and broken down into 16x16=256 non-overlapping patches, which are projected into 256 tokens. The ViT adds one additional token named `[CLS]` to hold global information about the entire image, for a total of 257 tokens. All patches are then passed through the ViT to create visual embeddings. Image taken from [@dosovitskiy2021imageworth16x16words]](images/vision-transformer.png){width="60%"}

![2. Second, the visual embeddings are concatenated with the text embeddings and fed into the LLM. Image taken from [@liu2023visualinstructiontuning]](images/llava-architecture.png){width="60%"}

In theory, we could visualize the multimodal attention patterns in with the same approach as the text-only pattern, like in figure @fig-induction-head. But the input sequence is very long now (257 tokens + text tokens), and the pattern grows quadratically with the number of tokens. Also, the image tokens are concatenated by row by row, so their vertical spatial structure is lost in the naive text-only visualization.

# Visualizing Multimodal Attention
This is where the new visualization shines: It overlays the attention pattern over the image, so we can appreciate the spatial structure of the attention over the image. The main visualization is split in two attention grids: The left grid shows **only a single row** of the image self-attention pattern, rearranged spatially on top of the image. The right grid is the classic self-attention of the text tokens. 

By clicking on any token on either grid, the token is selected as the "destination" token, and the left grid switches to that row of the attention pattern. It is possible to tune the contrast of the attention with a slider, to see patterns with lower attention values. See the video below as an example.

{{< video https://www.youtube.com/embed/oIhhqn1tDhk >}}

# Case Study: PaliGemma2

I use the PaliGemma2 VLM [@steiner2024paligemma2familyversatile] by Google as an interesting case study, because it does not use a causal attention mask, but a prefix-attention mask. This means that the attention pattern is not triangular, and it means that early tokens can attend to the later tokens. In particular, the image tokens, which are concatenated first in the sequence, can attend to text tokens in the prompt.

![PaliGemma Prefix Attention Pattern. Note that the image tokens and the text tokens fully attend to each other. Only the output tokens have a causal mask. Image from [@beyer2024paligemmaversatile3bvlm]](images/prefix-attn.png){width="50%"}

PaliGemma2 uses a special syntax for prompt. We must prefix the text question with "Answer en <question>" for the model to answer a question in the english (en) language. In the example below, the model correctly answers the question "Answer en what is the color of the frisbee?" with "purple". 

It was surprising to find that the tokens in the text prompt attend most intensely to each other, rather than to the image tokens. This effect persists across layers. In the example below, to highlight the top attention tokens only, I set `max-attention = 1.0`. In the middle layers (layer 15), vertical bars are visible in almost every head. This indicates that most text tokens are attending to the `<bos>` token, which is the first token after the image tokens. Interestingly, the `<bos>` does not attend back to the other text tokens. We can tell because the first row of the colored pattern is completely white (close to 0 attention) in middle and late layers.

![Layer 0](images/paligemma-layer-00-heads.png)

![Layer 15: Dark vertical bars, but first row (`<bos>` token) is white](images/paligemma-layer-15-heads.png){#fig-paligemma-layer-15}

![Layer 25](images/paligemma-layer-25-heads.png)

So what is the `<bos>` token attending to? Mostly to image tokens. To highlight this, I set `max-attention = 0.01` and compare the attentions with different destination text tokens. The `<bos>` token is attending uniformly to all the image tokens. The images below are all from intermediate layers (layer 15).

![The `<bos>` token attends uniformly to all image tokens](images/paligemma-layer-15-dest-token-bos-maxatt-low.png)

![The first text token mostly does not attend to image tokens](images/paligemma-layer-15-dest-token-first-maxatt-low.png)

![The last text token attends to patches of image tokens](images/paligemma-layer-15-dest-token-last-maxatt-low.png)

This suggest a hypothesis: Namely that the visual information flows from the image tokens into the `<bos>` token, and then from the `<bos>` token to the rest of the text tokens. To quantify this, I partition the input into 3 region: The image tokens, the `<bos>` token, and the rest of the text tokens. By summing up the attention of the tokens in each region, we get a measure of the attention between regions. This yields a 3x3 matrix, where each row sums up to 1.

![Self-attention over input regions. In lower layers, image tokens mostly attend to each other, despite having access to all tokens. Later, in middle and final layers, image tokens also attend to the `<bos>` token, (an example of information flowing back from text to image). The `<bos>` token increasingly attends to the image tokens as depth increases. Text tokens attend as much (or more) to the `<bos>`token as to the image tokens, despite their ratio being 1:256.](images/blockwise-attn-sums.png)

These numbers suggest that **PaliGemma2 has trained the `<bos>` token to be a "broker" token for visual information:** The `<bos>` token "collects" and aggregates visual information from the image tokens into a single place, and then "serves" it back to text and image tokens. It plays a similar role as the `[CLS]` token in the ViT.

# Conclusion
I showed how to visualize multimodal attention patterns using the new module for `circuitsviz`, which is useful for exploratory work in interpretability. I used PaliGemma2 as an interesting case study, because of its prefix-attention mask. After inspecting the attention patterns, I hypothesize that the `<bos>` token is trained to be a "broker" token for visual information. After this qualitative analysis, the next step is to quantitatively test the hypothesis, e.g. by performing causal intervention on the activations of the `<bos>` token, and by using other (image, text) pairs to see if the relation holds. Understanding the mechanisms of how VLMs process visual information helps us build better models, control them more easily, and ultimately improve their trustworthyness and safety.

# References
