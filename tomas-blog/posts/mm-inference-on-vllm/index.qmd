---
title: "Analyzing 35,000 Videos with Multimodal LLMs and the vLLM Engine"
author:
  - name: Tomas Ruiz
    email: t.ruiz@lmu.de
    affiliations: Ludwig-Maximilians-Universität München
date: "2025-09-22"
categories: [GPUs, vLLM, Transformers]
format:
  html:
    toc: true
    code-line-numbers: true
editor:
    render-on-save: true
# image: tiled-matrix-multiplication-squared.jpg
engine: jupyter
bibliography: refs.bib
code-annotations: below
citation: true
---

**TL;DR:** In this post I describe my inference workflow with the vLLM inference engine [@DBLP:conf/sosp/KwonLZ0ZY0ZS23] to analyze 35k videos collected from TikTok.

## Motivation

For the research project I collaborated with [Renata Topinkova](https://renatatopinkova.github.io/), 
a computational social sciences researcher. 
Our goal was to detect subtle product sales by influencers on the platform, and we presented a [poster with our findings](./other-files/poster_ic2s2_25.pdf) at the IC2S2 conference in 2025
This post focuses mainly on the **large inference workload**, which required running a large multimodal LLMs (70B parameters) on 35k videos as quickly as possible before the deadline.
The only way to get this done in any reasonable time was with an _inference engine_, specifically vLLM.
I have not tested yet the alternative engines _SGLang_ [@zheng2024sglang] or [NVIDIA Dynamo](https://github.com/ai-dynamo/dynamo).

## Dataset and Model
Our dataset consists of 35,000 multimodal TikTok posts that are mostly made up of a video, image(s), audio and text.
Therefore, we needed a multimodal model capable of consuming inputs with multiple modalities.
The cues we were looking in the videos were often subtle and implicit, so we also needed the model to grasp subtle communication cues.
Larger models, typically in the range of 70B parameters, are much more competent at this.

We decided to use the `Qwen2.5-VL-72B-Instruct` model by the Qwen team [@bai2025qwen25vltechnicalreport] [(Hugging Face Link)](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct).
Other multimodal models require you to cut up videos into individual frames and pass them as a stack of images, but the Qwen2.5-VL model instead natively consumes the videos directly.
The key innovation to do this is called multidimensional RoPE (mRoPE) embeddings.
mRoPE encodes the time dimension of the video into a separate dimension [@wang2024qwen2vlenhancingvisionlanguagemodels].
Thus, Qwen2.5-VL understands the temporal relationship between individual video frames, unlike other multimodal models.

The only shortcoming of this model is that it does not process audio natively, but most vision-language models don't either.
To alleviate this, we transcribe the audio using Whisper [@pmlr-v202-radford23a] and include the transcript in the text input.
Some models capable of processing audio natively came out recently (omni-style models), but they are not yet supported on vLLM.
Additionally, these models often output speech, which is not something we needed.

## GPU Cluster

For our workload, we needed to run multiple instances of this large 72B model in parallel.
To this end, we used the [LRZ AI Systems cluster](https://doku.lrz.de/lrz-ai-systems-11484278.html)
We use mainly the BayernKI partition, which has 30 nodes, each with 4 Hopper H100 GPUs (94 GB VRAM per GPU) connected over NVLink.
These are beefy GPUs, but nevertheless, the 72B model still does not fit on a single GPU, even in bfloat16 precision, since only the model weights require around 144 GB VRAM.
The model would fit on a single Blackwell B200 GPU (180GB VRAM), but those are not available on our cluster.
Beyond the model weights, the model requires also space for the KV cache, model activations, etc.
Therefore, we allocate 4 GPUs, totaling 376 GB VRAM, per model instance.
To launch jobs, the cluster uses the SLURM job scheduler [@yoo2003slurm].

## Parallelism Strategies
vLLM support distributing the model on multiple GPUs, a strategy called **Tensor Parallelism**.
In essence, this means that each tensor is split up across multiple GPUs so that no single GPU must hold the entire model in memory.
The drawback of this is that the GPUs must communicate with each other to complete the model computations.
Nevertheless, with the high bandwith NVLink, this overhead is acceptable for the ability to run larger models.
The flag to use multiple GPUs in vLLM is very simple: `vllm serve ... --tensor-parallel-size=4`.

Each model instance processes a only a portion of the full dataset, a strategy called **Data Parallelism**.
I implemented data parallelism on the client side, outside vLLM,
which means that the client decides what data to send to the vLLM server based on its own _rank_ within the full workload.
SLURM supports a feature called [Job Arrays](https://slurm.schedmd.com/job_array.html), which allow to submit a collection of similar jobs.
Each job in the collection gets a unique serial ID (e.g., 0, 1, ..., 100), which SLURM sets in the environment variable `SLURM_ARRAY_TASK_ID`.
The model instance can read this ID (its _rank_) and understand which partition of the dataset it is assigned to process.
I found the descriptions of these parallelism strategies on the [Hugging Face Parallelism Methods page](https://huggingface.co/docs/transformers/main/perf_train_gpu_many) useful, for starters.

## Fault Tolerance
The vLLM library supports different inference modes:
It is possible to use _batch processing_, which will take a batch of requests at once, process them
and them return the results.
Alternatively, its possible to spin up an inference server behind an OpenAI-compatible API,
which can take requests _asynchronously_, and return the results as they are ready.
For our large workload, I expected the batch processing to be more efficient, but at the time of writing,
the batch processing code waited until all requests were completed before returning.
If any error occurred during processing, the entire batch would fail, and failures were more common than one would expect:
E.g. an exception was raised mid-workload because a long video did not fit into the context length.

Therefore, I opted for the asynchronous inference mode, which offered better **fault tolerance**.
As soon as a request completed, we dumped its result to a file with JSON-lines format.
If the workload fails halfway, we still have all previous results on disk.
If a single request fails (e.g. video too long), that is also logged to the results file.
This approach makes it possible to restart the a failed workload later, by determining which requests are still missing from the results file. The JSON-lines file format allows for efficient append-only incremental writes, but is not a very efficient data format for downstream use, because it takes up a lot of space and is slow to read.
Therefore once result files are complete, we converted them to the [parquet format](https://github.com/apache/parquet-format), which has great lossless compression, and is super fast to read.

Sending thousands of multimodal requests at the server **concurrently** will choke it, because the server accepts all requests and starts processing all videos (e.g. loading from disk), but neglects generating tokens.
On top of this, processing too many requests concurrently will fill up the KV cache very quickly.
You can observe this in the vLLM logs because the KV cache utilization spikes to 100% and stays there for most of the workload.
To cope with this, the server evicts blocks from the cache, and puts most requests back in the queue, where they effectively sit idle.
To prevent this situation, I implemented a client-side throttling mechanism, which limits the number of requests sent to the server at the same time.
We set 16 concurrent requests for QwenVL-2.5-72B.
Getting this to work properly with Python's `asyncio` library and correct timeouts was tricky, but now you can just [copy the code from my repo](https://github.com/tomasruizt/llm_app/blob/8e122aed8f81880c4299416ffacc562c4b5f150f/llmlib/llmlib/openai/openai_completion.py#L174).

One more note about running the vLLM server in parallel: Multiple model instances might be colocated on the same node, e.g. a 8 GPU node can host 2x4 GPU jobs. To prevent the jobs from clashing over the same **vLLM port**, we set different ports for each model instance based on its rank (`SLURM_ARRAY_TASK_ID`).

# References
<!-- 
What to write about
The model, how many GPUs are needed (Tensor Parallelism)
that we transcribe using whisper
How I split the dataset (Data Parallelism)
The LRZ, what GPUs and nodes are available
   how these are connected to each other
SLURM, how array jobs work
    preventing port clashes
How much storage is 35k videos in TB
The posts are multimodal: videos, imgs, text
Incremental progress by splitting requests from backend server
Client-side throttling, since different models have different reqs
-->