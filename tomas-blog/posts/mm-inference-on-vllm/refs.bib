@inproceedings{DBLP:conf/sosp/KwonLZ0ZY0ZS23,
  author    = {Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph Gonzalez and Hao Zhang and Ion Stoica},
  title     = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
  year      = {2023},
  cdate     = {1672531200000},
  pages     = {611-626},
  url       = {https://doi.org/10.1145/3600006.3613165},
  booktitle = {SOSP},
  crossref  = {conf/sosp/2023}
}

@inproceedings{zheng2024sglang,
  title     = {{SGL}ang: Efficient Execution of Structured Language Model Programs},
  author    = {Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
  booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year      = {2024},
  url       = {https://openreview.net/forum?id=VqkAKQibpq}
}

@misc{bai2025qwen25vltechnicalreport,
  title         = {Qwen2.5-VL Technical Report},
  author        = {Shuai Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Sibo Song and Kai Dang and Peng Wang and Shijie Wang and Jun Tang and Humen Zhong and Yuanzhi Zhu and Mingkun Yang and Zhaohai Li and Jianqiang Wan and Pengfei Wang and Wei Ding and Zheren Fu and Yiheng Xu and Jiabo Ye and Xi Zhang and Tianbao Xie and Zesen Cheng and Hang Zhang and Zhibo Yang and Haiyang Xu and Junyang Lin},
  year          = {2025},
  eprint        = {2502.13923},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2502.13923}
}

@misc{wang2024qwen2vlenhancingvisionlanguagemodels,
  title         = {Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author        = {Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin},
  year          = {2024},
  eprint        = {2409.12191},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2409.12191}
}

@inproceedings{pmlr-v202-radford23a,
  title     = {Robust Speech Recognition via Large-Scale Weak Supervision},
  author    = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and Mcleavey, Christine and Sutskever, Ilya},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  pages     = {28492--28518},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume    = {202},
  series    = {Proceedings of Machine Learning Research},
  month     = {23--29 Jul},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v202/radford23a/radford23a.pdf},
  url       = {https://proceedings.mlr.press/v202/radford23a.html},
  abstract  = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.}
}
