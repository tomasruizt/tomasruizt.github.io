I put together everything I learned in the past 6 months about large-scale LLM video inference in this one guide.

We classified 35000 videos with a 72B parameters model (Qwen2.5-VL-72B) runnning on 4 H100 GPUs. I share lessons learned about computing on SLURM, about parallelism strategies (tensor- and data-parallelism), and share practical tips for fault tolerance and for video inference with vLLM. I also dive into concrete token throughput statistics of our workload, which is extremely prefill-heavy. And finally, I show how we corrected for the bias of the Qwen2.5-VL-72B model to make statistically valid statements about our dataset.