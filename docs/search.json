[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a research assistant at the Ludwig-Maximilian-University of Munich within Prof.¬†Schwemmer‚Äôs Computational Social Science Lab. My research area is the intersection of Machine Learning and Social Media, particularly on multi-modal understanding.\n\n\nIn previous jobs, I have worked as a software engineer in different corporations (Amazon, Allianz, BMW) and Startups. The projects ranged from optimization algorithms to backend-engineering. You can find my CV on LinkedIn.\n\n\n\n\n2021 - Technical University of Munich - Master of Science - Computer Science\n2018 - Technical University of Munich - Bachelor of Science - Engineering Science"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "",
    "text": "In previous jobs, I have worked as a software engineer in different corporations (Amazon, Allianz, BMW) and Startups. The projects ranged from optimization algorithms to backend-engineering. You can find my CV on LinkedIn."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "",
    "text": "2021 - Technical University of Munich - Master of Science - Computer Science\n2018 - Technical University of Munich - Bachelor of Science - Engineering Science"
  },
  {
    "objectID": "subscribe.html",
    "href": "subscribe.html",
    "title": "Subscribe",
    "section": "",
    "text": "To receive a notification when a new post is published, subscribe to the mailing list below.\n\n    \n\n\n  \n      \n          * indicates required\n          Email Address *\n      \n          \n          \n      \n  \n      /* real people should not fill this in and expect good things - do not remove this or risk form bot signups */"
  },
  {
    "objectID": "posts/01_linear-adaptation/index.html",
    "href": "posts/01_linear-adaptation/index.html",
    "title": "A Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification",
    "section": "",
    "text": "In this post I show how to linearly fine-tune a large language model (LLM) using a closed-form solution, based on the Moore-Penrose Inverse. I will focus on the special case of binary classification because the changes in output are easier to interpret. The new linear transformation \\(W\\) is shown in figure 1 (green).\n\n\n\n\n\n\nFigure¬†1: The learned transformation W (green) is applied in parallel to the existing linear layer (blue), preserving the existing knowledge of the model. Both are the summed to make the logits, which are passed to the softmax function."
  },
  {
    "objectID": "posts/01_linear-adaptation/index.html#references",
    "href": "posts/01_linear-adaptation/index.html#references",
    "title": "A Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification",
    "section": "References",
    "text": "References\n\n\nChowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al. 2023. ‚ÄúPalm: Scaling Language Modeling with Pathways.‚Äù Journal of Machine Learning Research 24 (240): 1‚Äì113.\n\n\nDutta, Abhinav, Sanjeev Krishnan, Nipun Kwatra, and Ramachandran Ramjee. 2024. ‚ÄúAccuracy Is Not All You Need.‚Äù arXiv Preprint arXiv:2407.09141.\n\n\nHu, Edward J, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. ‚ÄúLora: Low-Rank Adaptation of Large Language Models.‚Äù arXiv Preprint arXiv:2106.09685.\n\n\nWatkins, David S. 2004. Fundamentals of Matrix Computations. John Wiley & Sons."
  },
  {
    "objectID": "posts/01_linear-adaptation/index.html#footnotes",
    "href": "posts/01_linear-adaptation/index.html#footnotes",
    "title": "A Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n2024-09: The matrix \\(Z^T Z\\) is positive definite, so it is in theory efficiently invertible using the Cholesky decomposition. However, its condition number \\(\\kappa (Z^T Z)\\) is squarely proportional to the condition number of \\(\\kappa(Z)\\). This can lead to numerical instability when solving the linear system. In fact, I stumbled upon numerical instability while implementing this in linear system in PyTorch, which lead me to use an \\(L_2\\) regularization term. See Source.‚Ü©Ô∏é\n2024-09: It turns out that solving a linear system with \\(V\\) columns on the right-hand side can be done cheaper than in \\(V \\cdot \\frac{2}{3} d^3\\) flops. To solve \\(A X = B\\) (with \\(A \\in \\mathbb{R}^{d,d}\\) and \\(X, B \\in \\mathbb{R}^{d,V}\\)) the factorization of \\(A\\) requires \\(\\frac{2}{3} d^3\\) flops, but it only has to be done once. After that, solving for each of the \\(V\\) columns of \\(B\\) costs \\(2 d^2\\) flops each. So the total flop count is \\(\\frac{2}{3} d^3 + V \\cdot 2 d^2\\). This is a significant improvement over the naive approach. See (Watkins 2004, 77‚Äì78).‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/05_mm-inference-on-vllm/index.html",
    "href": "posts/05_mm-inference-on-vllm/index.html",
    "title": "A Guide to Classify 35,000 Videos with a 72B Multimodal LLM on the vLLM Engine",
    "section": "",
    "text": "In this post, I describe how we used the vLLM inference engine to classify 35k videos collected from TikTok for a research project. I share lessons learned about computing on SLURM, parallelism strategies (tensor- and data-parallelism), and practical tips for fault tolerance and video inference with vLLM. I also dive into concrete token throughput statistics of our workload, which is extremely prefill-heavy. Finally, I show how we corrected for the bias of the Qwen2.5-VL-72B model to make statistically valid statements about our dataset. As a bonus, I also share Python code for video classification using the vLLM library in Section¬†13, as well as the SLURM file for our workload in Section¬†12."
  },
  {
    "objectID": "posts/05_mm-inference-on-vllm/index.html#preventing-clashes",
    "href": "posts/05_mm-inference-on-vllm/index.html#preventing-clashes",
    "title": "A Guide to Classify 35,000 Videos with a 72B Multimodal LLM on the vLLM Engine",
    "section": "Preventing Clashes",
    "text": "Preventing Clashes\nMultiple model instances might be co-located on the same node, e.g., an 8 GPU node can host 2√ó4 GPU jobs. To prevent the jobs from clashing over the same vLLM port, we set different ports for each model instance based on its rank (SLURM_ARRAY_TASK_ID). We also gave each model instance a unique enroot container name based on the SLURM_ARRAY_TASK_ID, to prevent co-located jobs from clashing over the same container name. Also, we deleted the created enroot container after the job finished. To diagnose the failure of an individual job task within a job array, it‚Äôs useful to pipe stderr and stdout of each job task to different files, as shown in lines 5 and 6 of our SLURM file in Section¬†12. It‚Äôs possible to restart individual job tasks within a job array with SLURM: sbatch &lt;slurm-file&gt; --array=&lt;failed-task-id&gt;."
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "",
    "text": "The purpose of this post is two-fold:\n\nTo showcase Python running directly in your browser without any server behind it, like JavaScript. I will even import libraries like numpy and matplotlib. The underlying technologies that power this are WebAssembly and Pyodide, which I encourage you to check out.\nTo get you excited about a simple inequality and its application to vectors, functions & matrices. These are our objects of study, and we will inspect and visualize them interactively using Python.\n\nI created this document using Quarto Live. Big thanks to Renata Topinkova for showing me this tool!"
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html#summary",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html#summary",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "",
    "text": "The purpose of this post is two-fold:\n\nTo showcase Python running directly in your browser without any server behind it, like JavaScript. I will even import libraries like numpy and matplotlib. The underlying technologies that power this are WebAssembly and Pyodide, which I encourage you to check out.\nTo get you excited about a simple inequality and its application to vectors, functions & matrices. These are our objects of study, and we will inspect and visualize them interactively using Python.\n\nI created this document using Quarto Live. Big thanks to Renata Topinkova for showing me this tool!"
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html#the-inequality",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html#the-inequality",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "The Inequality",
    "text": "The Inequality\nMichael Steele presents in his book this simple inequality based only on the fact that \\((x-y)^2\\) is always positive1:\n\\[\n\\begin{align}\n0 &\\leq  (x - y)^2 \\\\\n\\implies 0 &\\leq x^2 -2xy +y^2 \\\\\n‚üπ xy &‚â§ \\frac{1}{2} (x^2 + y^2)\n\\end{align}\n\\]\nThe last inequality above is not intuitively obvious to me. I‚Äôm the kind of person that likes numerical proof to internalize these results, and this is where Python comes in handy. Change the variables x and y in the code below to see if the inequality holds.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis Python code is running in your browser. There is no juypter notebook, nor any deployment or any client-server communication behind it! ü§ØüöÄ"
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html#generalizing-to-vectors",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html#generalizing-to-vectors",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Generalizing to Vectors",
    "text": "Generalizing to Vectors\nWhat happens when we apply this inequality to more than just scalars? It also applies to sequences of numbers, e.g: \\[\nx_1 y_1 + x_2 y_2 &lt;= \\frac{1}{2} (x_1^2 + x_2^2) + \\frac{1}{2} (y_1^2 + y_2^2)\n\\] You might recognize that this is equivalent to an inner product: \\[ x^T y ‚â§ \\frac{1}{2} (x^T x + y^T y)\\] where \\(x = [x_1, \\dots, x_n]\\) and \\(y = [y_1, \\dots, y_n]\\).\nThe inequality is asserting that the vector product \\(x^Ty\\) of any two vectors \\(x\\) and \\(y\\) has an upper bound given by the average of \\(x^Tx\\) and \\(y^Ty\\).\nOnce again, I‚Äôm not intuitively convinced until I see code running. Notice how we import numpy, which calls compiled C routines under the hood (but our runtime is the browser now)."
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html#generalizing-to-functions",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html#generalizing-to-functions",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Generalizing to Functions",
    "text": "Generalizing to Functions\nYou might have heard that functions are infinite-dimensional vectors (ü§Ø). In that case, the inequality also applies! But how does the inner product \\(x^Ty\\) for two functions look like?\nThe convention is to use the bracket notation \\(‚ü® x, y ‚ü©\\) rather than \\(x^Ty\\). To sum over the infinite individual entries of the (function) vector, we use the integral:\n\\[‚ü® f, g ‚ü© = \\int f(x) g(x) dx\\] Using this definition, the inequality holds for functions as well: \\[\n\\begin{align}\n‚ü® f, g ‚ü© &‚â§ \\frac{1}{2} (‚ü® f, f ‚ü© + ‚ü® g, g ‚ü©) \\\\\n&= \\frac{1}{2} \\left( \\int f(x)^2 dx + \\int g(x)^2 dx \\right)\n\\end{align}\n\\]\nLet‚Äôs take two concrete functions \\(f(x) = \\cos(x)\\) and \\(g(x) = \\sin(4x)\\). I choose these arbitrarily because plotting them looks nice. Feel free to use different functions f and g in the code.\n\n\n\n\n\n\n\nIn the plot above, the individual functions \\(f^2\\) and \\(g^2\\) are plotted with light-blue lines. Their average is the red line, and the product \\(f ‚ãÖ g\\) is the green line. The red line is an upper bound for the green one. We see that the green line crosses over the two blue lines at different points but never crosses over the red line.\nAbout the integral: Perhaps you noticed that I formulated the inequality on inner-products, but I‚Äôm plotting the functions pointwise. The missing step is the integral, which is evaluated in Python using the numpy function np.trapz(). As we can confirm below, the inequality holds:"
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html#generalizing-to-matrices",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html#generalizing-to-matrices",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Generalizing to Matrices",
    "text": "Generalizing to Matrices\nWill the inequality also apply to matrices? The inner product of two matrices \\(A\\) and \\(B\\) (also called Frobenius inner product) is defined as: \\[‚ü®A, B‚ü© = \\text{tr}(A^T B)\\] where \\(\\text{tr}\\) is the trace operator.\n\n\n\n\n\n\nWarning\n\n\n\nBeware that this inner product is different from matrix multiplication \\[‚ü®A, B‚ü© = tr(A^T B) ‚â† AB\\]\n\n\nThe inequality for matrices then reads:\n\\[tr(A^T B) ‚â§ \\frac{1}{2} (tr(A^T A) + tr(B^T B))\\]\nIt‚Äôs easy to convince yourself that the inequality holds for matrices by writing down the trace as a sum of scalars. As we verified before, inequality holds for each scalar in the sum:\n\\[\n\\begin{align}\ntr(A^T B)\n&= \\sum_{i,j} A_{ij} B_{ij} & \\text{(definition)}\\\\\n&‚â§ \\frac{1}{2} \\left( \\sum_{i,j} A_{ij}^2 + \\sum_{i,j} B_{ij}^2 \\right) &\\text{(applied by scalar)}\\\\\n&= \\frac{1}{2} (tr(A^T A) + tr(B^T B))\n\\end{align}\n\\]\nLet‚Äôs check the inequality with random matrices. You can use the \"Start Over\" button to re-run the code with new matrices.\n\n\n\n\n\n\nThe inequality holds, but I have no geometric intuition about the trace of a matrix, or how this inequality could be visualized for matrices. If you have an idea, please let me know! üôè"
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html#further-sources",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html#further-sources",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Further Sources",
    "text": "Further Sources\nIf you found the mathematics interesting, particularly the generalization of inner products, I recommend MIT‚Äôs course Matrix Calculus for Machine Learning and Beyond, which covers this topic in more detail, and goes much further üòÑ."
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html#footnotes",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html#footnotes",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúThe Cauchy-Schwarz Master Class: An Introduction to the Art of Mathematical Inequalities‚Äù by J. Michael Steele.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/03_tiling-for-matrix-mult/index.html",
    "href": "posts/03_tiling-for-matrix-mult/index.html",
    "title": "How Does Tiling Speed Up Matrix Multiplications on GPUs?",
    "section": "",
    "text": "Tiled MatMul according to ChatGPT\nTL;DR: Tiling is a technique used to reduce the number of memory accesses performed during matrix multiplication. We see how it improves compute intensity and how it speeds up the matrix multiplication operation, not only on CPUs, but also on GPUs. I also provide a simple implementation of tiling in CUDA C."
  },
  {
    "objectID": "posts/03_tiling-for-matrix-mult/index.html#naive-memory-access",
    "href": "posts/03_tiling-for-matrix-mult/index.html#naive-memory-access",
    "title": "How Does Tiling Speed Up Matrix Multiplications on GPUs?",
    "section": "Naive Memory Access",
    "text": "Naive Memory Access\nIn the naive approach, computing \\(C_{ij}\\) requires fetching \\(n\\) elements from \\(A\\) (a row) and \\(n\\) elements from \\(B\\) (a column). That makes \\(2n\\) memory accesses. \\(C_{ij}\\) is computed as the dot product of the row and the column, which requires \\(2n\\) flops (1 mult and 1 add, \\(n\\) times). Thus, the compute intensity is: \\[\n\\text{Compute Intensity} = \\frac{\\text{flops}}{\\text{memory transfers}} = \\frac{2n}{2n} = 1\n\\]\nCan we do better? In this naive implementation, we are performing redundant memory transfers: For example, to compute \\(C_{11}\\), and \\(C_{12}\\), we fetch the first row of \\(A\\) twice from main memory."
  },
  {
    "objectID": "posts/03_tiling-for-matrix-mult/index.html#optimal-memory-access",
    "href": "posts/03_tiling-for-matrix-mult/index.html#optimal-memory-access",
    "title": "How Does Tiling Speed Up Matrix Multiplications on GPUs?",
    "section": "Optimal Memory Access",
    "text": "Optimal Memory Access\nIf our cache was (theoretically) large enough, we would transfer all elements of \\(A\\) and \\(B\\) into the cache at once (\\(2n^2\\) transfers) and perform the full matrix multiplication (\\(2n^3\\) flops).\n\\[\n\\text{Compute Intensity} = \\frac{\\text{flops}}{\\text{memory transfers}} = \\frac{2n^3}{2n^2} = n\n\\] The larger the matrices, the higher the compute intensity.\n\n\n\n\n\n\nExample\n\n\n\nAssuming a matrix size of \\(n=10,000\\), the naive approach does 1 flop per memory transfer, while the optimal approach does 10,000 flops per memory transfer. This would result in a 10,000x speedup, assuming memory bandwidth remained the bottleneck."
  },
  {
    "objectID": "posts/03_tiling-for-matrix-mult/index.html#a-middle-ground-tiling",
    "href": "posts/03_tiling-for-matrix-mult/index.html#a-middle-ground-tiling",
    "title": "How Does Tiling Speed Up Matrix Multiplications on GPUs?",
    "section": "A Middle Ground: Tiling",
    "text": "A Middle Ground: Tiling\nSince caching entire matrices is impractical, we divide matrices \\(A\\) and \\(B\\) into smaller square blocks of size \\(r\\), called tiles, perform block-wise multiplications, and aggregate the results.\nBut how does block-wise matrix multiplication work? Let‚Äôs go through an example to gain some intuition. We break the matrices \\(A\\), \\(B\\), and \\(C\\) into 4 blocks each (2x2). Each of these blocks has size \\(n/2\\).\n\\[\n\\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{bmatrix}\n\\begin{bmatrix}\nB_{11} & B_{12} \\\\\nB_{21} & B_{22}\n\\end{bmatrix} =\n\\begin{bmatrix}\nC_{11} & C_{12} \\\\\nC_{21} & C_{22}\n\\end{bmatrix}\n\\]\nTo compute an submatrix \\(C_{ij}\\), we multiply the corresponding blocks of \\(A\\) and \\(B\\) and sum the results. For example:\n\\[\nC_{11} = A_{11}B_{11} + A_{12}B_{21}\n\\]\nIn pseudocode this translates to the code below.\nn_blocks = n // r\nfor i in range(n_blocks):\n    for j in range(n_blocks):\n        for k in range(n_blocks):\n            C[i][j] += A[i][k] @ B[k][j]\nNote that the entry C[i][j] is now a block matrix rather than a scalar, and the @ operator denotes matrix multiplication, rather than scalar multiplication. Line 5 of the code above is loading blocks of size \\(r^2\\) from \\(A\\) and \\(B\\) into the cache, which takes \\(2r^2\\) memory transfers. Then, it multiplies the two blocks, which requires \\(2r^3\\) flops.\n\\[\n\\text{Compute Intensity} = \\frac{\\text{flops}}{\\text{memory transfers}} = \\frac{2r^3}{2r^2} = r\n\\]\n\n\n\n\n\n\nExample\n\n\n\nAssuming a block size of \\(r=100\\), the naive approach does 1 flop per memory transfer, while the tiling approach does 100 flops per memory transfer. This would result in a 100x speedup, assuming memory bandwidth remained the bottleneck.\n\n\nIt should be clear that \\(1 \\leq r \\leq n\\). Setting \\(r=1\\), we recover the naive approach, while setting \\(r=n\\) we recover the optimal approach. The table below compares the performance of the different methods. Note that for the tiled method, the flops and memory transfers are measured per block, rather than per matrix.\n\n\n\n\n\n\n\n\n\nMethod\nFlops\nMemory Transfers\nFlops/Memory Transfer\n\n\n\n\nNaive\n\\(2n^3\\)\n\\(2n \\cdot n^2\\)\n\\(\\frac{2n^3}{2n \\cdot n^2} = 1\\)\n\n\nTiling\n\\(2r^3\\)\n\\(2r^2\\)\n\\(\\frac{r^3}{r^2} = r\\)\n\n\nOptimal (Theoretical)\n\\(2n^3\\)\n\\(2n^2\\)\n\\(\\frac{n^3}{n^2} = n\\)"
  },
  {
    "objectID": "posts/08_cpu_gpu_synchronization/index.html",
    "href": "posts/08_cpu_gpu_synchronization/index.html",
    "title": "PyTorch and CPU-GPU Synchronizations",
    "section": "",
    "text": "TL;DR: This post is a guide to understand and prevent CPU-GPU synchronizations, which will help you write fast and efficient PyTorch programs üöÄ. I explain the concept with concrete PyTorch examples from this Github Gist, and profiles from NVIDIA Nsight Systems.\n\nIntroduction\nCPU-GPU synchronizations (also known as host-device synchronizations) are a subtle but important mechanism to write fast and efficient PyTorch programs. The CPU-GPU synchronization is a blocking operation that prevents the CPU from scheduling new work (PyTorch ops) on the GPU. The PyTorch Tuning Guide makes some basic suggestions about how to avoid CPU-GPU synchronizations, e.g.¬†by avoiding calls to tensor.item(), printing a device tensor, or calling tensor.cpu() / tensor.cuda(), but doesn‚Äôt explain in depth what is happening under the hood. Furthermore, there are other subtle ways to run into CPU-GPU synchronizations. In this post I dive deeper into synchronizations and explain how they slow down a PyTorch program. I show how to use the NVIDIA Nsight Systems profiler to identify and diagnose CPU-GPU synchronization issues. Finally, I also show how to use the experimental PyTorch API torch.cuda.set_sync_debug_mode in unit tests to verify the absence of CPU-GPU synchronizations in production code.\n\n\n\n\n\n\nFigure¬†1: Profile showing CPU-GPU synchronization issues (here triggered by printing a CUDA tensor). A deep dive into the profile details is in Section¬†4.\n\n\n\n\n\nWhat is a CPU-GPU Synchronization?\nTo fully take advantage of the speed of a GPU, it must be kept busy (high GPU utilization). This depends on the CPU scheduling enough work to keep the GPU busy. Typically, in a well-performing PyTorch program, the CPU schedules many instructions quickly, which are executed by the GPU. The CPU is said to run ahead of the GPU, because it issues instructions without waiting for the previous ones to complete. This is why PyTorch is said to be asynchronous. If the CPU fails to schedule enough work, the GPU will sit idle waiting for work.\nThe intuition: As an analogy to understand CPU-GPU synchronizations, think of a restaurant where a chef (CPU) schedules all the steps (PyTorch ops) for a big dinner party in advance. A CPU-GPU synchronization would be like the chef waiting to observe how a specific step in a specific dish turned out during the dinner, before scheduling the rest of the dishes and telling the cooks what to do. Obviously, this latency in communication will lead to the cooks being idle, waiting for the chef to schedule the dishes. Instead, the chef should schedule the entire dinner plan in advance (run ahead).\n\n\nExample\nLet‚Äôs look at a concrete example. Below we have a PyTorch program that executes a slow_operation() on the GPU, followed by a quick_operation() on the GPU. The quick operation just counts the number of 1s in the result tensor. The result of the quick operation is gathered in results tensor. Both functions are executed in a loop, to simulate a long-running program like LLM inference, where the slow operation could be the LLM forward pass, and the quick operation could be bookkeeping with the sampled tokens (a real vLLM example is explained in Section¬†7). To warm up the torch code, I run both the slow_operation() and quick_operation() once before the hot loop. I annotated regions of code with nvtx.annotate(), to keep track of GPU and CPU runtime during profiling.\n\n\n\nProfiling Analysis\nWe can run the NVIDIA Nsight Systems profiler with the command below to produce the timelines shown in Figure¬†2 and Figure¬†3. The script takes a flag --do-print to include a CPU-GPU synchronization in the quick operation: a print of a GPU tensor, which forces data back from the GPU to the CPU.\nWithout CPU-GPU synchronization:\nnsys profile \\\n  -o profile-no-print.nsys-rep \\\n  python cpu_gpu_sync_example.py\nWith CPU-GPU synchronization:\nnsys profile \\\n  -o profile-do-print.nsys-rep \\\n  python cpu_gpu_sync_example.py --do-print\nThe resulting profiles are shown below and annotated. I ran the code on an NVIDIA RTX3090 GPU and Nsight Systems 2025.3.1.90.\nTo inspect the report:\n\nOpen the .nsys-rep in the Nsight Systems UI\nZoom into the NVTX-annotated region.\nLook for long CPU-side CUDA API calls like cudaStreamSynchronize and correlate them with gaps in GPU utilization.\n\n\nRun With CPU-GPU Synchronization\n\n\n\n\n\n\nFigure¬†2: Profile with CPU-GPU synchronization.\n\n\n\n\nWe see gaps in the GPU utilization (first blue bar from top to bottom). These gaps indicate that the GPU has idle times and waits for work.\nWe observe that the runtimes of the GPU and CPU are similarly long, as seen both timelines being equal in length in the horizontal axis. In a well-performing program, the CPU runtime should be shorter than the GPU runtime (CPU runs ahead of the GPU).\nIn the CPU ops timeline, we see long green bars (cudaStreamSynchronize), which mean that the CPU is blocked waiting for the GPU to return some data.\nOn the GPU ops timeline we observe that the slow operation takes longer than the quick operation, which just means that the quick operation is the one blocking the CPU.\n\n\n\nHealthy Run\n\n\n\n\n\n\nFigure¬†3: Profile without CPU-GPU synchronization.\n\n\n\n\nWe observe that the CPU runs ahead of the GPU, so it dispatches all the work quickly and finishes way before the GPU.\nThe GPU utilization does not have any gaps. The blue line showing utilization is continuous.\nIn the GPU ops timeline, the slow operation is slower than the quick operation.\nThe full region interleaved-code runs faster than with the CPU-GPU synchronization (4.434 ms vs 4.827 ms).\n\n\n\n\nDynamic Shapes Can Trigger Synchronizations\nBesides the code mentioned in the PyTorch Tuning Guide there is another common pattern that triggers CPU-GPU synchronizations, namely dynamic shapes. One example of dynamic shapes is boolean indexing x = t[bool_mask], where bool_mask is a boolean tensor on GPU. The size of the tensor x cannot be determined by the CPU alone, because it depends on the number of true values in bool_mask. Therefore, PyTorch typically needs to fetch data from the GPU to determine the amount of memory to allocate for x. This creates a CPU-GPU synchronization.\nAnother example of this same problem is slicing x = t[:gpu_index], where gpu_index is a scalar integer tensor on GPU. Once again, the size of the tensor x cannot be determined by the CPU alone, because it depends on the value of gpu_index.\nThere are other ways to slice a torch tensor, e.g.¬†x = t.narrow(dim=0, start, length) (link). However, this operation requires length to be a Python int. Passing a GPU tensor as length will typically trigger a cast to Python int on CPU, which introduces a CPU-GPU synchronization.\nWhat all these synchronization triggers have in common is that the length of the resulting tensor depends on data residing on the GPU. This is why dynamic shapes are so problematic. Instead, if the length of the resulting tensor can be known statically on the CPU side, then we can find a way to avoid the CPU-GPU synchronization. PyTorch might not provide the precise API you need to avoid the synchronization. In that case, you might want to write a custom kernel in Triton to avoid the synchronization. The advantage of doing this is that you can fuse many sequential PyTorch operations into a single Triton kernel, which reduces the overhead of dispatching many small kernels on the CPU-side (around 2¬µs to 3¬µs per kernel). A concrete example is explained in Section¬†7. The accompanying Github Gist also uses Triton kernels to fuse together PyTorch ops, and prevent CPU-GPU synchronizations.\n\n\nUnit Testing for CPU-GPU Synchronizations\nSo we need to prevent synchronization in our code. How can we do this without running a profiler on each code section we are interested in? PyTorch provides an experimental feature that can raise warnings or errors when CPU-GPU synchronizations occur: The torch.cuda.set_sync_debug_mode() function (link). Since it raises errors immediately, it can be used in unit tests to verify that the code is free of CPU-GPU synchronizations. For example it can be used in a context manager:\nimport functools\nfrom contextlib import contextmanager\n\nimport torch\n\n@contextmanager\ndef fail_if_gpu_cpu_synchronization(fail: bool):\n    \"\"\"Within this context, GPU-CPU synchronization raises an error if `fail` is True.\"\"\"\n\n    new_mode = 2 if fail else 0\n    old_mode = torch.cuda.get_sync_debug_mode()\n    torch.cuda.set_sync_debug_mode(new_mode)\n    try:\n        yield\n    finally:\n        torch.cuda.set_sync_debug_mode(old_mode)\n\n\nx = torch.arange(10, device=\"cuda\")\nwith fail_if_gpu_cpu_synchronization(fail=True):\n    print(x)  # Raises an error\nThe context manager can be used in a decorator as well, to decorate functions that should fail if they contain CPU-GPU synchronizations.\ndef on_gpu_cpu_synchronization(fail: bool):\n    \"\"\"\n    Wrap a function to raise an error on GPU-CPU synchronization if `fail` is True.\n    \"\"\"\n\n    def decorator(fn):\n        @functools.wraps(fn)\n        def wrapper(*args, **kwargs):\n            with fail_if_gpu_cpu_synchronization(fail):\n                return fn(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\n@on_gpu_cpu_synchronization(fail=True)\ndef test_should_not_sync():\n    x = torch.arange(10, device=\"cuda\")\n    y = x ** 2\nNote that the decorator is used only on test functions, meaning that the PyTorch API to raise synchronization errors is never applied to production code, but only to the test code. This keeps this experimental feature isolated from production code while still providing quick and valuable feedback on CPU-GPU synchronizations. I used this pattern to verify the absence of synchronizations in my Github Gist about vLLM TokenGroup. The mechanism itself of raising errors is also unit tested in the test_helpers.py file in the Github Gist.\n\n\n\n\n\n\nWarning\n\n\n\nThis PyTorch API is experimental as of today, and does not cover all CPU-GPU synchronizations. For example, it does not cover the torch.distributed and torch.sparse namespaces (see docs).\n\n\n\n\nIn Practice: vLLM\nI encountered CPU-GPU synchronizations in the context of contributing speculative decoding to vLLM (PR #24322). In particular, the LLM forward passes are heavy operations, and preparing the input tokens for the forward pass requires lots of small PyTorch operations, which can introduce CPU-GPU synchronizations, if not carefully written. Senior NVIDIA engineer Benjamin Chislett helped me understand what causes the synchronizations, in particular dynamic shapes, and the value of writing custom kernels in Triton to fuse together multiple sequential PyTorch operations (example: PR #28597). I thank him for his support and feedback! üí™\n\n\nSummary\nIn this post, I explained what CPU-GPU synchronizations are, and how to identify and diagnose them with the NVIDIA Nsight Systems profiler. I discussed dynamic shapes as a common trigger for CPU-GPU synchronizations, and the value of writing custom kernels in Triton to fuse together multiple sequential PyTorch operations. I also showed how to use the experimental PyTorch API torch.cuda.set_sync_debug_mode in unit tests to verify the absence of CPU-GPU synchronizations in production code. This guide aims to help engineers prevent CPU-GPU synchronizations, which is key to write fast and efficient PyTorch programs.\n\n\nFurther References\nFor a deep dive into kernel benchmarking in practice, I recommend this YouTube lecture by NVIDIA engineering manager Georgii Evtushenko.\n\n\n\n\n\nCitationBibTeX citation:@online{ruiz2026,\n  author = {Ruiz, Tomas},\n  title = {PyTorch and {CPU-GPU} {Synchronizations}},\n  date = {2026-01-07},\n  url = {https://tomasruizt.github.io/posts/08_cpu_gpu_synchronization/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRuiz, Tomas. 2026. ‚ÄúPyTorch and CPU-GPU Synchronizations.‚Äù\nJanuary 7, 2026. https://tomasruizt.github.io/posts/08_cpu_gpu_synchronization/."
  },
  {
    "objectID": "posts/04_multimodal-attn/index.html",
    "href": "posts/04_multimodal-attn/index.html",
    "title": "Drilling Down into Multimodal Attention",
    "section": "",
    "text": "Visualizing Multimodal Attention Patterns"
  },
  {
    "objectID": "posts/04_multimodal-attn/index.html#paligemma2s-attention-patterns",
    "href": "posts/04_multimodal-attn/index.html#paligemma2s-attention-patterns",
    "title": "Drilling Down into Multimodal Attention",
    "section": "PaliGemma2‚Äôs Attention Patterns",
    "text": "PaliGemma2‚Äôs Attention Patterns\nWe now drill down into PaliGemma2‚Äôs attention patterns. When looking at the attention patterns, the first thing that jumps out is that the text tokens are not attending to the image tokens very much, because the image is almost completely white (even at zero attention, the image remains visible to prevent it from dissapearing completely). This effect is consistent across layers (See Figure¬†2, Figure¬†3, Figure¬†4). This is surprising, because the question can only be answered by attending to the image. How does then PaliGemma2 answer the question?\n\n\n\n\n\n\nFigure¬†2: Layer 0: Link to full visualization\n\n\n\n\n\n\n\n\n\nFigure¬†3: Layer 15: Link to full visualization Dark vertical bars, but first row (&lt;bos&gt; token) is white\n\n\n\n\n\n\n\n\n\nFigure¬†4: Layer 25: Link to full visualization\n\n\n\nIn the middle layers (layer 15), vertical bars are visible in almost every head. This indicates that most text tokens are attending to the &lt;bos&gt; token, which is the first token after the image tokens. Interestingly, the &lt;bos&gt; does not attend back to the other text tokens. We can tell because the first row of the colored pattern is completely white (close to 0 attention) in middle and late layers.\nSo what is the &lt;bos&gt; token attending to? Mostly to image tokens. To see this, I increase the contrast of the attention patterns using the slider and compare the attentions with different destination text tokens. The &lt;bos&gt; token is attending uniformly to many image tokens. The images below are all from intermediate layers (layer 15).\n\n\n\nThe &lt;bos&gt; token attends uniformly to many image tokens\n\n\n\n\n\nThe next text token attends sparsely to image tokens\n\n\n\n\n\nThe last text token also attends sparsely to image tokens, although more in patches.\n\n\nThis suggests a hypothesis: Namely that the visual information flows from the image tokens into the &lt;bos&gt; token, and then from the &lt;bos&gt; token to the rest of the text tokens. To quantify this, I partition the input into 3 regions: The image tokens, the &lt;bos&gt; token, and the rest of the text tokens. By summing up the attention of the tokens in each region, we get a measure of the attention between regions. This yields a 3x3 matrix, where each row sums up to 1.\n\n\n\n\n\n\nFigure¬†5: Self-attention over input regions. In lower layers, image tokens mostly attend to each other, despite having access to all tokens. Later, in middle and final layers, image tokens also attend to the &lt;bos&gt; token, (an example of information flowing back from text to image). The &lt;bos&gt; token increasingly attends to the image tokens as depth increases. Text tokens attend as much (or more) to the &lt;bos&gt;token as to the image tokens, despite their ratio being 1:256.\n\n\n\nThese numbers suggest that PaliGemma2 has trained the &lt;bos&gt; token to be a ‚Äúbroker‚Äù token for visual information: The &lt;bos&gt; token ‚Äúcollects‚Äù and aggregates visual information from the image tokens into a single place, and then ‚Äúserves‚Äù it back to text and image tokens. It plays a similar role as the [CLS] token in the ViT."
  },
  {
    "objectID": "posts/04_multimodal-attn/index.html#do-the-numbers-generalize",
    "href": "posts/04_multimodal-attn/index.html#do-the-numbers-generalize",
    "title": "Drilling Down into Multimodal Attention",
    "section": "Do the Numbers Generalize?",
    "text": "Do the Numbers Generalize?\nTo test if the hypothesis holds in general for (image, text) pairs other than the example of the dog with the frisbee, I ran the analysis on the first 1000 distinct images from the VQA dataset (train) and their corresponding questions. The dataset has multiple questions per image, but I used only the first question so as to have the most visual variation within the 1000 samples.\n\n\n\nVQA images, questions, and model answers. The images are diverse real-life images, and the questions are short open- and closed-ended questions. Note that the questions are formatted in the PaliGemma2 QA format (‚ÄúAnswer en ‚Äù).\n\n\nI computed the self-attention matrix over regions for each (image, question) pair and computed the average and the standard deviation over the 1000 pairs. We observe that the standard deviations are very small, indicating that the ‚Äúbroker‚Äù role of the &lt;bos&gt; token is robust and independent of the image and question.\n\n\n\nSelf-attention over input regions for 1000 VQA samples. The standard deviations are small. The numbers for previous example (purple frisbee) in Figure¬†5 are mostly within the 1-\\(\\sigma\\) confidence interval here, suggesting it is a typical example."
  },
  {
    "objectID": "posts/04_multimodal-attn/index.html#acknowledgement",
    "href": "posts/04_multimodal-attn/index.html#acknowledgement",
    "title": "Drilling Down into Multimodal Attention",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis is the final project for the course ‚ÄúArtificial Intelligence Safety Fundamentals‚Äù (AISF) by BlueDot Impact. The author is funded by the Bavarian Research Institute for Digital Transformation (bidt) and the Ludwig Maximilian University of Munich."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "PyTorch and CPU-GPU Synchronizations\n\n\nWriting Fast PyTorch Code\n\n\n\nGPUs\n\n\nPyTorch\n\n\nTriton\n\n\n\n\n\n\n\n\n\nJan 7, 2026\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nA Guide to Classify 35,000 Videos with a 72B Multimodal LLM on the vLLM Engine\n\n\n\n\n\n\nGPUs\n\n\nvLLM\n\n\nTransformers\n\n\n\n\n\n\n\n\n\nSep 22, 2025\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nDrilling Down into Multimodal Attention\n\n\n\n\n\n\nTransformers\n\n\nAttention\n\n\n\n\n\n\n\n\n\nFeb 1, 2025\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nHow Does Tiling Speed Up Matrix Multiplications on GPUs?\n\n\n\n\n\n\nMathematics\n\n\nGPUs\n\n\n\n\n\n\n\n\n\nDec 23, 2024\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nGrokking an Inner Product Inequality With Python on WebAssembly\n\n\n\n\n\n\nMathematics\n\n\nPython\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nA Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nAug 2, 2024\n\n\nTomas Ruiz\n\n\n\n\n\n\nNo matching items"
  }
]