[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Drilling Down into Multimodal Attention\n\n\n\n\n\n\nTransformers\n\n\nAttention\n\n\n\n\n\n\n\n\n\nFeb 1, 2025\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nHow Does Tiling Speed Up Matrix Multiplications on GPUs?\n\n\n\n\n\n\nMathematics\n\n\nGPUs\n\n\n\n\n\n\n\n\n\nDec 23, 2024\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nGrokking an Inner Product Inequality With Python on WebAssembly\n\n\n\n\n\n\nMathematics\n\n\nPython\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nA Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nAug 2, 2024\n\n\nTomas Ruiz\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "",
    "text": "The purpose of this post is two-fold:\n\nTo showcase Python running directly in your browser without any server behind it, like JavaScript. I will even import libraries like numpy and matplotlib. The underlying technologies that power this are WebAssembly and Pyodide, which I encourage you to check out.\nTo get you excited about a simple inequality and its application to vectors, functions & matrices. These are our objects of study, and we will inspect and visualize them interactively using Python.\n\nI created this document using Quarto Live. Big thanks to Renata Topinkova for showing me this tool!"
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html#summary",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html#summary",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "",
    "text": "The purpose of this post is two-fold:\n\nTo showcase Python running directly in your browser without any server behind it, like JavaScript. I will even import libraries like numpy and matplotlib. The underlying technologies that power this are WebAssembly and Pyodide, which I encourage you to check out.\nTo get you excited about a simple inequality and its application to vectors, functions & matrices. These are our objects of study, and we will inspect and visualize them interactively using Python.\n\nI created this document using Quarto Live. Big thanks to Renata Topinkova for showing me this tool!"
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html#the-inequality",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html#the-inequality",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "The Inequality",
    "text": "The Inequality\nMichael Steele presents in his book this simple inequality based only on the fact that \\((x-y)^2\\) is always positive1:\n\\[\n\\begin{align}\n0 &\\leq  (x - y)^2 \\\\\n\\implies 0 &\\leq x^2 -2xy +y^2 \\\\\n‚üπ xy &‚â§ \\frac{1}{2} (x^2 + y^2)\n\\end{align}\n\\]\nThe last inequality above is not intuitively obvious to me. I‚Äôm the kind of person that likes numerical proof to internalize these results, and this is where Python comes in handy. Change the variables x and y in the code below to see if the inequality holds.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis Python code is running in your browser. There is no juypter notebook, nor any deployment or any client-server communication behind it! ü§ØüöÄ"
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html#generalizing-to-vectors",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html#generalizing-to-vectors",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Generalizing to Vectors",
    "text": "Generalizing to Vectors\nWhat happens when we apply this inequality to more than just scalars? It also applies to sequences of numbers, e.g: \\[\nx_1 y_1 + x_2 y_2 &lt;= \\frac{1}{2} (x_1^2 + x_2^2) + \\frac{1}{2} (y_1^2 + y_2^2)\n\\] You might recognize that this is equivalent to an inner product: \\[ x^T y ‚â§ \\frac{1}{2} (x^T x + y^T y)\\] where \\(x = [x_1, \\dots, x_n]\\) and \\(y = [y_1, \\dots, y_n]\\).\nThe inequality is asserting that the vector product \\(x^Ty\\) of any two vectors \\(x\\) and \\(y\\) has an upper bound given by the average of \\(x^Tx\\) and \\(y^Ty\\).\nOnce again, I‚Äôm not intuitively convinced until I see code running. Notice how we import numpy, which calls compiled C routines under the hood (but our runtime is the browser now)."
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html#generalizing-to-functions",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html#generalizing-to-functions",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Generalizing to Functions",
    "text": "Generalizing to Functions\nYou might have heard that functions are infinite-dimensional vectors (ü§Ø). In that case, the inequality also applies! But how does the inner product \\(x^Ty\\) for two functions look like?\nThe convention is to use the bracket notation \\(‚ü® x, y ‚ü©\\) rather than \\(x^Ty\\). To sum over the infinite individual entries of the (function) vector, we use the integral:\n\\[‚ü® f, g ‚ü© = \\int f(x) g(x) dx\\] Using this definition, the inequality holds for functions as well: \\[\n\\begin{align}\n‚ü® f, g ‚ü© &‚â§ \\frac{1}{2} (‚ü® f, f ‚ü© + ‚ü® g, g ‚ü©) \\\\\n&= \\frac{1}{2} \\left( \\int f(x)^2 dx + \\int g(x)^2 dx \\right)\n\\end{align}\n\\]\nLet‚Äôs take two concrete functions \\(f(x) = \\cos(x)\\) and \\(g(x) = \\sin(4x)\\). I choose these arbitrarily because plotting them looks nice. Feel free to use different functions f and g in the code.\n\n\n\n\n\n\n\nIn the plot above, the individual functions \\(f^2\\) and \\(g^2\\) are plotted with light-blue lines. Their average is the red line, and the product \\(f ‚ãÖ g\\) is the green line. The red line is an upper bound for the green one. We see that the green line crosses over the two blue lines at different points but never crosses over the red line.\nAbout the integral: Perhaps you noticed that I formulated the inequality on inner-products, but I‚Äôm plotting the functions pointwise. The missing step is the integral, which is evaluated in Python using the numpy function np.trapz(). As we can confirm below, the inequality holds:"
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html#generalizing-to-matrices",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html#generalizing-to-matrices",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Generalizing to Matrices",
    "text": "Generalizing to Matrices\nWill the inequality also apply to matrices? The inner product of two matrices \\(A\\) and \\(B\\) (also called Frobenius inner product) is defined as: \\[‚ü®A, B‚ü© = \\text{tr}(A^T B)\\] where \\(\\text{tr}\\) is the trace operator.\n\n\n\n\n\n\nWarning\n\n\n\nBeware that this inner product is different from matrix multiplication \\[‚ü®A, B‚ü© = tr(A^T B) ‚â† AB\\]\n\n\nThe inequality for matrices then reads:\n\\[tr(A^T B) ‚â§ \\frac{1}{2} (tr(A^T A) + tr(B^T B))\\]\nIt‚Äôs easy to convince yourself that the inequality holds for matrices by writing down the trace as a sum of scalars. As we verified before, inequality holds for each scalar in the sum:\n\\[\n\\begin{align}\ntr(A^T B)\n&= \\sum_{i,j} A_{ij} B_{ij} & \\text{(definition)}\\\\\n&‚â§ \\frac{1}{2} \\left( \\sum_{i,j} A_{ij}^2 + \\sum_{i,j} B_{ij}^2 \\right) &\\text{(applied by scalar)}\\\\\n&= \\frac{1}{2} (tr(A^T A) + tr(B^T B))\n\\end{align}\n\\]\nLet‚Äôs check the inequality with random matrices. You can use the \"Start Over\" button to re-run the code with new matrices.\n\n\n\n\n\n\nThe inequality holds, but I have no geometric intuition about the trace of a matrix, or how this inequality could be visualized for matrices. If you have an idea, please let me know! üôè"
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html#further-sources",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html#further-sources",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Further Sources",
    "text": "Further Sources\nIf you found the mathematics interesting, particularly the generalization of inner products, I recommend MIT‚Äôs course Matrix Calculus for Machine Learning and Beyond, which covers this topic in more detail, and goes much further üòÑ."
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html#footnotes",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html#footnotes",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúThe Cauchy-Schwarz Master Class: An Introduction to the Art of Mathematical Inequalities‚Äù by J. Michael Steele.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/linear-adaptation/index.html",
    "href": "posts/linear-adaptation/index.html",
    "title": "A Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification",
    "section": "",
    "text": "In this post I show how to linearly fine-tune a large language model (LLM) using a closed-form solution, based on the Moore-Penrose Inverse. I will focus on the special case of binary classification because the changes in output are easier to interpret. The new linear transformation \\(W\\) is shown in figure 1 (green).\n\n\n\n\n\n\nFigure¬†1: The learned transformation W (green) is applied in parallel to the existing linear layer (blue), preserving the existing knowledge of the model. Both are the summed to make the logits, which are passed to the softmax function."
  },
  {
    "objectID": "posts/linear-adaptation/index.html#references",
    "href": "posts/linear-adaptation/index.html#references",
    "title": "A Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification",
    "section": "References",
    "text": "References\n\n\nChowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al. 2023. ‚ÄúPalm: Scaling Language Modeling with Pathways.‚Äù Journal of Machine Learning Research 24 (240): 1‚Äì113.\n\n\nDutta, Abhinav, Sanjeev Krishnan, Nipun Kwatra, and Ramachandran Ramjee. 2024. ‚ÄúAccuracy Is Not All You Need.‚Äù arXiv Preprint arXiv:2407.09141.\n\n\nHu, Edward J, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. ‚ÄúLora: Low-Rank Adaptation of Large Language Models.‚Äù arXiv Preprint arXiv:2106.09685.\n\n\nWatkins, David S. 2004. Fundamentals of Matrix Computations. John Wiley & Sons."
  },
  {
    "objectID": "posts/linear-adaptation/index.html#footnotes",
    "href": "posts/linear-adaptation/index.html#footnotes",
    "title": "A Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n2024-09: The matrix \\(Z^T Z\\) is positive definite, so it is in theory efficiently invertible using the Cholesky decomposition. However, its condition number \\(\\kappa (Z^T Z)\\) is squarely proportional to the condition number of \\(\\kappa(Z)\\). This can lead to numerical instability when solving the linear system. In fact, I stumbled upon numerical instability while implementing this in linear system in PyTorch, which lead me to use an \\(L_2\\) regularization term. See Source.‚Ü©Ô∏é\n2024-09: It turns out that solving a linear system with \\(V\\) columns on the right-hand side can be done cheaper than in \\(V \\cdot \\frac{2}{3} d^3\\) flops. To solve \\(A X = B\\) (with \\(A \\in \\mathbb{R}^{d,d}\\) and \\(X, B \\in \\mathbb{R}^{d,V}\\)) the factorization of \\(A\\) requires \\(\\frac{2}{3} d^3\\) flops, but it only has to be done once. After that, solving for each of the \\(V\\) columns of \\(B\\) costs \\(2 d^2\\) flops each. So the total flop count is \\(\\frac{2}{3} d^3 + V \\cdot 2 d^2\\). This is a significant improvement over the naive approach. See (Watkins 2004, 77‚Äì78).‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/multimodal-attn/index.html",
    "href": "posts/multimodal-attn/index.html",
    "title": "Drilling Down into Multimodal Attention",
    "section": "",
    "text": "Visualizing Multimodal Attention Patterns"
  },
  {
    "objectID": "posts/multimodal-attn/index.html#paligemma2s-attention-patterns",
    "href": "posts/multimodal-attn/index.html#paligemma2s-attention-patterns",
    "title": "Drilling Down into Multimodal Attention",
    "section": "PaliGemma2‚Äôs Attention Patterns",
    "text": "PaliGemma2‚Äôs Attention Patterns\nWe now drill down into PaliGemma2‚Äôs attention patterns. When looking at the attention patterns, the first thing that jumps out is that the text tokens are not attending to the image tokens very much, because the image is almost completely white (even at zero attention, the image remains visible to prevent it from dissapearing completely). This effect is consistent across layers (See Figure¬†2, Figure¬†3, Figure¬†4). This is surprising, because the question can only be answered by attending to the image. How does then PaliGemma2 answer the question?\n\n\n\n\n\n\nFigure¬†2: Layer 0: Link to full visualization\n\n\n\n\n\n\n\n\n\nFigure¬†3: Layer 15: Link to full visualization Dark vertical bars, but first row (&lt;bos&gt; token) is white\n\n\n\n\n\n\n\n\n\nFigure¬†4: Layer 25: Link to full visualization\n\n\n\nIn the middle layers (layer 15), vertical bars are visible in almost every head. This indicates that most text tokens are attending to the &lt;bos&gt; token, which is the first token after the image tokens. Interestingly, the &lt;bos&gt; does not attend back to the other text tokens. We can tell because the first row of the colored pattern is completely white (close to 0 attention) in middle and late layers.\nSo what is the &lt;bos&gt; token attending to? Mostly to image tokens. To see this, I increase the contrast of the attention patterns using the slider and compare the attentions with different destination text tokens. The &lt;bos&gt; token is attending uniformly to many image tokens. The images below are all from intermediate layers (layer 15).\n\n\n\nThe &lt;bos&gt; token attends uniformly to many image tokens\n\n\n\n\n\nThe next text token attends sparsely to image tokens\n\n\n\n\n\nThe last text token also attends sparsely to image tokens, although more in patches.\n\n\nThis suggests a hypothesis: Namely that the visual information flows from the image tokens into the &lt;bos&gt; token, and then from the &lt;bos&gt; token to the rest of the text tokens. To quantify this, I partition the input into 3 regions: The image tokens, the &lt;bos&gt; token, and the rest of the text tokens. By summing up the attention of the tokens in each region, we get a measure of the attention between regions. This yields a 3x3 matrix, where each row sums up to 1.\n\n\n\n\n\n\nFigure¬†5: Self-attention over input regions. In lower layers, image tokens mostly attend to each other, despite having access to all tokens. Later, in middle and final layers, image tokens also attend to the &lt;bos&gt; token, (an example of information flowing back from text to image). The &lt;bos&gt; token increasingly attends to the image tokens as depth increases. Text tokens attend as much (or more) to the &lt;bos&gt;token as to the image tokens, despite their ratio being 1:256.\n\n\n\nThese numbers suggest that PaliGemma2 has trained the &lt;bos&gt; token to be a ‚Äúbroker‚Äù token for visual information: The &lt;bos&gt; token ‚Äúcollects‚Äù and aggregates visual information from the image tokens into a single place, and then ‚Äúserves‚Äù it back to text and image tokens. It plays a similar role as the [CLS] token in the ViT."
  },
  {
    "objectID": "posts/multimodal-attn/index.html#do-the-numbers-generalize",
    "href": "posts/multimodal-attn/index.html#do-the-numbers-generalize",
    "title": "Drilling Down into Multimodal Attention",
    "section": "Do the Numbers Generalize?",
    "text": "Do the Numbers Generalize?\nTo test if the hypothesis holds in general for (image, text) pairs other than the example of the dog with the frisbee, I ran the analysis on the first 1000 distinct images from the VQA dataset (train) and their corresponding questions. The dataset has multiple questions per image, but I used only the first question so as to have the most visual variation within the 1000 samples.\n\n\n\nVQA images, questions, and model answers. The images are diverse real-life images, and the questions are short open- and closed-ended questions. Note that the questions are formatted in the PaliGemma2 QA format (‚ÄúAnswer en ‚Äù).\n\n\nI computed the self-attention matrix over regions for each (image, question) pair and computed the average and the standard deviation over the 1000 pairs. We observe that the standard deviations are very small, indicating that the ‚Äúbroker‚Äù role of the &lt;bos&gt; token is robust and independent of the image and question.\n\n\n\nSelf-attention over input regions for 1000 VQA samples. The standard deviations are small. The numbers for previous example (purple frisbee) in Figure¬†5 are mostly within the 1-\\(\\sigma\\) confidence interval here, suggesting it is a typical example."
  },
  {
    "objectID": "posts/multimodal-attn/index.html#acknowledgement",
    "href": "posts/multimodal-attn/index.html#acknowledgement",
    "title": "Drilling Down into Multimodal Attention",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis is the final project for the course ‚ÄúArtificial Intelligence Safety Fundamentals‚Äù (AISF) by BlueDot Impact. The author is funded by the Bavarian Research Institute for Digital Transformation (bidt) and the Ludwig Maximilian University of Munich."
  },
  {
    "objectID": "posts/tiling-for-matrix-mult/index.html",
    "href": "posts/tiling-for-matrix-mult/index.html",
    "title": "How Does Tiling Speed Up Matrix Multiplications on GPUs?",
    "section": "",
    "text": "Tiled MatMul according to ChatGPT\nTL;DR: Tiling is a technique used to reduce the number of memory accesses performed during matrix multiplication. We see how it improves compute intensity and how it speeds up the matrix multiplication operation, not only on CPUs, but also on GPUs. I also provide a simple implementation of tiling in CUDA C."
  },
  {
    "objectID": "posts/tiling-for-matrix-mult/index.html#naive-memory-access",
    "href": "posts/tiling-for-matrix-mult/index.html#naive-memory-access",
    "title": "How Does Tiling Speed Up Matrix Multiplications on GPUs?",
    "section": "Naive Memory Access",
    "text": "Naive Memory Access\nIn the naive approach, computing \\(C_{ij}\\) requires fetching \\(n\\) elements from \\(A\\) (a row) and \\(n\\) elements from \\(B\\) (a column). That makes \\(2n\\) memory accesses. \\(C_{ij}\\) is computed as the dot product of the row and the column, which requires \\(2n\\) flops (1 mult and 1 add, \\(n\\) times). Thus, the compute intensity is: \\[\n\\text{Compute Intensity} = \\frac{\\text{flops}}{\\text{memory transfers}} = \\frac{2n}{2n} = 1\n\\]\nCan we do better? In this naive implementation, we are performing redundant memory transfers: For example, to compute \\(C_{11}\\), and \\(C_{12}\\), we fetch the first row of \\(A\\) twice from main memory."
  },
  {
    "objectID": "posts/tiling-for-matrix-mult/index.html#optimal-memory-access",
    "href": "posts/tiling-for-matrix-mult/index.html#optimal-memory-access",
    "title": "How Does Tiling Speed Up Matrix Multiplications on GPUs?",
    "section": "Optimal Memory Access",
    "text": "Optimal Memory Access\nIf our cache was (theoretically) large enough, we would transfer all elements of \\(A\\) and \\(B\\) into the cache at once (\\(2n^2\\) transfers) and perform the full matrix multiplication (\\(2n^3\\) flops).\n\\[\n\\text{Compute Intensity} = \\frac{\\text{flops}}{\\text{memory transfers}} = \\frac{2n^3}{2n^2} = n\n\\] The larger the matrices, the higher the compute intensity.\n\n\n\n\n\n\nExample\n\n\n\nAssuming a matrix size of \\(n=10,000\\), the naive approach does 1 flop per memory transfer, while the optimal approach does 10,000 flops per memory transfer. This would result in a 10,000x speedup, assuming memory bandwidth remained the bottleneck."
  },
  {
    "objectID": "posts/tiling-for-matrix-mult/index.html#a-middle-ground-tiling",
    "href": "posts/tiling-for-matrix-mult/index.html#a-middle-ground-tiling",
    "title": "How Does Tiling Speed Up Matrix Multiplications on GPUs?",
    "section": "A Middle Ground: Tiling",
    "text": "A Middle Ground: Tiling\nSince caching entire matrices is impractical, we divide matrices \\(A\\) and \\(B\\) into smaller square blocks of size \\(r\\), called tiles, perform block-wise multiplications, and aggregate the results.\nBut how does block-wise matrix multiplication work? Let‚Äôs go through an example to gain some intuition. We break the matrices \\(A\\), \\(B\\), and \\(C\\) into 4 blocks each (2x2). Each of these blocks has size \\(n/2\\).\n\\[\n\\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{bmatrix}\n\\begin{bmatrix}\nB_{11} & B_{12} \\\\\nB_{21} & B_{22}\n\\end{bmatrix} =\n\\begin{bmatrix}\nC_{11} & C_{12} \\\\\nC_{21} & C_{22}\n\\end{bmatrix}\n\\]\nTo compute an submatrix \\(C_{ij}\\), we multiply the corresponding blocks of \\(A\\) and \\(B\\) and sum the results. For example:\n\\[\nC_{11} = A_{11}B_{11} + A_{12}B_{21}\n\\]\nIn pseudocode this translates to the code below.\nn_blocks = n // r\nfor i in range(n_blocks):\n    for j in range(n_blocks):\n        for k in range(n_blocks):\n            C[i][j] += A[i][k] @ B[k][j]\nNote that the entry C[i][j] is now a block matrix rather than a scalar, and the @ operator denotes matrix multiplication, rather than scalar multiplication. Line 5 of the code above is loading blocks of size \\(r^2\\) from \\(A\\) and \\(B\\) into the cache, which takes \\(2r^2\\) memory transfers. Then, it multiplies the two blocks, which requires \\(2r^3\\) flops.\n\\[\n\\text{Compute Intensity} = \\frac{\\text{flops}}{\\text{memory transfers}} = \\frac{2r^3}{2r^2} = r\n\\]\n\n\n\n\n\n\nExample\n\n\n\nAssuming a block size of \\(r=100\\), the naive approach does 1 flop per memory transfer, while the tiling approach does 100 flops per memory transfer. This would result in a 100x speedup, assuming memory bandwidth remained the bottleneck.\n\n\nIt should be clear that \\(1 \\leq r \\leq n\\). Setting \\(r=1\\), we recover the naive approach, while setting \\(r=n\\) we recover the optimal approach. The table below compares the performance of the different methods. Note that for the tiled method, the flops and memory transfers are measured per block, rather than per matrix.\n\n\n\n\n\n\n\n\n\nMethod\nFlops\nMemory Transfers\nFlops/Memory Transfer\n\n\n\n\nNaive\n\\(2n^3\\)\n\\(2n \\cdot n^2\\)\n\\(\\frac{2n^3}{2n \\cdot n^2} = 1\\)\n\n\nTiling\n\\(2r^3\\)\n\\(2r^2\\)\n\\(\\frac{r^3}{r^2} = r\\)\n\n\nOptimal (Theoretical)\n\\(2n^3\\)\n\\(2n^2\\)\n\\(\\frac{n^3}{n^2} = n\\)"
  },
  {
    "objectID": "subscribe.html",
    "href": "subscribe.html",
    "title": "Subscribe",
    "section": "",
    "text": "To receive a notification when a new post is published, subscribe to the mailing list below.\n\n    \n\n\n  \n      \n          * indicates required\n          Email Address *\n      \n          \n          \n      \n  \n      /* real people should not fill this in and expect good things - do not remove this or risk form bot signups */"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a research assistant at the Ludwig-Maximilian-University of Munich within Prof.¬†Schwemmer‚Äôs Computational Social Science Lab. My research area is the intersection of Machine Learning and Social Media, particularly on multi-modal understanding.\n\n\nIn previous jobs, I have worked as a software engineer in different corporations (Amazon, Allianz, BMW) and Startups. The projects ranged from optimization algorithms to backend-engineering. You can find my CV on LinkedIn.\n\n\n\n\n2021 - Technical University of Munich - Master of Science - Computer Science\n2018 - Technical University of Munich - Bachelor of Science - Engineering Science"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "",
    "text": "In previous jobs, I have worked as a software engineer in different corporations (Amazon, Allianz, BMW) and Startups. The projects ranged from optimization algorithms to backend-engineering. You can find my CV on LinkedIn."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "",
    "text": "2021 - Technical University of Munich - Master of Science - Computer Science\n2018 - Technical University of Munich - Bachelor of Science - Engineering Science"
  }
]