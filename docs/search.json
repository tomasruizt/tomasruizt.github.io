[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a research assistant at the Ludwig-Maximilian-University of Munich within Prof.¬†Schwemmer‚Äôs Computational Social Science Lab. My research area is the intersection of Machine Learning and Social Media, particularly on multi-modal understanding.\n\n\nIn previous jobs, I have worked as a software engineer in different corporations (Amazon, Allianz, BMW) and Startups. The projects ranged from optimization algorithms to backend-engineering. You can find my CV on LinkedIn.\n\n\n\n\n2021 - Technical University of Munich - Master of Science - Computer Science\n2018 - Technical University of Munich - Bachelor of Science - Engineering Science"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "",
    "text": "In previous jobs, I have worked as a software engineer in different corporations (Amazon, Allianz, BMW) and Startups. The projects ranged from optimization algorithms to backend-engineering. You can find my CV on LinkedIn."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "",
    "text": "2021 - Technical University of Munich - Master of Science - Computer Science\n2018 - Technical University of Munich - Bachelor of Science - Engineering Science"
  },
  {
    "objectID": "subscribe.html",
    "href": "subscribe.html",
    "title": "Subscribe",
    "section": "",
    "text": "To receive a notification when a new post is published, subscribe to the mailing list below.\n\n    \n\n\n  \n      \n          * indicates required\n          Email Address *\n      \n          \n          \n      \n  \n      /* real people should not fill this in and expect good things - do not remove this or risk form bot signups */"
  },
  {
    "objectID": "posts/01_linear-adaptation/index.html",
    "href": "posts/01_linear-adaptation/index.html",
    "title": "A Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification",
    "section": "",
    "text": "In this post I show how to linearly fine-tune a large language model (LLM) using a closed-form solution, based on the Moore-Penrose Inverse. I will focus on the special case of binary classification because the changes in output are easier to interpret. The new linear transformation \\(W\\) is shown in figure 1 (green).\n\n\n\n\n\n\nFigure¬†1: The learned transformation W (green) is applied in parallel to the existing linear layer (blue), preserving the existing knowledge of the model. Both are the summed to make the logits, which are passed to the softmax function."
  },
  {
    "objectID": "posts/01_linear-adaptation/index.html#references",
    "href": "posts/01_linear-adaptation/index.html#references",
    "title": "A Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification",
    "section": "References",
    "text": "References\n\n\nChowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al. 2023. ‚ÄúPalm: Scaling Language Modeling with Pathways.‚Äù Journal of Machine Learning Research 24 (240): 1‚Äì113.\n\n\nDutta, Abhinav, Sanjeev Krishnan, Nipun Kwatra, and Ramachandran Ramjee. 2024. ‚ÄúAccuracy Is Not All You Need.‚Äù arXiv Preprint arXiv:2407.09141.\n\n\nHu, Edward J, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. ‚ÄúLora: Low-Rank Adaptation of Large Language Models.‚Äù arXiv Preprint arXiv:2106.09685.\n\n\nWatkins, David S. 2004. Fundamentals of Matrix Computations. John Wiley & Sons."
  },
  {
    "objectID": "posts/01_linear-adaptation/index.html#footnotes",
    "href": "posts/01_linear-adaptation/index.html#footnotes",
    "title": "A Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n2024-09: The matrix \\(Z^T Z\\) is positive definite, so it is in theory efficiently invertible using the Cholesky decomposition. However, its condition number \\(\\kappa (Z^T Z)\\) is squarely proportional to the condition number of \\(\\kappa(Z)\\). This can lead to numerical instability when solving the linear system. In fact, I stumbled upon numerical instability while implementing this in linear system in PyTorch, which lead me to use an \\(L_2\\) regularization term. See Source.‚Ü©Ô∏é\n2024-09: It turns out that solving a linear system with \\(V\\) columns on the right-hand side can be done cheaper than in \\(V \\cdot \\frac{2}{3} d^3\\) flops. To solve \\(A X = B\\) (with \\(A \\in \\mathbb{R}^{d,d}\\) and \\(X, B \\in \\mathbb{R}^{d,V}\\)) the factorization of \\(A\\) requires \\(\\frac{2}{3} d^3\\) flops, but it only has to be done once. After that, solving for each of the \\(V\\) columns of \\(B\\) costs \\(2 d^2\\) flops each. So the total flop count is \\(\\frac{2}{3} d^3 + V \\cdot 2 d^2\\). This is a significant improvement over the naive approach. See (Watkins 2004, 77‚Äì78).‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/05_mm-inference-on-vllm/index.html",
    "href": "posts/05_mm-inference-on-vllm/index.html",
    "title": "A Guide to Classify 35,000 Videos with a 72B Multimodal LLM on the vLLM Engine",
    "section": "",
    "text": "In this post, I describe how we used the vLLM inference engine to classify 35k videos collected from TikTok for a research project. I share lessons learned about computing on SLURM, parallelism strategies (tensor- and data-parallelism), and practical tips for fault tolerance and video inference with vLLM. I also dive into concrete token throughput statistics of our workload, which is extremely prefill-heavy. Finally, I show how we corrected for the bias of the Qwen2.5-VL-72B model to make statistically valid statements about our dataset. As a bonus, I also share Python code for video classification using the vLLM library in Section¬†13, as well as the SLURM file for our workload in Section¬†12."
  },
  {
    "objectID": "posts/05_mm-inference-on-vllm/index.html#preventing-clashes",
    "href": "posts/05_mm-inference-on-vllm/index.html#preventing-clashes",
    "title": "A Guide to Classify 35,000 Videos with a 72B Multimodal LLM on the vLLM Engine",
    "section": "Preventing Clashes",
    "text": "Preventing Clashes\nMultiple model instances might be co-located on the same node, e.g., an 8 GPU node can host 2√ó4 GPU jobs. To prevent the jobs from clashing over the same vLLM port, we set different ports for each model instance based on its rank (SLURM_ARRAY_TASK_ID). We also gave each model instance a unique enroot container name based on the SLURM_ARRAY_TASK_ID, to prevent co-located jobs from clashing over the same container name. Also, we deleted the created enroot container after the job finished. To diagnose the failure of an individual job task within a job array, it‚Äôs useful to pipe stderr and stdout of each job task to different files, as shown in lines 5 and 6 of our SLURM file in Section¬†12. It‚Äôs possible to restart individual job tasks within a job array with SLURM: sbatch &lt;slurm-file&gt; --array=&lt;failed-task-id&gt;."
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "",
    "text": "The purpose of this post is two-fold:\n\nTo showcase Python running directly in your browser without any server behind it, like JavaScript. I will even import libraries like numpy and matplotlib. The underlying technologies that power this are WebAssembly and Pyodide, which I encourage you to check out.\nTo get you excited about a simple inequality and its application to vectors, functions & matrices. These are our objects of study, and we will inspect and visualize them interactively using Python.\n\nI created this document using Quarto Live. Big thanks to Renata Topinkova for showing me this tool!"
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html#summary",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html#summary",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "",
    "text": "The purpose of this post is two-fold:\n\nTo showcase Python running directly in your browser without any server behind it, like JavaScript. I will even import libraries like numpy and matplotlib. The underlying technologies that power this are WebAssembly and Pyodide, which I encourage you to check out.\nTo get you excited about a simple inequality and its application to vectors, functions & matrices. These are our objects of study, and we will inspect and visualize them interactively using Python.\n\nI created this document using Quarto Live. Big thanks to Renata Topinkova for showing me this tool!"
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html#the-inequality",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html#the-inequality",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "The Inequality",
    "text": "The Inequality\nMichael Steele presents in his book this simple inequality based only on the fact that \\((x-y)^2\\) is always positive1:\n\\[\n\\begin{align}\n0 &\\leq  (x - y)^2 \\\\\n\\implies 0 &\\leq x^2 -2xy +y^2 \\\\\n‚üπ xy &‚â§ \\frac{1}{2} (x^2 + y^2)\n\\end{align}\n\\]\nThe last inequality above is not intuitively obvious to me. I‚Äôm the kind of person that likes numerical proof to internalize these results, and this is where Python comes in handy. Change the variables x and y in the code below to see if the inequality holds.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis Python code is running in your browser. There is no juypter notebook, nor any deployment or any client-server communication behind it! ü§ØüöÄ"
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html#generalizing-to-vectors",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html#generalizing-to-vectors",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Generalizing to Vectors",
    "text": "Generalizing to Vectors\nWhat happens when we apply this inequality to more than just scalars? It also applies to sequences of numbers, e.g: \\[\nx_1 y_1 + x_2 y_2 &lt;= \\frac{1}{2} (x_1^2 + x_2^2) + \\frac{1}{2} (y_1^2 + y_2^2)\n\\] You might recognize that this is equivalent to an inner product: \\[ x^T y ‚â§ \\frac{1}{2} (x^T x + y^T y)\\] where \\(x = [x_1, \\dots, x_n]\\) and \\(y = [y_1, \\dots, y_n]\\).\nThe inequality is asserting that the vector product \\(x^Ty\\) of any two vectors \\(x\\) and \\(y\\) has an upper bound given by the average of \\(x^Tx\\) and \\(y^Ty\\).\nOnce again, I‚Äôm not intuitively convinced until I see code running. Notice how we import numpy, which calls compiled C routines under the hood (but our runtime is the browser now)."
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html#generalizing-to-functions",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html#generalizing-to-functions",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Generalizing to Functions",
    "text": "Generalizing to Functions\nYou might have heard that functions are infinite-dimensional vectors (ü§Ø). In that case, the inequality also applies! But how does the inner product \\(x^Ty\\) for two functions look like?\nThe convention is to use the bracket notation \\(‚ü® x, y ‚ü©\\) rather than \\(x^Ty\\). To sum over the infinite individual entries of the (function) vector, we use the integral:\n\\[‚ü® f, g ‚ü© = \\int f(x) g(x) dx\\] Using this definition, the inequality holds for functions as well: \\[\n\\begin{align}\n‚ü® f, g ‚ü© &‚â§ \\frac{1}{2} (‚ü® f, f ‚ü© + ‚ü® g, g ‚ü©) \\\\\n&= \\frac{1}{2} \\left( \\int f(x)^2 dx + \\int g(x)^2 dx \\right)\n\\end{align}\n\\]\nLet‚Äôs take two concrete functions \\(f(x) = \\cos(x)\\) and \\(g(x) = \\sin(4x)\\). I choose these arbitrarily because plotting them looks nice. Feel free to use different functions f and g in the code.\n\n\n\n\n\n\n\nIn the plot above, the individual functions \\(f^2\\) and \\(g^2\\) are plotted with light-blue lines. Their average is the red line, and the product \\(f ‚ãÖ g\\) is the green line. The red line is an upper bound for the green one. We see that the green line crosses over the two blue lines at different points but never crosses over the red line.\nAbout the integral: Perhaps you noticed that I formulated the inequality on inner-products, but I‚Äôm plotting the functions pointwise. The missing step is the integral, which is evaluated in Python using the numpy function np.trapz(). As we can confirm below, the inequality holds:"
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html#generalizing-to-matrices",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html#generalizing-to-matrices",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Generalizing to Matrices",
    "text": "Generalizing to Matrices\nWill the inequality also apply to matrices? The inner product of two matrices \\(A\\) and \\(B\\) (also called Frobenius inner product) is defined as: \\[‚ü®A, B‚ü© = \\text{tr}(A^T B)\\] where \\(\\text{tr}\\) is the trace operator.\n\n\n\n\n\n\nWarning\n\n\n\nBeware that this inner product is different from matrix multiplication \\[‚ü®A, B‚ü© = tr(A^T B) ‚â† AB\\]\n\n\nThe inequality for matrices then reads:\n\\[tr(A^T B) ‚â§ \\frac{1}{2} (tr(A^T A) + tr(B^T B))\\]\nIt‚Äôs easy to convince yourself that the inequality holds for matrices by writing down the trace as a sum of scalars. As we verified before, inequality holds for each scalar in the sum:\n\\[\n\\begin{align}\ntr(A^T B)\n&= \\sum_{i,j} A_{ij} B_{ij} & \\text{(definition)}\\\\\n&‚â§ \\frac{1}{2} \\left( \\sum_{i,j} A_{ij}^2 + \\sum_{i,j} B_{ij}^2 \\right) &\\text{(applied by scalar)}\\\\\n&= \\frac{1}{2} (tr(A^T A) + tr(B^T B))\n\\end{align}\n\\]\nLet‚Äôs check the inequality with random matrices. You can use the \"Start Over\" button to re-run the code with new matrices.\n\n\n\n\n\n\nThe inequality holds, but I have no geometric intuition about the trace of a matrix, or how this inequality could be visualized for matrices. If you have an idea, please let me know! üôè"
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html#further-sources",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html#further-sources",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Further Sources",
    "text": "Further Sources\nIf you found the mathematics interesting, particularly the generalization of inner products, I recommend MIT‚Äôs course Matrix Calculus for Machine Learning and Beyond, which covers this topic in more detail, and goes much further üòÑ."
  },
  {
    "objectID": "posts/02_viz-inequalities-inner-prod-wasm/index.html#footnotes",
    "href": "posts/02_viz-inequalities-inner-prod-wasm/index.html#footnotes",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúThe Cauchy-Schwarz Master Class: An Introduction to the Art of Mathematical Inequalities‚Äù by J. Michael Steele.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/03_tiling-for-matrix-mult/index.html",
    "href": "posts/03_tiling-for-matrix-mult/index.html",
    "title": "How Does Tiling Speed Up Matrix Multiplications on GPUs?",
    "section": "",
    "text": "Tiled MatMul according to ChatGPT\nTL;DR: Tiling is a technique used to reduce the number of memory accesses performed during matrix multiplication. We see how it improves compute intensity and how it speeds up the matrix multiplication operation, not only on CPUs, but also on GPUs. I also provide a simple implementation of tiling in CUDA C."
  },
  {
    "objectID": "posts/03_tiling-for-matrix-mult/index.html#naive-memory-access",
    "href": "posts/03_tiling-for-matrix-mult/index.html#naive-memory-access",
    "title": "How Does Tiling Speed Up Matrix Multiplications on GPUs?",
    "section": "Naive Memory Access",
    "text": "Naive Memory Access\nIn the naive approach, computing \\(C_{ij}\\) requires fetching \\(n\\) elements from \\(A\\) (a row) and \\(n\\) elements from \\(B\\) (a column). That makes \\(2n\\) memory accesses. \\(C_{ij}\\) is computed as the dot product of the row and the column, which requires \\(2n\\) flops (1 mult and 1 add, \\(n\\) times). Thus, the compute intensity is: \\[\n\\text{Compute Intensity} = \\frac{\\text{flops}}{\\text{memory transfers}} = \\frac{2n}{2n} = 1\n\\]\nCan we do better? In this naive implementation, we are performing redundant memory transfers: For example, to compute \\(C_{11}\\), and \\(C_{12}\\), we fetch the first row of \\(A\\) twice from main memory."
  },
  {
    "objectID": "posts/03_tiling-for-matrix-mult/index.html#optimal-memory-access",
    "href": "posts/03_tiling-for-matrix-mult/index.html#optimal-memory-access",
    "title": "How Does Tiling Speed Up Matrix Multiplications on GPUs?",
    "section": "Optimal Memory Access",
    "text": "Optimal Memory Access\nIf our cache was (theoretically) large enough, we would transfer all elements of \\(A\\) and \\(B\\) into the cache at once (\\(2n^2\\) transfers) and perform the full matrix multiplication (\\(2n^3\\) flops).\n\\[\n\\text{Compute Intensity} = \\frac{\\text{flops}}{\\text{memory transfers}} = \\frac{2n^3}{2n^2} = n\n\\] The larger the matrices, the higher the compute intensity.\n\n\n\n\n\n\nExample\n\n\n\nAssuming a matrix size of \\(n=10,000\\), the naive approach does 1 flop per memory transfer, while the optimal approach does 10,000 flops per memory transfer. This would result in a 10,000x speedup, assuming memory bandwidth remained the bottleneck."
  },
  {
    "objectID": "posts/03_tiling-for-matrix-mult/index.html#a-middle-ground-tiling",
    "href": "posts/03_tiling-for-matrix-mult/index.html#a-middle-ground-tiling",
    "title": "How Does Tiling Speed Up Matrix Multiplications on GPUs?",
    "section": "A Middle Ground: Tiling",
    "text": "A Middle Ground: Tiling\nSince caching entire matrices is impractical, we divide matrices \\(A\\) and \\(B\\) into smaller square blocks of size \\(r\\), called tiles, perform block-wise multiplications, and aggregate the results.\nBut how does block-wise matrix multiplication work? Let‚Äôs go through an example to gain some intuition. We break the matrices \\(A\\), \\(B\\), and \\(C\\) into 4 blocks each (2x2). Each of these blocks has size \\(n/2\\).\n\\[\n\\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{bmatrix}\n\\begin{bmatrix}\nB_{11} & B_{12} \\\\\nB_{21} & B_{22}\n\\end{bmatrix} =\n\\begin{bmatrix}\nC_{11} & C_{12} \\\\\nC_{21} & C_{22}\n\\end{bmatrix}\n\\]\nTo compute an submatrix \\(C_{ij}\\), we multiply the corresponding blocks of \\(A\\) and \\(B\\) and sum the results. For example:\n\\[\nC_{11} = A_{11}B_{11} + A_{12}B_{21}\n\\]\nIn pseudocode this translates to the code below.\nn_blocks = n // r\nfor i in range(n_blocks):\n    for j in range(n_blocks):\n        for k in range(n_blocks):\n            C[i][j] += A[i][k] @ B[k][j]\nNote that the entry C[i][j] is now a block matrix rather than a scalar, and the @ operator denotes matrix multiplication, rather than scalar multiplication. Line 5 of the code above is loading blocks of size \\(r^2\\) from \\(A\\) and \\(B\\) into the cache, which takes \\(2r^2\\) memory transfers. Then, it multiplies the two blocks, which requires \\(2r^3\\) flops.\n\\[\n\\text{Compute Intensity} = \\frac{\\text{flops}}{\\text{memory transfers}} = \\frac{2r^3}{2r^2} = r\n\\]\n\n\n\n\n\n\nExample\n\n\n\nAssuming a block size of \\(r=100\\), the naive approach does 1 flop per memory transfer, while the tiling approach does 100 flops per memory transfer. This would result in a 100x speedup, assuming memory bandwidth remained the bottleneck.\n\n\nIt should be clear that \\(1 \\leq r \\leq n\\). Setting \\(r=1\\), we recover the naive approach, while setting \\(r=n\\) we recover the optimal approach. The table below compares the performance of the different methods. Note that for the tiled method, the flops and memory transfers are measured per block, rather than per matrix.\n\n\n\n\n\n\n\n\n\nMethod\nFlops\nMemory Transfers\nFlops/Memory Transfer\n\n\n\n\nNaive\n\\(2n^3\\)\n\\(2n \\cdot n^2\\)\n\\(\\frac{2n^3}{2n \\cdot n^2} = 1\\)\n\n\nTiling\n\\(2r^3\\)\n\\(2r^2\\)\n\\(\\frac{r^3}{r^2} = r\\)\n\n\nOptimal (Theoretical)\n\\(2n^3\\)\n\\(2n^2\\)\n\\(\\frac{n^3}{n^2} = n\\)"
  },
  {
    "objectID": "posts/06_vllm-spec-decode/index.html",
    "href": "posts/06_vllm-spec-decode/index.html",
    "title": "Up to 3.55x Faster: Contributing Speculative Decoding with Draft Models to vLLM V1",
    "section": "",
    "text": "I recently contributed speculative decoding with draft models to vLLM V1 (PR #24322). In this post, I benchmark the performance of my implementation (draft_model) and compare it against vanilla decoding and EAGLE-3, a modern speculative decoding technique supported by vLLM. My benchmarks show that draft_model can achieve speedups of up to 3.55√ó on InstructCoder, outperforming EAGLE-3, while requiring no specialized training. At the end, I share key learnings from working in the vLLM codebase and a few practical tips for approaching and navigating it."
  },
  {
    "objectID": "posts/06_vllm-spec-decode/index.html#background",
    "href": "posts/06_vllm-spec-decode/index.html#background",
    "title": "Up to 3.55x Faster: Contributing Speculative Decoding with Draft Models to vLLM V1",
    "section": "Background",
    "text": "Background\nSpeculative decoding is a technique that accelerates LLM inference by using a smaller ‚Äúdraft‚Äù model to predict multiple tokens ahead, which are then verified in parallel by the larger target model. This approach can significantly improve throughput, especially in memory-bound regimes with small batch sizes.\nA bit of history: Speculative decoding with draft models was previously available in vLLM V0. There‚Äôs an excellent GPU Mode lecture by Daniel Cade from June 2024 that outlines the original implementation. However, during the rearchitecting to V1, this feature was removed and hadn‚Äôt been reimplemented until now."
  },
  {
    "objectID": "posts/06_vllm-spec-decode/index.html#using-speculative-decoding",
    "href": "posts/06_vllm-spec-decode/index.html#using-speculative-decoding",
    "title": "Up to 3.55x Faster: Contributing Speculative Decoding with Draft Models to vLLM V1",
    "section": "Using Speculative Decoding",
    "text": "Using Speculative Decoding\nTo use speculative decoding with vLLM, simply pass additional arguments to the vllm serve command. You‚Äôll need to specify the draft model and the number of speculative tokens to generate. I also recommend setting --max-model-len to leave more memory for the KV cache.\nvllm serve Qwen/Qwen3-32B \\\n    --speculative_config.method=draft_model \\\n    --speculative_config.model=Qwen/Qwen3-1.7B \\\n    --speculative_config.num_speculative_tokens=4 \\\n    --speculative_config.max_model_len=5000 \\\n    --max-model-len 5000"
  },
  {
    "objectID": "posts/06_vllm-spec-decode/index.html#benchmarking-setup",
    "href": "posts/06_vllm-spec-decode/index.html#benchmarking-setup",
    "title": "Up to 3.55x Faster: Contributing Speculative Decoding with Draft Models to vLLM V1",
    "section": "Benchmarking Setup",
    "text": "Benchmarking Setup\n\nModels Evaluated\nI evaluated two model families: Qwen3 (dense) and LLaMa 3.3 70B. A summary of the models evaluated is shown in Table¬†1. For vanilla decoding, I evaluate the target model without any speculative decoding. Within the Qwen3 family, using the 0.6B model as a draft_model raised a runtime exception (likely a bug). The Qwen3 familiy was evaluated on a single NVIDIA H100 GPU, while the LlaMa 3.3 family required 2√ó H100 GPUs (tensor-parallelism).\n\n\n\nTable¬†1: Model configurations evaluated\n\n\n\n\n\nFamily\nTarget Model\nDraft Model(s)\nEAGLE-3 Model\nGPUs\n\n\n\n\nQwen3\nQwen3-32B1\nQwen3-1.7B2  Qwen3-4B3\nQwen3-32B-speculator.eagle34\n1√ó H100 96GB\n\n\nLLaMa-3.3\nLlama-3.3-70B-Instruct5\nLlama-3.2-1B6\nEAGLE3-LLaMA3.3-Instruct-70B7\n2√ó H100 80GB\n\n\n\n\n\n\n\n\nDatasets\nI evaluated performance on two datasets:\n\nMT-Bench8: A dataset of 80 multi-turn questions used to benchmark speedups in the EAGLE-3 paper by Li et al. (2025). This dataset is too small to test batch sizes larger than 80 without repeating prompts.\nInstructCoder9: A dataset of 114k prompts about programming tasks. This larger dataset enables testing at higher batch sizes (I benchmark up to 25610). Programming tasks typically contain substantial repetition between input and output tokens, which may be beneficial for speculative decoding.\n\n\n\nBenchmarking Methodology\nThe benchmarking code can be found on Github. All benchmark results are available in a single Parquet file for download.\nI benchmarked performance using vllm bench serve to inference metrics: token throughput (toks/s), TTFT, TPOT, and ITL. The speedup ratio metric is computed as the token throughput of speculative decoding over vanilla decoding. All experiments were run with temperature set to 0.0 (greedy sampling) because vLLM does not yet support lossless sampling with temperature &gt; 0.0. This feature has an open PR. I ran benchmarks across different batch sizes, since speculative decoding is known to be most effective in lower batch sizes. The batch size is implemented as concurrent requests sent to the server (--max-concurrency). I set --request-rate equal to --max-concurrency to prevent all prefills from happening at the exact same time, which would inflate the TTFT for vanilla decoding.\n\n\nExample Benchmarking Command\n\nvllm bench serve \\\n  --model Qwen/Qwen3-32B \\\n  --dataset-name hf \\\n  --dataset-path likaixin/InstructCoder \\\n  --max-concurrency 32 \\\n  --request-rate 32 \\\n  --num-prompts 320 \\\n  --temperature 0.0 \\\n  --top-p 1.0 \\\n  --ready-check-timeout-sec 600\nFor --num-prompts, I use 10 times --max-concurrency or at least 50 prompts.\n\n\n\nExample Benchmarking Output\n\n============ Serving Benchmark Result ============\nSuccessful requests:                     320       \nFailed requests:                         0         \nMaximum request concurrency:             32        \nBenchmark duration (s):                  47.20     \nTotal input tokens:                      48266     \nTotal generated tokens:                  63991     \nRequest throughput (req/s):              6.78      \nOutput token throughput (tok/s):         1355.72   \nPeak output token throughput (tok/s):    576.00    \nPeak concurrent requests:                56.00     \nTotal token throughput (tok/s):          2378.30   \n---------------Time to First Token----------------\nMean TTFT (ms):                          192.97    \nMedian TTFT (ms):                        140.48    \nP99 TTFT (ms):                           629.06    \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          21.75     \nMedian TPOT (ms):                        21.67     \nP99 TPOT (ms):                           26.55     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           61.50     \nMedian ITL (ms):                         58.23     \nP99 ITL (ms):                            114.37    \n---------------Speculative Decoding---------------\nAcceptance rate (%):                     46.13     \nAcceptance length:                       2.85      \nDrafts:                                  22519     \nDraft tokens:                            90076     \nAccepted tokens:                         41554     \nPer-position acceptance (%):\n  Position 0:                            71.44     \n  Position 1:                            49.82     \n  Position 2:                            36.32     \n  Position 3:                            26.96     \n==================================================\n\n\nHardware\nThe Qwen3 experiments were run on an NVIDIA H100 96GB GPU, within the SLURM cluster of the LRZ AI Systems, while the LLaMa 3.3 experiments were run on a 2√ó H100 80GB GPU launched from NVIDIA Brev."
  },
  {
    "objectID": "posts/06_vllm-spec-decode/index.html#results",
    "href": "posts/06_vllm-spec-decode/index.html#results",
    "title": "Up to 3.55x Faster: Contributing Speculative Decoding with Draft Models to vLLM V1",
    "section": "Results",
    "text": "Results\n\nVersus Vanilla Decoding\nI compare the token throughput of speculative decoding (draft_model) vs.¬†vanilla decoding on Figure¬†1. We observe that using speculative decoding dramatically increases token throughput compared to vanilla decoding. The number of speculative tokens \\(K\\) was chosen to maximize the speedup ratio. In the next section, we see what happens when we change \\(K\\).\n\nInstructCoder (LLaMa 70B)MT-Bench (Qwen3)InstructCoder (Qwen3)\n\n\n\n\n\n\n\n\nFigure¬†1: Token Throughput Metrics.\n\n\n\n\n\n\n\n\nToken Throughput Metrics.\n\n\n\n\n\n\n\nToken Throughput Metrics.\n\n\n\n\n\n\n\nOptimal Number of Speculative Tokens \\(K\\)\nThe optimal value of \\(K\\) depends on the acceptance rate of draft tokens, which varies with the data distribution. If \\(K\\) is set too high, the model wastes time predicting draft tokens that will be rejected. If \\(K\\) is set too low, we miss potential speedups.\nTo highlight this dynamic, I compared speedup ratios across different values of \\(K\\). The results are shown in Figure¬†2. The y-axis shows the speedup ratio (or simply speedup), which measures how much faster speculative decoding is compared to vanilla decoding (in wall-clock time). A speedup of 2√ó means speculative decoding finishes the workload in half the time of vanilla decoding.\n\nInstructCoder (LLaMa 70B)MT-Bench (Qwen3)InstructCoder (Qwen3)\n\n\n\n\n\n\n\n\nFigure¬†2: Throughput as a function of \\(K\\), evaluating both the 1.7 and the 4B models as draft models.\n\n\n\n\n\n\n\n\nThroughput as a function of \\(K\\), evaluating both the 1.7 and the 4B models as draft models.\n\n\n\n\n\n\n\nThroughput as a function of \\(K\\), evaluating both the 1.7 and the 4B models as draft models.\n\n\n\n\n\nThe results show several key patterns:\n\nMulti-GPU Setting: The greatest speedups (3.55√ó) were achieved with LLaMa 70B on 2√ó H100s. Surprisingly, inter-GPU communication overhead doesn‚Äôt dominate. draft_model works well with tensor parallelism &gt; 1.\nBatch Size Dependency: Speedups peak at small batch sizes (concurrency 1-4) and decay at larger ones. At batch size 128+, speculative decoding becomes slower than vanilla decoding. This aligns with the literature (Li et al. 2025; Tang et al. 2025): speculative decoding helps most in the memory-bound regime, but adds overhead in the compute-bound regime.\nSaturation Effect: Increasing \\(K\\) beyond a threshold yields no further gains. Saturation occurs around \\(K=3\\)-\\(4\\) for Qwen3 and \\(K=7\\) for LLaMa 70B.\nDraft Model Size Trade-off: The smaller 1.7B draft model achieves higher speedups than the 4B, despite the 4B having better acceptance rates (Figure¬†5). The 4B‚Äôs slower inference limits its overall speedup.\nLow \\(K\\) Robustness: In InstructCoder (Qwen3), \\(K=1\\) achieves modest speedups at small batch sizes, but degrades more gracefully at large batch sizes than higher \\(K\\) values.\n\n\n\nVersus EAGLE-3\nEAGLE-3 is a modern speculative decoding technique supported by vLLM that uses lightweight draft models trained specifically for speculative decoding. The EAGLE-3 paper by Li et al. (2025) reports speedup ratios on SGLang11 between 1.38√ó and 1.82√ó at batch sizes ranging from 2 to 64 (table 3), and slightly lower speedups on vLLM (table 5).\nMy own benchmark results in Figure¬†3 show greater speedups on vLLM: peaking at 2.94√ó for the LLaMa 70B family and 2.10√ó for the Qwen3 family. This confirms that EAGLE-3 works very well on vLLM and provides a strong baseline against which to compare my draft_model implementation.\n\nInstructCoder (LLaMa 70B)MT-Bench (Qwen3)InstructCoder (Qwen3)\n\n\n\n\n\n\n\n\nFigure¬†3: Speedup ratios of EAGLE-3\n\n\n\n\n\n\n\n\nSpeedup ratios of EAGLE-3\n\n\n\n\n\n\n\nSpeedup ratios of EAGLE-3\n\n\n\n\n\nFigure¬†4 compares the speedup ratios of both speculative decoding methods. I used \\(K=4\\) in the Qwen3 family and \\(K=7\\) in the LLaMa 70B family. The speedups of draft_model are surprisingly good, often exceeding those of EAGLE-3 by a significant margin. This is particularly visible on InstructCoder (LLaMa 70B setting) and MT-Bench (Qwen3 setting). Both methods have very similar speedups on InstructCoder (Qwen3 setting).\n\nInstructCoder (LLaMa 70B)MT-Bench (Qwen3)InstructCoder (Qwen3)\n\n\n\n\n\n\n\n\nFigure¬†4: Comparison of Speedups of Draft Model and EAGLE-3\n\n\n\n\n\n\n\n\nComparison of Speedups of Draft Model and EAGLE-3\n\n\n\n\n\n\n\nComparison of Speedups of Draft Model and EAGLE-3\n\n\n\n\n\nBoth EAGLE-3 and draft_model have very different Acceptance Lengths (AL), shown in Figure¬†5. The AL curve for EAGLE-3 is flatter, meaning that predicting more tokens leads to diminishing returns. In contrast, the AL curve for draft models continues to climb with more speculative tokens. The shaded area represents (min, max) ALs over different batch sizes12. Despite the EAGLE-3 draft models being lighter, the lower AL is a limiting factor that prevents it from reaching the same speedups as draft_model.\n\nInstructCoder (LLaMa 70B)MT-Bench (Qwen3)InstructCoder (Qwen3)\n\n\n\n\n\n\n\n\nFigure¬†5: Acceptance Lengths for draft_model and EAGLE-3.\n\n\n\n\n\n\n\n\nAcceptance Lengths for draft_model and EAGLE-3.\n\n\n\n\n\n\n\nAcceptance Lengths for draft_model and EAGLE-3.\n\n\n\n\n\n\nToken Throughput\nFigure¬†6 shows the Output Token Throughput (toks/s) metric for both speculative decoding methods and vanilla decoding (SD=None). Using a speculative decoding method outperforms vanilla decoding up to batch size 64. Beyond this point, the throughput of speculative decoding methods plateaus, while vanilla decoding continues to scale with larger batch sizes.\n\nInstructCoder (LLaMa 70B)MT-Bench (Qwen3)InstructCoder (Qwen3)\n\n\n\n\n\n\n\n\nFigure¬†6: Token Throughputs of all Methods\n\n\n\n\n\n\n\n\nToken Throughputs of all Methods\n\n\n\n\n\n\n\nToken Throughputs of all Methods\n\n\n\n\n\n\n\n\nInference Metrics\nThe vllm bench serve command reports several request-level inference metrics (lower is better for all):\n\nTTFT: Time to first token\nTPOT: Time per output token (excluding the first token)\nITL: Inter-token latency\n\nThe benchmark reports means, medians, and 99th percentiles for these metrics. Figure¬†7 shows the 99th percentile values for both speculative decoding methods and vanilla decoding.\n\nInstructCoder (LLaMa 70B)MT-Bench (Qwen3)InstructCoder (Qwen3)\n\n\n\n\n\n\n\n\nFigure¬†7: Inference Metrics. 99th percentile of TPOT, ITL, and TTFT. Lower is better.\n\n\n\n\n\n\n\n\nInference Metrics. 99th percentile of TPOT, ITL, and TTFT. Lower is better.\n\n\n\n\n\n\n\nInference Metrics. 99th percentile of TPOT, ITL, and TTFT. Lower is better.\n\n\n\n\n\nAll metrics generally increase (worsen) with batch size. Note that the graph shows worst-case statistics (99th percentile). We observe the following patterns:\n\nTTFT: The TTFT is generally higher for speculative decoding methods than vanilla decoding because the server must prefill with both the draft and target models, rather than just the target model. In MT-Bench (Qwen3), the TTFT of EAGLE-3 is consistently higher than for draft_model, which is surprising since EAGLE-3 drafters are lighter.\nTPOT: Both speculative decoding methods significantly reduce TPOT compared to vanilla decoding, particularly in low batch sizes.\nITL: The inter-token latency (ITL) differs from TPOT in a subtle way: it measures the time between batches of tokens as seen by the client. During speculative decoding, the client receives batches of multiple tokens, so ITL will be higher than TPOT. In vanilla decoding, ITL and TPOT should be equal or very similar. In the plot, vanilla decoding (SD=None) has the lowest ITL, followed by EAGLE-3, and finally draft_model."
  },
  {
    "objectID": "posts/06_vllm-spec-decode/index.html#discussion",
    "href": "posts/06_vllm-spec-decode/index.html#discussion",
    "title": "Up to 3.55x Faster: Contributing Speculative Decoding with Draft Models to vLLM V1",
    "section": "Discussion",
    "text": "Discussion\n\nLimitations\nThe draft model must share the same tokenizer as the target model, i.e., both models must ‚Äúspeak the same language.‚Äù This is typically the case when models belong to the same family.\nAt the moment, draft_model is limited to models with only a single KV-cache group. There is an open issue about this on Github (#33133)\nMy benchmarks use the default --output-len=200 parameter, which means that the speedups presented are valid for short-context inference. Long context inference is more complex: performance is bottlenecked KV-cache loading, and batch sizes must become a lot smaller to fit in VRAM. I might publish an analysis of long-context performance in the future.\n\n\nWhen to Use Draft Models vs.¬†EAGLE-3\nMy benchmarks show that draft_model outperforms EAGLE-3 in 2 of 3 tested settings, primarily due to higher acceptance lengths (Figure¬†5). Prefer draft_model when a smaller model from the same family is available, no training required. Distilled models (trained to mimic the larger model‚Äôs logits) work especially well. Consider EAGLE-3 when a pre-trained EAGLE-3 drafter exists on Hugging Face and you want the lowest possible overhead. When in doubt, benchmark both methods on your workload. You can also train your own using the vLLM speculators library13."
  },
  {
    "objectID": "posts/06_vllm-spec-decode/index.html#key-learnings",
    "href": "posts/06_vllm-spec-decode/index.html#key-learnings",
    "title": "Up to 3.55x Faster: Contributing Speculative Decoding with Draft Models to vLLM V1",
    "section": "Key Learnings",
    "text": "Key Learnings\n\nSide-Effects\nThe vLLM project uses a lot of side effects, which can make it difficult to understand what the inputs to the model really are. The most important example to understand is the forward_context object, which is a global variable that contains information about every layer in the model and references its KV cache. The forward context is manipulated at runtime with the set_forward_context() function, here in the main loop of the gpu_model_runner.py file. It looks similar to Listing¬†1, and it‚Äôs important to understand that it partially determines the inputs to the model.\n\n\n\nListing¬†1: Set Forward Context\n\n\nwith set_forward_context(\n  ... # A bunch of params that determine the inputs to the model\n):\n    self._model_forward(\n      ...  # What you THINK are the only inputs to the model\n    )\n\n\n\nThe forward context contains, for example, the AttentionMetadata.slot_mapping object containing pointers to the KV cache. If these pointers are incorrect, the KV cache will be corrupted at runtime, and your outputs will be garbage. Saving to the KV cache happens concretely in the function reshape_and_cache_flash() (FlashAttention code path: permalink). The function directly takes this slot_mapping tensor as an input.\n\n\nPerformance Optimizations\nThe three biggest performance themes I ran into were: CUDA graphs, avoiding CPU‚ÄìGPU synchronizations, and Triton kernels and Fusion.\n\nCUDA Graphs\nCUDA graphs are a complex topic by themselves, but in short: they make the code within the graph run impressively fast. There is an introduction to CUDA graphs in the vLLM docs14. What‚Äôs important to understand is that CUDA graphs require the input and output tensors (buffers) to be allocated before running the code (i.e.¬†no dynamic-shape allocation of PyTorch tensors during the graph‚Äôs execution). All operations on buffers must be done in-place, rather than creating new tensors, so the code becomes heavy in side-effects. I found that if you run CUDA graphs, but don‚Äôt reuse the buffers, you will not get hard exceptions, but rather your outputs will be garbage, again. The forward context I mentioned earlier includes the batch_descriptor and cudagraph_runtime_mode objects, which together determine which CUDA graph mode the model will dispatch to. This is why you find comments like Listing¬†2 everywhere in the codebase.\n\n\n\nListing¬†2: Example Comment About CUDA Graphs\n\n\n# Run the model.\n# Use persistent buffers for CUDA graphs.\nwith set_forward_context(...):\n\n\n\n\n\nCPU-GPU synchronizations\nTo write fast and efficient PyTorch code, it‚Äôs important to avoid CPU-GPU synchronizations. Synchronizations prevent the CPU from scheduling enough work for the GPU to keep it busy, leading to low GPU utilization. I wrote a full-length separate post about them, which I recommend reading for a deeper explanation including PyTorch examples.\n\n\nTriton Kernels\nPreparing the input tokens for the speculative decoding draft model required a lot of small PyTorch operations. Launching many small kernels incurs unnecessary overhead (2-3¬µs per kernel). To fuse all these operations into a single one, I implemented a custom Triton kernel. The kernel is quite simple, but it replaced around 8 calls to PyTorch functions, avoiding their launch overhead. If you are interested in learning about Triton kernels, I recommend looking at the tutorial examples. The language is easy to learn and it gets you 85% of the way to speed-of-light performance."
  },
  {
    "objectID": "posts/06_vllm-spec-decode/index.html#contributing-to-vllm",
    "href": "posts/06_vllm-spec-decode/index.html#contributing-to-vllm",
    "title": "Up to 3.55x Faster: Contributing Speculative Decoding with Draft Models to vLLM V1",
    "section": "Contributing to vLLM",
    "text": "Contributing to vLLM\nIn this section, I share some tips about how to approach the vLLM codebase as a new contributor.\n\nStart High-Level\nIntroduce one or more unit tests for your desired API (e.g., method=draft_model for speculative decoding). Think about how, as a user, you would observe the outputs to identify if the API is correct (e.g., acceptance_rate=100% in greedy decoding). Then go ahead and run the tests. You will find yourself waiting for vLLM to spin up and compile the model, so you want to deactivate all features that make you wait for your code to execute (e.g., pass --enforce-eager). The reason is that slow feedback loops sap your focus and kill your flow. You will obviously hit errors that prevent your feature from working, but that is the goal. You are working from the inside out, catching and solving problems locally one at a time without having to understand the whole codebase at once. Set a debugger and try to understand only one error at a time, interactively and with the local context. vLLM is a very complex system, and trying to understand the whole codebase at once would be overwhelming and lead to frustration. Reading about the big picture of the codebase can be useful after getting some exposure to the codebase, e.g., in this post by Aleksa Gordiƒá.\n\n\nUse the Debugger\nMost features require you to manipulate tensors and change their shape or content. This type of PyTorch code is often not simple to read or write, because it‚Äôs often more imperative than declarative. Good contributors comment their code to communicate the intent without repeating the code itself, and without making the code so verbose that it‚Äôs buried in noisy comments. For code regions heavy in PyTorch code, I suggest factoring out a pure Python function that you can test independently. Then you can create a unit test for it and easily set breakpoints in the function without having to restart the entire vLLM server (which is very slow).\n\n\nTest Extensively\nI mentioned that you speed up your feedback cycles by passing, e.g., --enforce-eager, but that leaves the most important paths of the codebase untested. In production, you never run in eager mode, so you want to test these paths as well. I recommend the pytest.mark.parametrize decorator to test your code with different inputs, e.g., as shown in Listing¬†3 to test both eager and compiled mode. If you look at my draft_model PR, you will find that I used parametrizations almost obsessively to test many combinations. For example: Is the target model in FP8 or FP16? Is the draft model in FP8/FP16? Are we using greedy sampling or temperature &gt;= 0.0? Is the target model using tensor parallelism of 1 or more? What happens if we pass invalid combinations of these parameters? Obviously, these are specific cases for speculative decoding, and the code paths that are relevant to your feature will become apparent as you understand the code from the outside in.\n\n\n\nListing¬†3: Pytest Parametrization\n\n\n@pytest.mark.parametrize(\"enforce_eager\", [True, False])\ndef test_draft_model_correctness(enforce_eager):\n  ...\n\n\n\n\n\nCode Evolution\nThe vLLM project has far more contributors than reviewers, so many PRs sit unreviewed for extended periods. My PR was open from 2025-09-05 and merged on 2026-01-19, which is 136 days, or about 4.5 months, despite having relatively engaged reviewers. With the vLLM codebase evolving so quickly, this long review cycle meant I had to resolve merge conflicts at least 10 times.\nAfter resolving several painful conflicts, I learned that certain types of changes and patterns are more likely to cause conflicts. For example, moving a function to a new file and changing its body creates painful merge conflicts, because the function continues to evolve on the main branch in its old location. The same applies to extracting a superclass or child class, changing its body, and then moving it to a new file.\nWhile we shouldn‚Äôt plan for PRs to stay open this long, we should be prepared for the main branch to evolve in parallel to our PR. One strategy is to modify functions and classes in-place (within the same file) and defer moving them to other files in a subsequent PR. This might sound counterintuitive to experienced engineers, but it proved effective to minimize merge conflicts."
  },
  {
    "objectID": "posts/06_vllm-spec-decode/index.html#summary",
    "href": "posts/06_vllm-spec-decode/index.html#summary",
    "title": "Up to 3.55x Faster: Contributing Speculative Decoding with Draft Models to vLLM V1",
    "section": "Summary",
    "text": "Summary\nIn this post, I benchmarked the implementation of speculative decoding with draft models I contributed to vLLM V1 (PR #24322) and compared it against vanilla decoding and EAGLE-3. On MT-Bench and InstructCoder, it delivers substantial speedups (up to 2.24√ó) without specialized training. Speedups are most pronounced at smaller batch sizes and fade at larger batch sizes. The performance numbers are meant as a reference as of today (2026-01-25, commit e1a34c3), and can vary with model, workload, and deployment. Finally, I hope the key learnings and contributor tips I shared here help other new vLLM contributors get productive faster."
  },
  {
    "objectID": "posts/06_vllm-spec-decode/index.html#acknowledgments",
    "href": "posts/06_vllm-spec-decode/index.html#acknowledgments",
    "title": "Up to 3.55x Faster: Contributing Speculative Decoding with Draft Models to vLLM V1",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI thank the reviewers and collaborators of the vLLM project who engaged with my PR and provided valuable feedback: Benjamin Chislett from NVIDIA, Ekagra Ranjan from Cohere, Lily Liu from OpenAI, Wentao Ye from Red Hat, Harry Mellor from Hugging Face."
  },
  {
    "objectID": "posts/06_vllm-spec-decode/index.html#footnotes",
    "href": "posts/06_vllm-spec-decode/index.html#footnotes",
    "title": "Up to 3.55x Faster: Contributing Speculative Decoding with Draft Models to vLLM V1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://huggingface.co/Qwen/Qwen3-32B‚Ü©Ô∏é\nhttps://huggingface.co/Qwen/Qwen3-1.7B‚Ü©Ô∏é\nhttps://huggingface.co/Qwen/Qwen3-4B‚Ü©Ô∏é\nhttps://huggingface.co/RedHatAI/Qwen3-32B-speculator.eagle3‚Ü©Ô∏é\nhttps://huggingface.co/meta-llama/Llama-3.3-70B-Instruct‚Ü©Ô∏é\nhttps://huggingface.co/meta-llama/Llama-3.2-1B‚Ü©Ô∏é\nhttps://huggingface.co/yuhuili/EAGLE3-LLaMA3.3-Instruct-70B‚Ü©Ô∏é\nhttps://huggingface.co/datasets/philschmid/mt-bench‚Ü©Ô∏é\nhttps://huggingface.co/datasets/likaixin/InstructCoder‚Ü©Ô∏é\nOn InstructCoder with LLaMa 70B the KV cache fills up completely at batch size 64, meaning some requests are put to wait. Increasing the batch size beyond doesn‚Äôt increase the parallelism, so I only benchmark up to 64 in this case.‚Ü©Ô∏é\nhttps://github.com/sgl-project/sglang‚Ü©Ô∏é\nI had not expected AL variation with batch size. Perhaps batch invariance would solve this.‚Ü©Ô∏é\nhttps://github.com/vllm-project/speculators‚Ü©Ô∏é\nhttps://docs.vllm.ai/en/stable/design/cuda_graphs/‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/09_full-cuda-graphs-spec-decode/index.html",
    "href": "posts/09_full-cuda-graphs-spec-decode/index.html",
    "title": "Forecasting the Performance of Full CUDA Graphs for Speculative Decoding",
    "section": "",
    "text": "In a previous post, I benchmarked speculative decoding with draft models in vLLM V1. This follow-up analyzes the potential performance gains from supporting full CUDA graphs (Full CG) for draft-model speculative decoding. My results suggest that the speedup ratio over vanilla decoding would improve by ~5%, growing from from 2.26√ó to 2.37 (Qwen3-32B on MT-Bench)."
  },
  {
    "objectID": "posts/09_full-cuda-graphs-spec-decode/index.html#background",
    "href": "posts/09_full-cuda-graphs-spec-decode/index.html#background",
    "title": "Forecasting the Performance of Full CUDA Graphs for Speculative Decoding",
    "section": "Background",
    "text": "Background\nCurrently, vLLM supports two CG modes: Piecewise CG and Full CG. The Full CG mode is typically fastest, but it comes with stricter constraints and isn‚Äôt compatible with all execution paths. At the moment, draft models (both EAGLE-3 and draft_model) use Piecewise CG."
  },
  {
    "objectID": "posts/09_full-cuda-graphs-spec-decode/index.html#method",
    "href": "posts/09_full-cuda-graphs-spec-decode/index.html#method",
    "title": "Forecasting the Performance of Full CUDA Graphs for Speculative Decoding",
    "section": "Method",
    "text": "Method\nWhat speedups could we achieve by supporting Full CG for speculative decoding? I measure the similar work under both Piecewise CG and Full CG, and compare the results.\n\nStandalone (Full CG): I disable speculative decoding, run each model (Qwen3-32B and Qwen3-1.7B) separately on a single InstructCoder request with 1000 output tokens, and measure ITL. This estimates the per-token runtime under Full CG.\nWithin speculative decoding (Piecewise CG): I enable speculative decoding (draft_model), run draft + target, and measure ITL while varying the number of speculative tokens (K). Because each additional speculative token triggers one extra draft forward pass, a linear fit of ITL vs K is a good approximation. The regression in Figure¬†1 can be interpreted as follows:\n\nThe Intercept: the target-model cost inside the SD loop\nThe Slope: the incremental draft-model cost per additional speculative token\n\n\n\n\n\n\n\n\nFigure¬†1: Regression of ITL over number of speculative tokens. The linear fit is good.\n\n\n\nWith those two measurements, we can compare per-token runtimes across graph modes in Table¬†1. ‚ÄúSD Runtime‚Äù comes from the ITL regression (Piecewise CG, inside SD), and ‚ÄúStandalone Runtime‚Äù is the median ITL from vllm bench serve without SD (Full CG). For this setup, the draft model is 9.10% faster under full graphs, while the target model is 2.91% faster.\n\n\n\nTable¬†1: Table: Speed comparison of Full CG and Piecewise CG.\n\n\n\n\n\n\n\n\n\n\n\nModel\nSD Runtime (Piecewise CG)\nStandalone Runtime (Full CG)\nRuntime Reduction\n\n\n\n\nQwen3-1.7B\n2.50 ms\n2.27 ms\n9.10 %\n\n\nQwen3-32B\n24.36 ms\n23.65 ms\n2.91 %"
  },
  {
    "objectID": "posts/09_full-cuda-graphs-spec-decode/index.html#forecasted-end-to-end-impact",
    "href": "posts/09_full-cuda-graphs-spec-decode/index.html#forecasted-end-to-end-impact",
    "title": "Forecasting the Performance of Full CUDA Graphs for Speculative Decoding",
    "section": "Forecasted End-to-end Impact",
    "text": "Forecasted End-to-end Impact\nAssuming we could realize those per-token reductions inside speculative decoding, what does that translate to in terms of TPOT? I use the formula below to forecast the speedups for full CUDA graphs. \\[\n\\text{TPOT} = \\frac{\\text{ITL}}{\\text{AL}} = \\frac{T_{d} \\cdot K + T_{t}}{\\text{AL}}\n\\]\nwhere \\(T_{d}\\) is the runtime of the draft model, \\(T_{t}\\) is the runtime of the target model, \\(K\\) is the number of speculative tokens, and \\(\\text{AL}\\) is the acceptance length. Figure¬†2 shows the TPOT values under Piecewise CG (Current), and Full CG (Expected). The minimum (best) TPOT values for each curve are marked with a cross. At \\(K=5\\)1, the TPOT improves from 10.43ms to 9.91ms (4.9% improvement). The speedup ratios over vanilla decoding grow from 2.26√ó to 2.37√ó for Qwen3-32B (1.7B drafter) on MT-Bench.\n\n\n\n\n\n\nFigure¬†2: TPOT Comparison between Piecewise CG and Full CG (forecast)."
  },
  {
    "objectID": "posts/09_full-cuda-graphs-spec-decode/index.html#summary",
    "href": "posts/09_full-cuda-graphs-spec-decode/index.html#summary",
    "title": "Forecasting the Performance of Full CUDA Graphs for Speculative Decoding",
    "section": "Summary",
    "text": "Summary\nSupporting Full CG for speculative decoding could improve speedup ratios by ~5% from 2.26√ó to 2.37√ó over vanilla decoding. When I started this analysis, I expected larger improvements (closer to 20%), but the target model dominates overall ITL, so draft-side gains translate to single-digit TPOT gains. Nevertheless, this improvement would positively impact the performance of draft_model across all workloads. The same method could be applied to estimate the performance gains for other combinations of draft, target models, and datasets."
  },
  {
    "objectID": "posts/09_full-cuda-graphs-spec-decode/index.html#footnotes",
    "href": "posts/09_full-cuda-graphs-spec-decode/index.html#footnotes",
    "title": "Forecasting the Performance of Full CUDA Graphs for Speculative Decoding",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this experiment, I used only a single request from the InstructCoder dataset, with --output-len=1000. The best TPOT values were achieved at \\(K=5\\) rather than the \\(K=4\\) for the larger benchmark, but the TPOT differences are insignificant. The code for this analysis is available on Github‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/08_cpu_gpu_synchronization/index.html",
    "href": "posts/08_cpu_gpu_synchronization/index.html",
    "title": "PyTorch and CPU-GPU Synchronizations",
    "section": "",
    "text": "TL;DR: This post is a guide to understand and prevent CPU-GPU synchronizations, which will help you write fast and efficient PyTorch programs üöÄ. I explain the concept with concrete PyTorch examples from this Github Gist, and profiles from NVIDIA Nsight Systems.\n\nIntroduction\nCPU-GPU synchronizations (also known as host-device synchronizations) are a subtle but important mechanism to write fast and efficient PyTorch programs. The CPU-GPU synchronization is a blocking operation that prevents the CPU from scheduling new work (PyTorch ops) on the GPU. The PyTorch Tuning Guide makes some basic suggestions about how to avoid CPU-GPU synchronizations, e.g.¬†by avoiding calls to tensor.item(), printing a device tensor, or calling tensor.cpu() / tensor.cuda(), but doesn‚Äôt explain in depth what is happening under the hood. Furthermore, there are other subtle ways to run into CPU-GPU synchronizations. In this post I dive deeper into synchronizations and explain how they slow down a PyTorch program. I show how to use the NVIDIA Nsight Systems profiler to identify and diagnose CPU-GPU synchronization issues. Finally, I also show how to use the experimental PyTorch API torch.cuda.set_sync_debug_mode in unit tests to verify the absence of CPU-GPU synchronizations in production code.\n\n\n\n\n\n\nFigure¬†1: Profile showing CPU-GPU synchronization issues (here triggered by printing a CUDA tensor). A deep dive into the profile details is in Section¬†4.\n\n\n\n\n\nWhat is a CPU-GPU Synchronization?\nTo fully take advantage of the speed of a GPU, it must be kept busy (high GPU utilization). This depends on the CPU scheduling enough work to keep the GPU busy. Typically, in a well-performing PyTorch program, the CPU schedules many instructions quickly, which are executed by the GPU. The CPU is said to run ahead of the GPU, because it issues instructions without waiting for the previous ones to complete. This is why PyTorch is said to be asynchronous. If the CPU fails to schedule enough work, the GPU will sit idle waiting for work.\nThe intuition: As an analogy to understand CPU-GPU synchronizations, think of a restaurant where a chef (CPU) schedules all the steps (PyTorch ops) for a big dinner party in advance. A CPU-GPU synchronization would be like the chef waiting to observe how a specific step in a specific dish turned out during the dinner, before scheduling the rest of the dishes and telling the cooks what to do. Obviously, this latency in communication will lead to the cooks being idle, waiting for the chef to schedule the dishes. Instead, the chef should schedule the entire dinner plan in advance (run ahead).\n\n\nExample\nLet‚Äôs look at a concrete example. Below we have a PyTorch program that executes a slow_operation() on the GPU, followed by a quick_operation() on the GPU. The quick operation just counts the number of 1s in the result tensor. The result of the quick operation is gathered in results tensor. Both functions are executed in a loop, to simulate a long-running program like LLM inference, where the slow operation could be the LLM forward pass, and the quick operation could be bookkeeping with the sampled tokens (a real vLLM example is explained in Section¬†7). To warm up the torch code, I run both the slow_operation() and quick_operation() once before the hot loop. I annotated regions of code with nvtx.annotate(), to keep track of GPU and CPU runtime during profiling.\n\n\n\nProfiling Analysis\nWe can run the NVIDIA Nsight Systems profiler with the command below to produce the timelines shown in Figure¬†2 and Figure¬†3. The script takes a flag --do-print to include a CPU-GPU synchronization in the quick operation: a print of a GPU tensor, which forces data back from the GPU to the CPU.\nWithout CPU-GPU synchronization:\nnsys profile \\\n  -o profile-no-print.nsys-rep \\\n  python cpu_gpu_sync_example.py\nWith CPU-GPU synchronization:\nnsys profile \\\n  -o profile-do-print.nsys-rep \\\n  python cpu_gpu_sync_example.py --do-print\nThe resulting profiles are shown below and annotated. I ran the code on an NVIDIA RTX3090 GPU and Nsight Systems 2025.3.1.90.\nTo inspect the report:\n\nOpen the .nsys-rep in the Nsight Systems UI\nZoom into the NVTX-annotated region.\nLook for long CPU-side CUDA API calls like cudaStreamSynchronize and correlate them with gaps in GPU utilization.\n\n\nRun With CPU-GPU Synchronization\n\n\n\n\n\n\nFigure¬†2: Profile with CPU-GPU synchronization.\n\n\n\n\nWe see gaps in the GPU utilization (first blue bar from top to bottom). These gaps indicate that the GPU has idle times and waits for work.\nWe observe that the runtimes of the GPU and CPU are similarly long, as seen both timelines being equal in length in the horizontal axis. In a well-performing program, the CPU runtime should be shorter than the GPU runtime (CPU runs ahead of the GPU).\nIn the CPU ops timeline, we see long green bars (cudaStreamSynchronize), which mean that the CPU is blocked waiting for the GPU to return some data.\nOn the GPU ops timeline we observe that the slow operation takes longer than the quick operation, while in the CPU ops timeline we observe the reverse. This means that the quick operation is the one blocking the CPU.\n\n\n\nHealthy Run\n\n\n\n\n\n\nFigure¬†3: Profile without CPU-GPU synchronization.\n\n\n\n\nWe observe that the CPU runs ahead of the GPU, so it dispatches all the work quickly and finishes way before the GPU.\nThe GPU utilization does not have any gaps. The blue line showing utilization is continuous.\nIn the GPU ops timeline, the slow operation is slower than the quick operation.\nThe full region interleaved-code runs faster than with the CPU-GPU synchronization (4.434 ms vs 4.827 ms).\n\n\n\n\nDynamic Shapes Can Trigger Synchronizations\nBesides the code mentioned in the PyTorch Tuning Guide there is another common pattern that triggers CPU-GPU synchronizations, namely dynamic shapes. One example of dynamic shapes is boolean indexing x = t[bool_mask], where bool_mask is a boolean tensor on GPU. The size of the tensor x cannot be determined by the CPU alone, because it depends on the number of true values in bool_mask. Therefore, PyTorch typically needs to fetch data from the GPU to determine the amount of memory to allocate for x. This creates a CPU-GPU synchronization.\nAnother example of this same problem is slicing x = t[:gpu_index], where gpu_index is a scalar integer tensor on GPU. Once again, the size of the tensor x cannot be determined by the CPU alone, because it depends on the value of gpu_index.\nThere are other ways to slice a torch tensor, e.g.¬†x = t.narrow(dim=0, start, length) (link). However, this operation requires length to be a Python int. Passing a GPU tensor as length will typically trigger a cast to Python int on CPU, which introduces a CPU-GPU synchronization.\nA similar situation arises with x = torch.repeat_interleave(y, repeats). But interestingly, this API exposes an optional argument output_size that can be used to prevent the synchronization (link).\nWhat all these synchronization triggers have in common is that the length of the resulting tensor depends on data residing on the GPU. This is why dynamic shapes are so problematic. Instead, if the length of the resulting tensor can be known statically on the CPU side, then we can find a way to avoid the CPU-GPU synchronization. PyTorch might not provide the precise API you need to avoid the synchronization. In that case, you might want to write a custom kernel in Triton to avoid the synchronization. The advantage of doing this is that you can fuse many sequential PyTorch operations into a single Triton kernel, which reduces the overhead of dispatching many small kernels on the CPU-side (around 2¬µs to 3¬µs per kernel). A concrete example is explained in Section¬†7. The accompanying Github Gist also uses Triton kernels to fuse together PyTorch ops, and prevent CPU-GPU synchronizations.\n\n\nUnit Testing for CPU-GPU Synchronizations\nSo we need to prevent synchronization in our code. How can we do this without running a profiler on each code section we are interested in? PyTorch provides an experimental feature that can raise warnings or errors when CPU-GPU synchronizations occur: The torch.cuda.set_sync_debug_mode() function (link). Since it raises errors immediately, it can be used in unit tests to verify that the code is free of CPU-GPU synchronizations. For example it can be used in a context manager:\nimport functools\nfrom contextlib import contextmanager\n\nimport torch\n\n@contextmanager\ndef fail_if_gpu_cpu_synchronization(fail: bool):\n    \"\"\"Within this context, GPU-CPU synchronization raises an error if `fail` is True.\"\"\"\n\n    new_mode = 2 if fail else 0\n    old_mode = torch.cuda.get_sync_debug_mode()\n    torch.cuda.set_sync_debug_mode(new_mode)\n    try:\n        yield\n    finally:\n        torch.cuda.set_sync_debug_mode(old_mode)\n\n\nx = torch.arange(10, device=\"cuda\")\nwith fail_if_gpu_cpu_synchronization(fail=True):\n    print(x)  # Raises an error\nThe context manager can be used in a decorator as well, to decorate functions that should fail if they contain CPU-GPU synchronizations.\ndef on_gpu_cpu_synchronization(fail: bool):\n    \"\"\"\n    Wrap a function to raise an error on GPU-CPU synchronization if `fail` is True.\n    \"\"\"\n\n    def decorator(fn):\n        @functools.wraps(fn)\n        def wrapper(*args, **kwargs):\n            with fail_if_gpu_cpu_synchronization(fail):\n                return fn(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\n@on_gpu_cpu_synchronization(fail=True)\ndef test_should_not_sync():\n    x = torch.arange(10, device=\"cuda\")\n    y = x ** 2\nNote that the decorator is used only on test functions, meaning that the PyTorch API to raise synchronization errors is never applied to production code, but only to the test code. This keeps this experimental feature isolated from production code while still providing quick and valuable feedback on CPU-GPU synchronizations. I used this pattern to verify the absence of synchronizations in my Github Gist about vLLM TokenGroup. The mechanism itself of raising errors is also unit tested in the test_helpers.py file in the Github Gist.\n\n\n\n\n\n\nWarning\n\n\n\nThis PyTorch API is experimental as of today, and does not cover all CPU-GPU synchronizations. For example, it does not cover the torch.distributed and torch.sparse namespaces (see docs).\n\n\n\n\nIn Practice: vLLM\nI encountered CPU-GPU synchronizations in the context of contributing speculative decoding to vLLM (PR #24322). In particular, the LLM forward passes are heavy operations, and preparing the input tokens for the forward pass requires lots of small PyTorch operations, which can introduce CPU-GPU synchronizations, if not carefully written. Senior NVIDIA engineer Benjamin Chislett helped me understand what causes the synchronizations, in particular dynamic shapes, and the value of writing custom kernels in Triton to fuse together multiple sequential PyTorch operations (example: PR #28597). I thank him for his support and feedback! üí™\n\n\nSummary\nIn this post, I explained what CPU-GPU synchronizations are, and how to identify and diagnose them with the NVIDIA Nsight Systems profiler. I discussed dynamic shapes as a common trigger for CPU-GPU synchronizations, and the value of writing custom kernels in Triton to fuse together multiple sequential PyTorch operations. I also showed how to use the experimental PyTorch API torch.cuda.set_sync_debug_mode in unit tests to verify the absence of CPU-GPU synchronizations in production code. This guide aims to help engineers prevent CPU-GPU synchronizations, which is key to write fast and efficient PyTorch programs.\n\n\nFurther References\nFor a deep dive into kernel benchmarking in practice, I recommend this YouTube lecture by NVIDIA engineering manager Georgii Evtushenko.\n\n\n\n\n\nCitationBibTeX citation:@online{ruiz2026,\n  author = {Ruiz, Tomas},\n  title = {PyTorch and {CPU-GPU} {Synchronizations}},\n  date = {2026-01-07},\n  url = {https://tomasruizt.github.io/posts/08_cpu_gpu_synchronization/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRuiz, Tomas. 2026. ‚ÄúPyTorch and CPU-GPU Synchronizations.‚Äù\nJanuary 7, 2026. https://tomasruizt.github.io/posts/08_cpu_gpu_synchronization/."
  },
  {
    "objectID": "posts/04_multimodal-attn/index.html",
    "href": "posts/04_multimodal-attn/index.html",
    "title": "Drilling Down into Multimodal Attention",
    "section": "",
    "text": "Visualizing Multimodal Attention Patterns"
  },
  {
    "objectID": "posts/04_multimodal-attn/index.html#paligemma2s-attention-patterns",
    "href": "posts/04_multimodal-attn/index.html#paligemma2s-attention-patterns",
    "title": "Drilling Down into Multimodal Attention",
    "section": "PaliGemma2‚Äôs Attention Patterns",
    "text": "PaliGemma2‚Äôs Attention Patterns\nWe now drill down into PaliGemma2‚Äôs attention patterns. When looking at the attention patterns, the first thing that jumps out is that the text tokens are not attending to the image tokens very much, because the image is almost completely white (even at zero attention, the image remains visible to prevent it from dissapearing completely). This effect is consistent across layers (See Figure¬†2, Figure¬†3, Figure¬†4). This is surprising, because the question can only be answered by attending to the image. How does then PaliGemma2 answer the question?\n\n\n\n\n\n\nFigure¬†2: Layer 0: Link to full visualization\n\n\n\n\n\n\n\n\n\nFigure¬†3: Layer 15: Link to full visualization Dark vertical bars, but first row (&lt;bos&gt; token) is white\n\n\n\n\n\n\n\n\n\nFigure¬†4: Layer 25: Link to full visualization\n\n\n\nIn the middle layers (layer 15), vertical bars are visible in almost every head. This indicates that most text tokens are attending to the &lt;bos&gt; token, which is the first token after the image tokens. Interestingly, the &lt;bos&gt; does not attend back to the other text tokens. We can tell because the first row of the colored pattern is completely white (close to 0 attention) in middle and late layers.\nSo what is the &lt;bos&gt; token attending to? Mostly to image tokens. To see this, I increase the contrast of the attention patterns using the slider and compare the attentions with different destination text tokens. The &lt;bos&gt; token is attending uniformly to many image tokens. The images below are all from intermediate layers (layer 15).\n\n\n\nThe &lt;bos&gt; token attends uniformly to many image tokens\n\n\n\n\n\nThe next text token attends sparsely to image tokens\n\n\n\n\n\nThe last text token also attends sparsely to image tokens, although more in patches.\n\n\nThis suggests a hypothesis: Namely that the visual information flows from the image tokens into the &lt;bos&gt; token, and then from the &lt;bos&gt; token to the rest of the text tokens. To quantify this, I partition the input into 3 regions: The image tokens, the &lt;bos&gt; token, and the rest of the text tokens. By summing up the attention of the tokens in each region, we get a measure of the attention between regions. This yields a 3x3 matrix, where each row sums up to 1.\n\n\n\n\n\n\nFigure¬†5: Self-attention over input regions. In lower layers, image tokens mostly attend to each other, despite having access to all tokens. Later, in middle and final layers, image tokens also attend to the &lt;bos&gt; token, (an example of information flowing back from text to image). The &lt;bos&gt; token increasingly attends to the image tokens as depth increases. Text tokens attend as much (or more) to the &lt;bos&gt;token as to the image tokens, despite their ratio being 1:256.\n\n\n\nThese numbers suggest that PaliGemma2 has trained the &lt;bos&gt; token to be a ‚Äúbroker‚Äù token for visual information: The &lt;bos&gt; token ‚Äúcollects‚Äù and aggregates visual information from the image tokens into a single place, and then ‚Äúserves‚Äù it back to text and image tokens. It plays a similar role as the [CLS] token in the ViT."
  },
  {
    "objectID": "posts/04_multimodal-attn/index.html#do-the-numbers-generalize",
    "href": "posts/04_multimodal-attn/index.html#do-the-numbers-generalize",
    "title": "Drilling Down into Multimodal Attention",
    "section": "Do the Numbers Generalize?",
    "text": "Do the Numbers Generalize?\nTo test if the hypothesis holds in general for (image, text) pairs other than the example of the dog with the frisbee, I ran the analysis on the first 1000 distinct images from the VQA dataset (train) and their corresponding questions. The dataset has multiple questions per image, but I used only the first question so as to have the most visual variation within the 1000 samples.\n\n\n\nVQA images, questions, and model answers. The images are diverse real-life images, and the questions are short open- and closed-ended questions. Note that the questions are formatted in the PaliGemma2 QA format (‚ÄúAnswer en ‚Äù).\n\n\nI computed the self-attention matrix over regions for each (image, question) pair and computed the average and the standard deviation over the 1000 pairs. We observe that the standard deviations are very small, indicating that the ‚Äúbroker‚Äù role of the &lt;bos&gt; token is robust and independent of the image and question.\n\n\n\nSelf-attention over input regions for 1000 VQA samples. The standard deviations are small. The numbers for previous example (purple frisbee) in Figure¬†5 are mostly within the 1-\\(\\sigma\\) confidence interval here, suggesting it is a typical example."
  },
  {
    "objectID": "posts/04_multimodal-attn/index.html#acknowledgement",
    "href": "posts/04_multimodal-attn/index.html#acknowledgement",
    "title": "Drilling Down into Multimodal Attention",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis is the final project for the course ‚ÄúArtificial Intelligence Safety Fundamentals‚Äù (AISF) by BlueDot Impact. The author is funded by the Bavarian Research Institute for Digital Transformation (bidt) and the Ludwig Maximilian University of Munich."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Forecasting the Performance of Full CUDA Graphs for Speculative Decoding\n\n\nEstimating Speedups from Piecewise to Full CUDA Graphs\n\n\n\nvLLM\n\n\nCUDA\n\n\nPerformance\n\n\n\n\n\n\n\n\n\nJan 29, 2026\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nUp to 3.55x Faster: Contributing Speculative Decoding with Draft Models to vLLM V1\n\n\nBenchmarks and Key Learnings\n\n\n\nvLLM\n\n\nPyTorch\n\n\nTriton\n\n\nTransformers\n\n\n\n\n\n\n\n\n\nJan 28, 2026\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nPyTorch and CPU-GPU Synchronizations\n\n\nWriting Fast PyTorch Code\n\n\n\nGPUs\n\n\nPyTorch\n\n\nTriton\n\n\n\n\n\n\n\n\n\nJan 7, 2026\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nA Guide to Classify 35,000 Videos with a 72B Multimodal LLM on the vLLM Engine\n\n\n\n\n\n\nGPUs\n\n\nvLLM\n\n\nTransformers\n\n\n\n\n\n\n\n\n\nSep 22, 2025\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nDrilling Down into Multimodal Attention\n\n\n\n\n\n\nTransformers\n\n\nAttention\n\n\n\n\n\n\n\n\n\nFeb 1, 2025\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nHow Does Tiling Speed Up Matrix Multiplications on GPUs?\n\n\n\n\n\n\nMathematics\n\n\nGPUs\n\n\n\n\n\n\n\n\n\nDec 23, 2024\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nGrokking an Inner Product Inequality With Python on WebAssembly\n\n\n\n\n\n\nMathematics\n\n\nPython\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nA Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nAug 2, 2024\n\n\nTomas Ruiz\n\n\n\n\n\n\nNo matching items"
  }
]