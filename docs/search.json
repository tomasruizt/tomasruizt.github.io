[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Grokking an Inner Product Inequality With Python on WebAssembly\n\n\n\n\n\n\nMathematics\n\n\nPython\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\nTomas Ruiz\n\n\n\n\n\n\n\n\n\n\n\n\nA Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nAug 2, 2024\n\n\nTomas Ruiz\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "",
    "text": "The purpose of this post is two-fold:\n\nTo showcase Python running directly in your browser without any server behind it, like JavaScript. I will even import libraries like numpy and matplotlib. The underlying technologies that power this are WebAssembly and Pyodide, which I encourage you to check out.\nTo get you excited about a simple inequality and its application to vectors, functions & matrices. These are our objects of study, and we will inspect and visualize them interactively using Python.\n\nI created this document using Quarto Live. Big thanks to Renata Topinkova for showing me this tool!\n\n\n\nMichael Steele presents in his book this simple inequality based only on the fact that \\((x-y)^2\\) is always positive1:\n\\[\n\\begin{align}\n0 &\\leq  (x - y)^2 \\\\\n\\implies 0 &\\leq x^2 -2xy +y^2 \\\\\n‚üπ xy &‚â§ \\frac{1}{2} (x^2 + y^2)\n\\end{align}\n\\]\nThe last inequality above is not intuitively obvious to me. I‚Äôm the kind of person that likes numerical proof to internalize these results, and this is where Python comes in handy. Change the variables x and y in the code below to see if the inequality holds.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis Python code is running in your browser. There is no juypter notebook, nor any deployment or any client-server communication behind it! ü§ØüöÄ\n\n\n\n\n\nWhat happens when we apply this inequality to more than just scalars? It also applies to sequences of numbers, e.g: \\[\nx_1 y_1 + x_2 y_2 &lt;= \\frac{1}{2} (x_1^2 + x_2^2) + \\frac{1}{2} (y_1^2 + y_2^2)\n\\] You might recognize that this is equivalent to an inner product: \\[ x^T y ‚â§ \\frac{1}{2} (x^T x + y^T y)\\] where \\(x = [x_1, \\dots, x_n]\\) and \\(y = [y_1, \\dots, y_n]\\).\nThe inequality is asserting that the vector product \\(x^Ty\\) of any two vectors \\(x\\) and \\(y\\) has an upper bound given by the average of \\(x^Tx\\) and \\(y^Ty\\).\nOnce again, I‚Äôm not intuitively convinced until I see code running. Notice how we import numpy, which calls compiled C routines under the hood (but our runtime is the browser now).\n\n\n\n\n\n\n\n\n\nYou might have heard that functions are infinite-dimensional vectors (ü§Ø). In that case, the inequality also applies! But how does the inner product \\(x^Ty\\) for two functions look like?\nThe convention is to use the bracket notation \\(‚ü® x, y ‚ü©\\) rather than \\(x^Ty\\). To sum over the infinite individual entries of the (function) vector, we use the integral:\n\\[‚ü® f, g ‚ü© = \\int f(x) g(x) dx\\] Using this definition, the inequality holds for functions as well: \\[\n\\begin{align}\n‚ü® f, g ‚ü© &‚â§ \\frac{1}{2} (‚ü® f, f ‚ü© + ‚ü® g, g ‚ü©) \\\\\n&= \\frac{1}{2} \\left( \\int f(x)^2 dx + \\int g(x)^2 dx \\right)\n\\end{align}\n\\]\nLet‚Äôs take two concrete functions \\(f(x) = \\cos(x)\\) and \\(g(x) = \\sin(4x)\\). I choose these arbitrarily because plotting them looks nice. Feel free to use different functions f and g in the code.\n\n\n\n\n\n\n\nIn the plot above, the individual functions \\(f^2\\) and \\(g^2\\) are plotted with light-blue lines. Their average is the red line, and the product \\(f ‚ãÖ g\\) is the green line. The red line is an upper bound for the green one. We see that the green line crosses over the two blue lines at different points but never crosses over the red line.\nAbout the integral: Perhaps you noticed that I formulated the inequality on inner-products, but I‚Äôm plotting the functions pointwise. The missing step is the integral, which is evaluated in Python using the numpy function np.trapz(). As we can confirm below, the inequality holds:\n\n\n\n\n\n\n\n\n\nWill the inequality also apply to matrices? The inner product of two matrices \\(A\\) and \\(B\\) (also called Frobenius inner product) is defined as: \\[‚ü®A, B‚ü© = \\text{tr}(A^T B)\\] where \\(\\text{tr}\\) is the trace operator.\n\n\n\n\n\n\nWarning\n\n\n\nBeware that this inner product is different from matrix multiplication \\[‚ü®A, B‚ü© = tr(A^T B) ‚â† AB\\]\n\n\nThe inequality for matrices then reads:\n\\[tr(A^T B) ‚â§ \\frac{1}{2} (tr(A^T A) + tr(B^T B))\\]\nIt‚Äôs easy to convince yourself that the inequality holds for matrices by writing down the trace as a sum of scalars. As we verified before, inequality holds for each scalar in the sum:\n\\[\n\\begin{align}\ntr(A^T B)\n&= \\sum_{i,j} A_{ij} B_{ij} & \\text{(definition)}\\\\\n&‚â§ \\frac{1}{2} \\left( \\sum_{i,j} A_{ij}^2 + \\sum_{i,j} B_{ij}^2 \\right) &\\text{(applied by scalar)}\\\\\n&= \\frac{1}{2} (tr(A^T A) + tr(B^T B))\n\\end{align}\n\\]\nLet‚Äôs check the inequality with random matrices. You can use the \"Start Over\" button to re-run the code with new matrices.\n\n\n\n\n\n\nThe inequality holds, but I have no geometric intuition about the trace of a matrix, or how this inequality could be visualized for matrices. If you have an idea, please let me know! üôè\n\n\n\nIf you found the mathematics interesting, particularly the generalization of inner products, I recommend MIT‚Äôs course Matrix Calculus for Machine Learning and Beyond, which covers this topic in more detail, and goes much further üòÑ."
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html#summary",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html#summary",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "",
    "text": "The purpose of this post is two-fold:\n\nTo showcase Python running directly in your browser without any server behind it, like JavaScript. I will even import libraries like numpy and matplotlib. The underlying technologies that power this are WebAssembly and Pyodide, which I encourage you to check out.\nTo get you excited about a simple inequality and its application to vectors, functions & matrices. These are our objects of study, and we will inspect and visualize them interactively using Python.\n\nI created this document using Quarto Live. Big thanks to Renata Topinkova for showing me this tool!"
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html#the-inequality",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html#the-inequality",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "",
    "text": "Michael Steele presents in his book this simple inequality based only on the fact that \\((x-y)^2\\) is always positive1:\n\\[\n\\begin{align}\n0 &\\leq  (x - y)^2 \\\\\n\\implies 0 &\\leq x^2 -2xy +y^2 \\\\\n‚üπ xy &‚â§ \\frac{1}{2} (x^2 + y^2)\n\\end{align}\n\\]\nThe last inequality above is not intuitively obvious to me. I‚Äôm the kind of person that likes numerical proof to internalize these results, and this is where Python comes in handy. Change the variables x and y in the code below to see if the inequality holds.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis Python code is running in your browser. There is no juypter notebook, nor any deployment or any client-server communication behind it! ü§ØüöÄ"
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html#generalizing-to-vectors",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html#generalizing-to-vectors",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "",
    "text": "What happens when we apply this inequality to more than just scalars? It also applies to sequences of numbers, e.g: \\[\nx_1 y_1 + x_2 y_2 &lt;= \\frac{1}{2} (x_1^2 + x_2^2) + \\frac{1}{2} (y_1^2 + y_2^2)\n\\] You might recognize that this is equivalent to an inner product: \\[ x^T y ‚â§ \\frac{1}{2} (x^T x + y^T y)\\] where \\(x = [x_1, \\dots, x_n]\\) and \\(y = [y_1, \\dots, y_n]\\).\nThe inequality is asserting that the vector product \\(x^Ty\\) of any two vectors \\(x\\) and \\(y\\) has an upper bound given by the average of \\(x^Tx\\) and \\(y^Ty\\).\nOnce again, I‚Äôm not intuitively convinced until I see code running. Notice how we import numpy, which calls compiled C routines under the hood (but our runtime is the browser now)."
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html#generalizing-to-functions",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html#generalizing-to-functions",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "",
    "text": "You might have heard that functions are infinite-dimensional vectors (ü§Ø). In that case, the inequality also applies! But how does the inner product \\(x^Ty\\) for two functions look like?\nThe convention is to use the bracket notation \\(‚ü® x, y ‚ü©\\) rather than \\(x^Ty\\). To sum over the infinite individual entries of the (function) vector, we use the integral:\n\\[‚ü® f, g ‚ü© = \\int f(x) g(x) dx\\] Using this definition, the inequality holds for functions as well: \\[\n\\begin{align}\n‚ü® f, g ‚ü© &‚â§ \\frac{1}{2} (‚ü® f, f ‚ü© + ‚ü® g, g ‚ü©) \\\\\n&= \\frac{1}{2} \\left( \\int f(x)^2 dx + \\int g(x)^2 dx \\right)\n\\end{align}\n\\]\nLet‚Äôs take two concrete functions \\(f(x) = \\cos(x)\\) and \\(g(x) = \\sin(4x)\\). I choose these arbitrarily because plotting them looks nice. Feel free to use different functions f and g in the code.\n\n\n\n\n\n\n\nIn the plot above, the individual functions \\(f^2\\) and \\(g^2\\) are plotted with light-blue lines. Their average is the red line, and the product \\(f ‚ãÖ g\\) is the green line. The red line is an upper bound for the green one. We see that the green line crosses over the two blue lines at different points but never crosses over the red line.\nAbout the integral: Perhaps you noticed that I formulated the inequality on inner-products, but I‚Äôm plotting the functions pointwise. The missing step is the integral, which is evaluated in Python using the numpy function np.trapz(). As we can confirm below, the inequality holds:"
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html#generalizing-to-matrices",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html#generalizing-to-matrices",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "",
    "text": "Will the inequality also apply to matrices? The inner product of two matrices \\(A\\) and \\(B\\) (also called Frobenius inner product) is defined as: \\[‚ü®A, B‚ü© = \\text{tr}(A^T B)\\] where \\(\\text{tr}\\) is the trace operator.\n\n\n\n\n\n\nWarning\n\n\n\nBeware that this inner product is different from matrix multiplication \\[‚ü®A, B‚ü© = tr(A^T B) ‚â† AB\\]\n\n\nThe inequality for matrices then reads:\n\\[tr(A^T B) ‚â§ \\frac{1}{2} (tr(A^T A) + tr(B^T B))\\]\nIt‚Äôs easy to convince yourself that the inequality holds for matrices by writing down the trace as a sum of scalars. As we verified before, inequality holds for each scalar in the sum:\n\\[\n\\begin{align}\ntr(A^T B)\n&= \\sum_{i,j} A_{ij} B_{ij} & \\text{(definition)}\\\\\n&‚â§ \\frac{1}{2} \\left( \\sum_{i,j} A_{ij}^2 + \\sum_{i,j} B_{ij}^2 \\right) &\\text{(applied by scalar)}\\\\\n&= \\frac{1}{2} (tr(A^T A) + tr(B^T B))\n\\end{align}\n\\]\nLet‚Äôs check the inequality with random matrices. You can use the \"Start Over\" button to re-run the code with new matrices.\n\n\n\n\n\n\nThe inequality holds, but I have no geometric intuition about the trace of a matrix, or how this inequality could be visualized for matrices. If you have an idea, please let me know! üôè"
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html#further-sources",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html#further-sources",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "",
    "text": "If you found the mathematics interesting, particularly the generalization of inner products, I recommend MIT‚Äôs course Matrix Calculus for Machine Learning and Beyond, which covers this topic in more detail, and goes much further üòÑ."
  },
  {
    "objectID": "posts/viz-inequalities-inner-prod-wasm/index.html#footnotes",
    "href": "posts/viz-inequalities-inner-prod-wasm/index.html#footnotes",
    "title": "Grokking an Inner Product Inequality With Python on WebAssembly",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúThe Cauchy-Schwarz Master Class: An Introduction to the Art of Mathematical Inequalities‚Äù by J. Michael Steele.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/linear-adaptation/index.html",
    "href": "posts/linear-adaptation/index.html",
    "title": "A Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification",
    "section": "",
    "text": "In this post I show how to linearly fine-tune a large language model (LLM) using a closed-form solution, based on the Moore-Penrose Inverse. I will focus on the special case of binary classification because the changes in output are easier to interpret. The new linear transformation \\(W\\) is shown in figure 1 (green).\n\n\n\n\n\n\nFigure¬†1: The learned transformation W (green) is applied in parallel to the existing linear layer (blue), preserving the existing knowledge of the model. Both are the summed to make the logits, which are passed to the softmax function."
  },
  {
    "objectID": "posts/linear-adaptation/index.html#references",
    "href": "posts/linear-adaptation/index.html#references",
    "title": "A Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification",
    "section": "References",
    "text": "References\n\n\nChowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al. 2023. ‚ÄúPalm: Scaling Language Modeling with Pathways.‚Äù Journal of Machine Learning Research 24 (240): 1‚Äì113.\n\n\nDutta, Abhinav, Sanjeev Krishnan, Nipun Kwatra, and Ramachandran Ramjee. 2024. ‚ÄúAccuracy Is Not All You Need.‚Äù arXiv Preprint arXiv:2407.09141.\n\n\nHu, Edward J, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. ‚ÄúLora: Low-Rank Adaptation of Large Language Models.‚Äù arXiv Preprint arXiv:2106.09685.\n\n\nWatkins, David S. 2004. Fundamentals of Matrix Computations. John Wiley & Sons."
  },
  {
    "objectID": "posts/linear-adaptation/index.html#footnotes",
    "href": "posts/linear-adaptation/index.html#footnotes",
    "title": "A Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n2024-09: The matrix \\(Z^T Z\\) is positive definite, so it is in theory efficiently invertible using the Cholesky decomposition. However, its condition number \\(\\kappa (Z^T Z)\\) is squarely proportional to the condition number of \\(\\kappa(Z)\\). This can lead to numerical instability when solving the linear system. In fact, I stumbled upon numerical instability while implementing this in linear system in PyTorch, which lead me to use an \\(L_2\\) regularization term. See Source.‚Ü©Ô∏é\n2024-09: It turns out that solving a linear system with \\(V\\) columns on the right-hand side can be done cheaper than in \\(V \\cdot \\frac{2}{3} d^3\\) flops. To solve \\(A X = B\\) (with \\(A \\in \\mathbb{R}^{d,d}\\) and \\(X, B \\in \\mathbb{R}^{d,V}\\)) the factorization of \\(A\\) requires \\(\\frac{2}{3} d^3\\) flops, but it only has to be done once. After that, solving for each of the \\(V\\) columns of \\(B\\) costs \\(2 d^2\\) flops each. So the total flop count is \\(\\frac{2}{3} d^3 + V \\cdot 2 d^2\\). This is a significant improvement over the naive approach. See (Watkins 2004, 77‚Äì78).‚Ü©Ô∏é"
  },
  {
    "objectID": "subscribe.html",
    "href": "subscribe.html",
    "title": "Subscribe",
    "section": "",
    "text": "To receive a notification when a new post is published, subscribe to the mailing list below.\n\n    \n\n\n  \n      \n          * indicates required\n          Email Address *\n      \n          \n          \n      \n  \n      /* real people should not fill this in and expect good things - do not remove this or risk form bot signups */"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a doctoral researcher at the Ludwig-Maximilian-University of Munich within Prof.¬†Schwemmer‚Äôs Computational Social Science Lab. My research area is the intersection of Machine Learning and Social Media, particularly on multi-modal understanding.\n\n\nIn previous jobs, I have worked as a software engineer in different corporations (Amazon, Allianz, BMW) and Startups. The projects ranged from optimization algorithms to backend-engineering. You can find my CV on LinkedIn.\n\n\n\n\n2021 - Technical University of Munich - Master of Science - Computer Science\n2018 - Technical University of Munich - Bachelor of Science - Engineering Science"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "",
    "text": "In previous jobs, I have worked as a software engineer in different corporations (Amazon, Allianz, BMW) and Startups. The projects ranged from optimization algorithms to backend-engineering. You can find my CV on LinkedIn."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "",
    "text": "2021 - Technical University of Munich - Master of Science - Computer Science\n2018 - Technical University of Munich - Bachelor of Science - Engineering Science"
  }
]