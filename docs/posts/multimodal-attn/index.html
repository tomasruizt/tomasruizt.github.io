<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tomas Ruiz">
<meta name="dcterms.date" content="2025-02-01">

<title>Drilling Down into Multimodal Attention – All Posts</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">All Posts</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../subscribe.html"> 
<span class="menu-text">Subscribe</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tomas-ruiz-649907b3/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/tomasruizt"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Drilling Down into Multimodal Attention</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Transformers</div>
                <div class="quarto-category">Attention</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author">Tomas Ruiz <a href="mailto:t.ruiz@lmu.de" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              Ludwig-Maximilians-Universität München
            </p>
        </div>
    </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 1, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#summary" id="toc-summary" class="nav-link active" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#mechanistic-interpretability" id="toc-mechanistic-interpretability" class="nav-link" data-scroll-target="#mechanistic-interpretability">Mechanistic Interpretability</a></li>
  <li><a href="#attention-patterns" id="toc-attention-patterns" class="nav-link" data-scroll-target="#attention-patterns">Attention Patterns</a></li>
  <li><a href="#multimodal-tokens" id="toc-multimodal-tokens" class="nav-link" data-scroll-target="#multimodal-tokens">Multimodal Tokens</a></li>
  <li><a href="#visualizing-multimodal-attention" id="toc-visualizing-multimodal-attention" class="nav-link" data-scroll-target="#visualizing-multimodal-attention">Visualizing Multimodal Attention</a></li>
  <li><a href="#case-study-paligemma2" id="toc-case-study-paligemma2" class="nav-link" data-scroll-target="#case-study-paligemma2">Case Study: PaliGemma2</a>
  <ul class="collapse">
  <li><a href="#paligemma2s-attention-patterns" id="toc-paligemma2s-attention-patterns" class="nav-link" data-scroll-target="#paligemma2s-attention-patterns">PaliGemma2’s Attention Patterns</a></li>
  <li><a href="#do-the-numbers-generalize" id="toc-do-the-numbers-generalize" class="nav-link" data-scroll-target="#do-the-numbers-generalize">Do the Numbers Generalize?</a></li>
  </ul></li>
  <li><a href="#conclusion-and-outlook" id="toc-conclusion-and-outlook" class="nav-link" data-scroll-target="#conclusion-and-outlook">Conclusion and Outlook</a>
  <ul class="collapse">
  <li><a href="#acknowledgement" id="toc-acknowledgement" class="nav-link" data-scroll-target="#acknowledgement">Acknowledgement</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/multimodal-attn.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Visualizing Multimodal Attention Patterns"><img src="images/multimodal-attn.png" class="img-fluid figure-img" style="width:70.0%" data-text-align="left" alt="Visualizing Multimodal Attention"></a></p>
<figcaption>Visualizing Multimodal Attention Patterns</figcaption>
</figure>
</div>
<section id="summary" class="level1">
<h1>Summary</h1>
<p>This post explains how to inspect the attention patterns of a vision-language models (VLMs) using a new module I created <a href="https://github.com/tomasruizt/CircuitsVis">on a fork</a> of the <code>circuitsviz</code> library. To interact with an example, <a href="https://storage.googleapis.com/tomas-ruiz-multimodal-attn/2025-02-03-visualization/PaliGemma2_layer_25_attention_heads.html">click here</a>. My analysis suggests that the PaliGemma2 model, which uses a prefix-attention mask, has trained its <code>&lt;bos&gt;</code> token to be a “broker” token for visual information. Finding key tokens like this has important implications for making VLMs more compute efficient and interpretable. All the code to reproduce the analysis is <a href="https://github.com/tomasruizt/visualizing-multimodal-attn">on Github</a>.</p>
</section>
<section id="mechanistic-interpretability" class="level1">
<h1>Mechanistic Interpretability</h1>
<p>Large language models (LLMs) are notoriously difficult to interpret (black-box). One approach to shed light on LLMs is mechanistic interpretability, which aims to understand the inner workings of the model by breaking down its components. The <a href="https://distill.pub/">distill.pub journal</a> hosted early works on this topic, the team at <a href="https://transformer-circuits.pub/">Anthropic</a> continued the tradition, and today researchers actively contribute to the field.</p>
</section>
<section id="attention-patterns" class="level1">
<h1>Attention Patterns</h1>
<p>The central component of the Transformer architecture is the attention mechanism, which allows the LLM to focus on different parts of the input sequence. Most interpretability research on attention has focused on text-only models, finding e.g.&nbsp;“induction heads”. These are heads that learn to copy part of the input sequence into the output, and form an important mechanism for in-context learning <span class="citation" data-cites="olsson2022incontextlearninginductionheads">(<a href="#ref-olsson2022incontextlearninginductionheads" role="doc-biblioref">Olsson et al. 2022</a>)</span>.</p>
<p>To find such attention patterns, it is essential to have effective visualization tools like the <code>circuitsviz</code> library. The examples below show two different modules in the library to visualize attention over tokens. Each token in the input sequence attends to the all other tokens (therefore the squared shape of the pattern). The attention mechanism determines the color intensity: dark fields mean high attention, white fields means low attention, and gray fields are inactive. Click on any image in this post to see a larger version.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-induction-head" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-induction-head-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/attn-pattern-induction-head.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;1: (Example 1) Induction Head Pattern. The top-right triangle is inactive due to the “causal attention mask” of the Transformer, which prevents tokens from attending to future tokens. We observe a diagonal pattern in blue, which shows the induction head “copying” the sequence, because the tokens are repeating the sequence."><img src="images/attn-pattern-induction-head.png" class="img-fluid figure-img" style="width:80.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-induction-head-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: (Example 1) Induction Head Pattern. The top-right triangle is inactive due to the “causal attention mask” of the Transformer, which prevents tokens from attending to future tokens. We observe a diagonal pattern in blue, which shows the induction head “copying” the sequence, because the tokens are repeating the sequence.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/many-txt-attention-heads.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="(Example 2) Multiple Attention Heads. Each head in the multi-head-attention block of the Transformer learn to attend to different elements in the inputs, and therefore has different attention patterns."><img src="images/many-txt-attention-heads.png" class="img-fluid figure-img" style="width:120.0%" alt="(Example 2) Multiple Attention Heads. Each head in the multi-head-attention block of the Transformer learn to attend to different elements in the inputs, and therefore has different attention patterns."></a></p>
<figcaption>(Example 2) Multiple Attention Heads. Each head in the multi-head-attention block of the Transformer learn to attend to different elements in the inputs, and therefore has different attention patterns.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="multimodal-tokens" class="level1">
<h1>Multimodal Tokens</h1>
<p>But how are images turned into tokens? In contrast to text-only LLMs, VLMs can also process images. A VLM consists of a vision encoder, an LLM and a linear layer to combine both. The vision encoder is a vision transformer (ViT) <span class="citation" data-cites="dosovitskiy2021imageworth16x16words">(<a href="#ref-dosovitskiy2021imageworth16x16words" role="doc-biblioref">Dosovitskiy et al. 2021</a>)</span> that has been pre-trained with (image, text) pairs, like CLIP <span class="citation" data-cites="radford2021learningtransferablevisualmodels">(<a href="#ref-radford2021learningtransferablevisualmodels" role="doc-biblioref">Radford et al. 2021</a>)</span> or SigLIP <span class="citation" data-cites="zhai2023sigmoid">(<a href="#ref-zhai2023sigmoid" role="doc-biblioref">Zhai et al. 2023</a>)</span>. The VLM converts the image into a sequence of image tokens in two steps:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/vision-transformer.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="1. First, The image is cropped into a square and broken down into 16x16=256 non-overlapping patches, which are projected into 256 tokens. The ViT adds one additional token named [CLS] to hold global information about the entire image, for a total of 257 tokens. All patches are then passed through the ViT to create visual embeddings. Image taken from [@dosovitskiy2021imageworth16x16words]"><img src="images/vision-transformer.png" class="img-fluid figure-img" style="width:60.0%" alt="1. First, The image is cropped into a square and broken down into 16x16=256 non-overlapping patches, which are projected into 256 tokens. The ViT adds one additional token named [CLS] to hold global information about the entire image, for a total of 257 tokens. All patches are then passed through the ViT to create visual embeddings. Image taken from (Dosovitskiy et al. 2021)"></a></p>
<figcaption>1. First, The image is cropped into a square and broken down into 16x16=256 non-overlapping patches, which are projected into 256 tokens. The ViT adds one additional token named <code>[CLS]</code> to hold global information about the entire image, for a total of 257 tokens. All patches are then passed through the ViT to create visual embeddings. Image taken from <span class="citation" data-cites="dosovitskiy2021imageworth16x16words">(<a href="#ref-dosovitskiy2021imageworth16x16words" role="doc-biblioref">Dosovitskiy et al. 2021</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/llava-architecture.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="2. Second, the visual embeddings are concatenated with the text embeddings and fed into the LLM. Image taken from [@liu2023visualinstructiontuning]"><img src="images/llava-architecture.png" class="img-fluid figure-img" style="width:60.0%" alt="2. Second, the visual embeddings are concatenated with the text embeddings and fed into the LLM. Image taken from (Liu et al. 2023)"></a></p>
<figcaption>2. Second, the visual embeddings are concatenated with the text embeddings and fed into the LLM. Image taken from <span class="citation" data-cites="liu2023visualinstructiontuning">(<a href="#ref-liu2023visualinstructiontuning" role="doc-biblioref">Liu et al. 2023</a>)</span></figcaption>
</figure>
</div>
<p>In theory, we could visualize the multimodal attention patterns in with the same approach as the text-only pattern, like in <a href="#fig-induction-head" class="quarto-xref">Figure&nbsp;1</a>. But the input sequence is very long now (257 tokens + text tokens), and the pattern grows quadratically with the number of tokens. Also, the image tokens are concatenated by row by row, so their vertical spatial structure is lost in the naive text-only visualization.</p>
</section>
<section id="visualizing-multimodal-attention" class="level1">
<h1>Visualizing Multimodal Attention</h1>
<p>This is where the new visualization shines: It overlays the attention pattern over the image, so we can appreciate the spatial structure of the attention over the image. The main visualization is split in two attention grids: The left grid shows <strong>only a single row</strong> of the image self-attention pattern, rearranged spatially on top of the image. The right grid is the classic self-attention of the text tokens.</p>
<p>By clicking on any token on either grid, the token is selected as the “destination” token, and the left grid switches to that row of the attention pattern. It is possible to tune the contrast of the attention with a slider, to see patterns with lower attention values. See the video below as an example.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/oIhhqn1tDhk" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="case-study-paligemma2" class="level1">
<h1>Case Study: PaliGemma2</h1>
<p>I use the PaliGemma2 VLM <span class="citation" data-cites="steiner2024paligemma2familyversatile">(<a href="#ref-steiner2024paligemma2familyversatile" role="doc-biblioref">Steiner et al. 2024</a>)</span> by Google as my case study, because it does not use a causal attention mask, but a prefix-attention mask. This means that the attention pattern is not triangular, and it means that early tokens can attend to the later tokens. In particular, the image tokens, which are concatenated first in the sequence, can attend to text tokens in the prompt. In contrast to other VLMs, the PaliGemma2 model does not use the <code>[CLS]</code> token of the ViT. However, PaliGemma2 prepends the text prompt with a <code>&lt;bos&gt;</code> (beginning of sentence) token, so the <code>&lt;bos&gt;</code> token becomes the first text token in the input sequence.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/paligemma2-architecture.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="PaliGemma2 architecture. The VLM consists of a vision encoder (SigLIP), an LLM (Gemma2) and a linear layer to combine both. Google released 3 sizes of the model, and with 3 different image resolutions. from [@steiner2024paligemma2familyversatile]"><img src="images/paligemma2-architecture.png" class="img-fluid figure-img" style="width:120.0%" alt="PaliGemma2 architecture. The VLM consists of a vision encoder (SigLIP), an LLM (Gemma2) and a linear layer to combine both. Google released 3 sizes of the model, and with 3 different image resolutions. from (Steiner et al. 2024)"></a></p>
<figcaption>PaliGemma2 architecture. The VLM consists of a vision encoder (SigLIP), an LLM (Gemma2) and a linear layer to combine both. Google released 3 sizes of the model, and with 3 different image resolutions. from <span class="citation" data-cites="steiner2024paligemma2familyversatile">(<a href="#ref-steiner2024paligemma2familyversatile" role="doc-biblioref">Steiner et al. 2024</a>)</span></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/prefix-attn.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="PaliGemma Prefix Attention Pattern. Note that the image tokens and the text tokens (Prefix) fully attend to each other. Only the output tokens (Suffix/Target) have a causal mask. The <eos> token is the “end of sentence” token. Image from [@beyer2024paligemmaversatile3bvlm]"><img src="images/prefix-attn.png" class="img-fluid figure-img" style="width:80.0%" alt="PaliGemma Prefix Attention Pattern. Note that the image tokens and the text tokens (Prefix) fully attend to each other. Only the output tokens (Suffix/Target) have a causal mask. The <eos> token is the “end of sentence” token. Image from (Beyer et al. 2024)"></a></p>
<figcaption>PaliGemma Prefix Attention Pattern. Note that the image tokens and the text tokens (Prefix) fully attend to each other. Only the output tokens (Suffix/Target) have a causal mask. The <code>&lt;eos&gt;</code> token is the “end of sentence” token. Image from <span class="citation" data-cites="beyer2024paligemmaversatile3bvlm">(<a href="#ref-beyer2024paligemmaversatile3bvlm" role="doc-biblioref">Beyer et al. 2024</a>)</span></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>PaliGemma2 uses a special syntax for the prompt: For the model to answer a question in the english (en) language, we must prefix the text question with <code>"Answer en &lt;question&gt;"</code>. For example, given the image of the dog with the frisbee, the model can correctly answer the question <code>"Answer en what is the color of the frisbee?"</code> with <code>"purple"</code>.</p>
<section id="paligemma2s-attention-patterns" class="level2">
<h2 class="anchored" data-anchor-id="paligemma2s-attention-patterns">PaliGemma2’s Attention Patterns</h2>
<p>We now drill down into PaliGemma2’s attention patterns. When looking at the attention patterns, the first thing that jumps out is that the text tokens are not attending to the image tokens very much, because the image is almost completely white (even at zero attention, the image remains visible to prevent it from dissapearing completely). This effect is consistent across layers (See <a href="#fig-paligemma-layer-00" class="quarto-xref">Figure&nbsp;2</a>, <a href="#fig-paligemma-layer-15" class="quarto-xref">Figure&nbsp;3</a>, <a href="#fig-paligemma-layer-25" class="quarto-xref">Figure&nbsp;4</a>). This is surprising, because the question can only be answered by attending to the image. How does then PaliGemma2 answer the question?</p>
<div id="fig-paligemma-layer-00" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-paligemma-layer-00-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/paligemma-layer-00-heads.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;2: Layer 0: Link to full visualization"><img src="images/paligemma-layer-00-heads.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-paligemma-layer-00-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <a href="https://storage.googleapis.com/tomas-ruiz-multimodal-attn/2025-02-03-visualization/PaliGemma2_layer_0_attention_heads.html">Layer 0: Link to full visualization</a>
</figcaption>
</figure>
</div>
<div id="fig-paligemma-layer-15" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-paligemma-layer-15-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/paligemma-layer-15-heads.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;3: Layer 15: Link to full visualization Dark vertical bars, but first row (<bos> token) is white"><img src="images/paligemma-layer-15-heads.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-paligemma-layer-15-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <a href="https://storage.googleapis.com/tomas-ruiz-multimodal-attn/2025-02-03-visualization/PaliGemma2_layer_15_attention_heads.html">Layer 15: Link to full visualization</a> Dark vertical bars, but first row (<code>&lt;bos&gt;</code> token) is white
</figcaption>
</figure>
</div>
<div id="fig-paligemma-layer-25" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-paligemma-layer-25-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/paligemma-layer-25-heads.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;4: Layer 25: Link to full visualization"><img src="images/paligemma-layer-25-heads.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-paligemma-layer-25-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <a href="https://storage.googleapis.com/tomas-ruiz-multimodal-attn/2025-02-03-visualization/PaliGemma2_layer_25_attention_heads.html">Layer 25: Link to full visualization</a>
</figcaption>
</figure>
</div>
<p>In the middle layers (layer 15), vertical bars are visible in almost every head. This indicates that most text tokens are attending to the <code>&lt;bos&gt;</code> token, which is the first token after the image tokens. Interestingly, the <code>&lt;bos&gt;</code> does not attend back to the other text tokens. We can tell because the first row of the colored pattern is completely white (close to 0 attention) in middle and late layers.</p>
<p>So what is the <code>&lt;bos&gt;</code> token attending to? Mostly to image tokens. To see this, I increase the contrast of the attention patterns using the slider and compare the attentions with different destination text tokens. The <code>&lt;bos&gt;</code> token is attending uniformly to many image tokens. The images below are all from intermediate layers (layer 15).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/paligemma-layer-15-dest-token-bos-maxatt-low.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="The <bos> token attends uniformly to many image tokens"><img src="images/paligemma-layer-15-dest-token-bos-maxatt-low.png" class="img-fluid figure-img" alt="The <bos> token attends uniformly to many image tokens"></a></p>
<figcaption>The <code>&lt;bos&gt;</code> token attends uniformly to many image tokens</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/paligemma-layer-15-dest-token-first-maxatt-low.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="The next text token attends sparsely to image tokens"><img src="images/paligemma-layer-15-dest-token-first-maxatt-low.png" class="img-fluid figure-img" alt="The next text token attends sparsely to image tokens"></a></p>
<figcaption>The next text token attends sparsely to image tokens</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/paligemma-layer-15-dest-token-last-maxatt-low.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="The last text token also attends sparsely to image tokens, although more in patches."><img src="images/paligemma-layer-15-dest-token-last-maxatt-low.png" class="img-fluid figure-img" alt="The last text token also attends sparsely to image tokens, although more in patches."></a></p>
<figcaption>The last text token also attends sparsely to image tokens, although more in patches.</figcaption>
</figure>
</div>
<p>This suggests a hypothesis: Namely that the visual information flows from the image tokens into the <code>&lt;bos&gt;</code> token, and then from the <code>&lt;bos&gt;</code> token to the rest of the text tokens. To quantify this, I partition the input into 3 regions: The image tokens, the <code>&lt;bos&gt;</code> token, and the rest of the text tokens. By summing up the attention of the tokens in each region, we get a measure of the attention between regions. This yields a 3x3 matrix, where each row sums up to 1.</p>
<div id="fig-blockwise-attn-sums-frisbee" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-blockwise-attn-sums-frisbee-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/blockwise-attn-sums.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;5: Self-attention over input regions. In lower layers, image tokens mostly attend to each other, despite having access to all tokens. Later, in middle and final layers, image tokens also attend to the <bos> token, (an example of information flowing back from text to image). The <bos> token increasingly attends to the image tokens as depth increases. Text tokens attend as much (or more) to the <bos>token as to the image tokens, despite their ratio being 1:256."><img src="images/blockwise-attn-sums.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-blockwise-attn-sums-frisbee-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Self-attention over input regions. In lower layers, image tokens mostly attend to each other, despite having access to all tokens. Later, in middle and final layers, image tokens also attend to the <code>&lt;bos&gt;</code> token, (an example of information flowing back from text to image). The <code>&lt;bos&gt;</code> token increasingly attends to the image tokens as depth increases. Text tokens attend as much (or more) to the <code>&lt;bos&gt;</code>token as to the image tokens, despite their ratio being 1:256.
</figcaption>
</figure>
</div>
<p>These numbers suggest that <strong>PaliGemma2 has trained the <code>&lt;bos&gt;</code> token to be a “broker” token for visual information:</strong> The <code>&lt;bos&gt;</code> token “collects” and aggregates visual information from the image tokens into a single place, and then “serves” it back to text and image tokens. It plays a similar role as the <code>[CLS]</code> token in the ViT.</p>
</section>
<section id="do-the-numbers-generalize" class="level2">
<h2 class="anchored" data-anchor-id="do-the-numbers-generalize">Do the Numbers Generalize?</h2>
<p>To test if the hypothesis holds in general for (image, text) pairs other than the example of the dog with the frisbee, I ran the analysis on the first 1000 distinct images from the VQA dataset (train) and their corresponding questions. The dataset has multiple questions per image, but I used only the first question so as to have the most visual variation within the 1000 samples.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/vqa-grid-of-img-question-answer.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="VQA images, questions, and model answers. The images are diverse real-life images, and the questions are short open- and closed-ended questions. Note that the questions are formatted in the PaliGemma2 QA format (“Answer en ”)."><img src="images/vqa-grid-of-img-question-answer.png" class="img-fluid figure-img" alt="VQA images, questions, and model answers. The images are diverse real-life images, and the questions are short open- and closed-ended questions. Note that the questions are formatted in the PaliGemma2 QA format (“Answer en ”)."></a></p>
<figcaption>VQA images, questions, and model answers. The images are diverse real-life images, and the questions are short open- and closed-ended questions. Note that the questions are formatted in the PaliGemma2 QA format (“Answer en <question>”).</question></figcaption>
</figure>
</div>
<p>I computed the self-attention matrix over regions for each (image, question) pair and computed the average and the standard deviation over the 1000 pairs. We observe that the standard deviations are very small, indicating that <strong>the “broker” role of the <code>&lt;bos&gt;</code> token is robust and independent of the image and question.</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/blockwise-attn-sums-vqa1000.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Self-attention over input regions for 1000 VQA samples. The standard deviations are small. The numbers for previous example (purple frisbee) in Figure&nbsp;5 are mostly within the 1-\sigma confidence interval here, suggesting it is a typical example."><img src="images/blockwise-attn-sums-vqa1000.png" class="img-fluid figure-img" alt="Self-attention over input regions for 1000 VQA samples. The standard deviations are small. The numbers for previous example (purple frisbee) in Figure&nbsp;5 are mostly within the 1-\sigma confidence interval here, suggesting it is a typical example."></a></p>
<figcaption>Self-attention over input regions for 1000 VQA samples. The standard deviations are small. The numbers for previous example (purple frisbee) in <a href="#fig-blockwise-attn-sums-frisbee" class="quarto-xref">Figure&nbsp;5</a> are mostly within the 1-<span class="math inline">\(\sigma\)</span> confidence interval here, suggesting it is a typical example.</figcaption>
</figure>
</div>
</section>
</section>
<section id="conclusion-and-outlook" class="level1">
<h1>Conclusion and Outlook</h1>
<p>I showed how to visualize multimodal attention patterns using the new module for <code>circuitsviz</code>, which is useful for exploratory work in interpretability. I used PaliGemma2 as an interesting case study, because of its prefix-attention mask. After inspecting the attention patterns, I hypothesized that the <code>&lt;bos&gt;</code> token is trained to be a “broker” token for visual information, and I showed that this phenomenon is independent of the input image and question on VQA.</p>
<p><strong>Yet, more analysis remains to be done:</strong> If the token is truly a “broker” token, then visual information flow should be disrupted if this token is causally intervened on (patching). It is also possible that the “broker” role is not tied to the <code>&lt;bos&gt;</code> token specifically, but to the first text token in the input (whatever it is). Finding key tokens in VLMs has been useful to improve their efficiency because the less important tokens can be pruned away and don’t have to be computed <span class="citation" data-cites="chen2024imageworth12tokens">(<a href="#ref-chen2024imageworth12tokens" role="doc-biblioref">Chen et al. 2024</a>)</span> <span class="citation" data-cites="wang2024clstokentellsneeded">(<a href="#ref-wang2024clstokentellsneeded" role="doc-biblioref">Wang et al. 2024</a>)</span>. We saw in our example that the image tokens outnumber the text tokens (around 256 to 15). This problem is worsened by the quadratic growth of the attention pattern, so pruning image tokens greatly reduces the compute and memory footprint of the model.</p>
<p>Finally, by understanding the mechanisms by which VLMs process visual information, as well as their information bottlenecks, we can monitor them better and make their usage more reliable and safe. We can also control them more easily, for example by intervening on the activations of key tokens when necessary, ultimately improving their safety once deployed.</p>
<section id="acknowledgement" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgement">Acknowledgement</h2>
<p>This is the final project for the course “Artificial Intelligence Safety Fundamentals” <a href="https://aisafetyfundamentals.com/">(AISF)</a> by BlueDot Impact. The author is funded by the Bavarian Research Institute for Digital Transformation (bidt) and the Ludwig Maximilian University of Munich.</p>
</section>
</section>
<section id="references" class="level1">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-beyer2024paligemmaversatile3bvlm" class="csl-entry" role="listitem">
Beyer, Lucas, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, et al. 2024. <span>“PaliGemma: A Versatile 3B VLM for Transfer.”</span> <a href="https://arxiv.org/abs/2407.07726">https://arxiv.org/abs/2407.07726</a>.
</div>
<div id="ref-chen2024imageworth12tokens" class="csl-entry" role="listitem">
Chen, Liang, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. 2024. <span>“An Image Is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models.”</span> <a href="https://arxiv.org/abs/2403.06764">https://arxiv.org/abs/2403.06764</a>.
</div>
<div id="ref-dosovitskiy2021imageworth16x16words" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.
</div>
<div id="ref-liu2023visualinstructiontuning" class="csl-entry" role="listitem">
Liu, Haotian, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. <span>“Visual Instruction Tuning.”</span> <a href="https://arxiv.org/abs/2304.08485">https://arxiv.org/abs/2304.08485</a>.
</div>
<div id="ref-olsson2022incontextlearninginductionheads" class="csl-entry" role="listitem">
Olsson, Catherine, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, et al. 2022. <span>“In-Context Learning and Induction Heads.”</span> <a href="https://arxiv.org/abs/2209.11895">https://arxiv.org/abs/2209.11895</a>.
</div>
<div id="ref-radford2021learningtransferablevisualmodels" class="csl-entry" role="listitem">
Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. <span>“Learning Transferable Visual Models from Natural Language Supervision.”</span> <a href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a>.
</div>
<div id="ref-steiner2024paligemma2familyversatile" class="csl-entry" role="listitem">
Steiner, Andreas, André Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, et al. 2024. <span>“PaliGemma 2: A Family of Versatile VLMs for Transfer.”</span> <a href="https://arxiv.org/abs/2412.03555">https://arxiv.org/abs/2412.03555</a>.
</div>
<div id="ref-wang2024clstokentellsneeded" class="csl-entry" role="listitem">
Wang, Ao, Fengyuan Sun, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding. 2024. <span>“[CLS] Token Tells Everything Needed for Training-Free Efficient MLLMs.”</span> <a href="https://arxiv.org/abs/2412.05819">https://arxiv.org/abs/2412.05819</a>.
</div>
<div id="ref-zhai2023sigmoid" class="csl-entry" role="listitem">
Zhai, Xiaohua, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. <span>“Sigmoid Loss for Language Image Pre-Training.”</span> In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 11975–86.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{ruiz2025,
  author = {Ruiz, Tomas},
  title = {Drilling {Down} into {Multimodal} {Attention}},
  date = {2025-02-01},
  url = {https://tomasruizt.github.io/posts/multimodal-attn/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-ruiz2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Ruiz, Tomas. 2025. <span>“Drilling Down into Multimodal
Attention.”</span> February 1, 2025. <a href="https://tomasruizt.github.io/posts/multimodal-attn/">https://tomasruizt.github.io/posts/multimodal-attn/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/tomasruizt\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="tomasruizt/tomasruizt.github.io" data-repo-id="R_kgDOMeb1fA" data-category="Announcements" data-category-id="DIC_kwDOMeb1fM4Cv1gL" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="light">
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"openEffect":"zoom","descPosition":"bottom","selector":".lightbox","loop":false,"closeEffect":"zoom"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>