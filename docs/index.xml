<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>All Posts</title>
<link>https://tomasruizt.github.io/</link>
<atom:link href="https://tomasruizt.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Tomas Ruiz&#39;s blog</description>
<generator>quarto-1.5.57</generator>
<lastBuildDate>Fri, 31 Jan 2025 23:00:00 GMT</lastBuildDate>
<item>
  <title>Drilling Down into Multimodal Attention</title>
  <dc:creator>Tomas Ruiz</dc:creator>
  <link>https://tomasruizt.github.io/posts/multimodal-attn/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/multimodal-attn.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Visualizing Multimodal Attention Patterns"><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/multimodal-attn.png" class="img-fluid figure-img" style="width:70.0%" data-text-align="left" alt="Visualizing Multimodal Attention"></a></p>
<figcaption>Visualizing Multimodal Attention Patterns</figcaption>
</figure>
</div>
<section id="summary" class="level1">
<h1>Summary</h1>
<p>This post explains how to inspect the attention patterns of a vision-language models (VLMs) using a new module I created <a href="https://github.com/tomasruizt/CircuitsVis">on a fork</a> of the <code>circuitsviz</code> library. To interact with an example, <a href="https://storage.googleapis.com/tomas-ruiz-multimodal-attn/2025-02-03-visualization/PaliGemma2_layer_25_attention_heads.html">click here</a>. My analysis suggests that the PaliGemma2 model, which uses a prefix-attention mask, has trained its <code>&lt;bos&gt;</code> token to be a “broker” token for visual information. Finding key tokens like this has important implications for making VLMs more compute efficient and interpretable. All code to reproduce the analysiscan be found <a href="https://github.com/tomasruizt/visualizing-multimodal-attn">on Github</a>.</p>
</section>
<section id="mechanistic-interpretability" class="level1">
<h1>Mechanistic Interpretability</h1>
<p>Large language models (LLMs) are notoriously difficult to interpret (black-box). One approach to shed light on LLMs is mechanistic interpretability, which aims to understand the inner workings of the model by breaking down its components. The <a href="https://distill.pub/">distill.pub journal</a> hosted early works on this topic, the team at <a href="https://transformer-circuits.pub/">Anthropic</a> continued the tradition, and today researchers actively contribute to the field.</p>
</section>
<section id="attention-patterns" class="level1">
<h1>Attention Patterns</h1>
<p>The central component of the Transformer architecture is the attention mechanism, which allows the LLM to focus on different parts of the input sequence. Most interpretability research on attention has focused on text-only models, finding e.g.&nbsp;“induction heads”. These are heads that learn to copy part of the input sequence into the output, and form an important mechanism for in-context learning <span class="citation" data-cites="olsson2022incontextlearninginductionheads">(Olsson et al. 2022)</span>.</p>
<p>To find such attention patterns, it is essential to have effective visualization tools like the <code>circuitsviz</code> library. The examples below show two different modules in the library to visualize attention over tokens. Each token in the input sequence attends to the all other tokens (therefore the squared shape of the pattern). The attention mechanism determines the color intensity: dark fields mean high attention, white fields means low attention, and gray fields are inactive. Click on any image in this post to see a larger version.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-induction-head" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-induction-head-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/attn-pattern-induction-head.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;1: (Example 1) Induction Head Pattern. The top-right triangle is inactive due to the “causal attention mask” of the Transformer, which prevents tokens from attending to future tokens. We observe a diagonal pattern in blue, which shows the induction head “copying” the sequence, because the tokens are repeating the sequence."><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/attn-pattern-induction-head.png" class="img-fluid figure-img" style="width:80.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-induction-head-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: (Example 1) Induction Head Pattern. The top-right triangle is inactive due to the “causal attention mask” of the Transformer, which prevents tokens from attending to future tokens. We observe a diagonal pattern in blue, which shows the induction head “copying” the sequence, because the tokens are repeating the sequence.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/many-txt-attention-heads.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="(Example 2) Multiple Attention Heads. Each head in the multi-head-attention block of the Transformer learn to attend to different elements in the inputs, and therefore has different attention patterns."><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/many-txt-attention-heads.png" class="img-fluid figure-img" style="width:120.0%" alt="(Example 2) Multiple Attention Heads. Each head in the multi-head-attention block of the Transformer learn to attend to different elements in the inputs, and therefore has different attention patterns."></a></p>
<figcaption>(Example 2) Multiple Attention Heads. Each head in the multi-head-attention block of the Transformer learn to attend to different elements in the inputs, and therefore has different attention patterns.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="multimodal-tokens" class="level1">
<h1>Multimodal Tokens</h1>
<p>But how are images turned into tokens? In contrast to text-only LLMs, VLMs can also process images. A VLM consists of a vision encoder, an LLM and a linear layer to combine both. The vision encoder is a vision transformer (ViT) <span class="citation" data-cites="dosovitskiy2021imageworth16x16words">(Dosovitskiy et al. 2021)</span> that has been pre-trained with (image, text) pairs, like CLIP <span class="citation" data-cites="radford2021learningtransferablevisualmodels">(Radford et al. 2021)</span> or SigLIP <span class="citation" data-cites="zhai2023sigmoid">(Zhai et al. 2023)</span>. The VLM converts the image into a sequence of image tokens in two steps:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/vision-transformer.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="1. First, The image is cropped into a square and broken down into 16x16=256 non-overlapping patches, which are projected into 256 tokens. The ViT adds one additional token named [CLS] to hold global information about the entire image, for a total of 257 tokens. All patches are then passed through the ViT to create visual embeddings. Image taken from [@dosovitskiy2021imageworth16x16words]"><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/vision-transformer.png" class="img-fluid figure-img" style="width:60.0%" alt="1. First, The image is cropped into a square and broken down into 16x16=256 non-overlapping patches, which are projected into 256 tokens. The ViT adds one additional token named [CLS] to hold global information about the entire image, for a total of 257 tokens. All patches are then passed through the ViT to create visual embeddings. Image taken from (Dosovitskiy et al. 2021)"></a></p>
<figcaption>1. First, The image is cropped into a square and broken down into 16x16=256 non-overlapping patches, which are projected into 256 tokens. The ViT adds one additional token named <code>[CLS]</code> to hold global information about the entire image, for a total of 257 tokens. All patches are then passed through the ViT to create visual embeddings. Image taken from <span class="citation" data-cites="dosovitskiy2021imageworth16x16words">(Dosovitskiy et al. 2021)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/llava-architecture.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="2. Second, the visual embeddings are concatenated with the text embeddings and fed into the LLM. Image taken from [@liu2023visualinstructiontuning]"><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/llava-architecture.png" class="img-fluid figure-img" style="width:60.0%" alt="2. Second, the visual embeddings are concatenated with the text embeddings and fed into the LLM. Image taken from (Liu et al. 2023)"></a></p>
<figcaption>2. Second, the visual embeddings are concatenated with the text embeddings and fed into the LLM. Image taken from <span class="citation" data-cites="liu2023visualinstructiontuning">(Liu et al. 2023)</span></figcaption>
</figure>
</div>
<p>In theory, we could visualize the multimodal attention patterns in with the same approach as the text-only pattern, like in Figure&nbsp;1. But the input sequence is very long now (257 tokens + text tokens), and the pattern grows quadratically with the number of tokens. Also, the image tokens are concatenated by row by row, so their vertical spatial structure is lost in the naive text-only visualization.</p>
</section>
<section id="visualizing-multimodal-attention" class="level1">
<h1>Visualizing Multimodal Attention</h1>
<p>This is where the new visualization shines: It overlays the attention pattern over the image, so we can appreciate the spatial structure of the attention over the image. The main visualization is split in two attention grids: The left grid shows <strong>only a single row</strong> of the image self-attention pattern, rearranged spatially on top of the image. The right grid is the classic self-attention of the text tokens.</p>
<p>By clicking on any token on either grid, the token is selected as the “destination” token, and the left grid switches to that row of the attention pattern. It is possible to tune the contrast of the attention with a slider, to see patterns with lower attention values. See the video below as an example.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/oIhhqn1tDhk" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="case-study-paligemma2" class="level1">
<h1>Case Study: PaliGemma2</h1>
<p>I use the PaliGemma2 VLM <span class="citation" data-cites="steiner2024paligemma2familyversatile">(Steiner et al. 2024)</span> by Google as my case study, because it does not use a causal attention mask, but a prefix-attention mask. This means that the attention pattern is not triangular, and it means that early tokens can attend to the later tokens. In particular, the image tokens, which are concatenated first in the sequence, can attend to text tokens in the prompt. In contrast to other VLMs, the PaliGemma2 model does not use the <code>[CLS]</code> token of the ViT. However, PaliGemma2 prepends the text prompt with a <code>&lt;bos&gt;</code> (beginning of sentence) token, so the <code>&lt;bos&gt;</code> token becomes the first text token in the input sequence.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/paligemma2-architecture.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="PaliGemma2 architecture. The VLM consists of a vision encoder (SigLIP), an LLM (Gemma2) and a linear layer to combine both. Google released 3 sizes of the model, and with 3 different image resolutions. from [@steiner2024paligemma2familyversatile]"><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/paligemma2-architecture.png" class="img-fluid figure-img" style="width:120.0%" alt="PaliGemma2 architecture. The VLM consists of a vision encoder (SigLIP), an LLM (Gemma2) and a linear layer to combine both. Google released 3 sizes of the model, and with 3 different image resolutions. from (Steiner et al. 2024)"></a></p>
<figcaption>PaliGemma2 architecture. The VLM consists of a vision encoder (SigLIP), an LLM (Gemma2) and a linear layer to combine both. Google released 3 sizes of the model, and with 3 different image resolutions. from <span class="citation" data-cites="steiner2024paligemma2familyversatile">(Steiner et al. 2024)</span></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/prefix-attn.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="PaliGemma Prefix Attention Pattern. Note that the image tokens and the text tokens (Prefix) fully attend to each other. Only the output tokens (Suffix/Target) have a causal mask. The <eos> token is the “end of sentence” token. Image from [@beyer2024paligemmaversatile3bvlm]"><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/prefix-attn.png" class="img-fluid figure-img" style="width:80.0%" alt="PaliGemma Prefix Attention Pattern. Note that the image tokens and the text tokens (Prefix) fully attend to each other. Only the output tokens (Suffix/Target) have a causal mask. The <eos> token is the “end of sentence” token. Image from (Beyer et al. 2024)"></a></p>
<figcaption>PaliGemma Prefix Attention Pattern. Note that the image tokens and the text tokens (Prefix) fully attend to each other. Only the output tokens (Suffix/Target) have a causal mask. The <code>&lt;eos&gt;</code> token is the “end of sentence” token. Image from <span class="citation" data-cites="beyer2024paligemmaversatile3bvlm">(Beyer et al. 2024)</span></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>PaliGemma2 uses a special syntax for the prompt: For the model to answer a question in the english (en) language, we must prefix the text question with <code>"Answer en &lt;question&gt;"</code>. For example, given the image of the dog with the frisbee, the model can correctly answer the question <code>"Answer en what is the color of the frisbee?"</code> with <code>"purple"</code>.</p>
<section id="paligemma2s-attention-patterns" class="level2">
<h2 class="anchored" data-anchor-id="paligemma2s-attention-patterns">PaliGemma2’s Attention Patterns</h2>
<p>We now drill down into PaliGemma2’s attention patterns. When looking at the attention patterns, the first thing that jumps out is that the text tokens are not attending to the image tokens very much, because the image is almost completely white (even at zero attention, the image remains visible to prevent it from dissapearing completely). This effect is consistent across layers (See Figure&nbsp;2, Figure&nbsp;3, Figure&nbsp;4). This is surprising, because the question can only be answered by attending to the image. How does then PaliGemma2 answer the question?</p>
<div id="fig-paligemma-layer-00" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-paligemma-layer-00-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/paligemma-layer-00-heads.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;2: Layer 0: Link to full visualization"><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/paligemma-layer-00-heads.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-paligemma-layer-00-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <a href="https://storage.googleapis.com/tomas-ruiz-multimodal-attn/2025-02-03-visualization/PaliGemma2_layer_0_attention_heads.html">Layer 0: Link to full visualization</a>
</figcaption>
</figure>
</div>
<div id="fig-paligemma-layer-15" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-paligemma-layer-15-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/paligemma-layer-15-heads.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;3: Layer 15: Link to full visualization Dark vertical bars, but first row (<bos> token) is white"><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/paligemma-layer-15-heads.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-paligemma-layer-15-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <a href="https://storage.googleapis.com/tomas-ruiz-multimodal-attn/2025-02-03-visualization/PaliGemma2_layer_15_attention_heads.html">Layer 15: Link to full visualization</a> Dark vertical bars, but first row (<code>&lt;bos&gt;</code> token) is white
</figcaption>
</figure>
</div>
<div id="fig-paligemma-layer-25" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-paligemma-layer-25-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/paligemma-layer-25-heads.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;4: Layer 25: Link to full visualization"><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/paligemma-layer-25-heads.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-paligemma-layer-25-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <a href="https://storage.googleapis.com/tomas-ruiz-multimodal-attn/2025-02-03-visualization/PaliGemma2_layer_25_attention_heads.html">Layer 25: Link to full visualization</a>
</figcaption>
</figure>
</div>
<p>In the middle layers (layer 15), vertical bars are visible in almost every head. This indicates that most text tokens are attending to the <code>&lt;bos&gt;</code> token, which is the first token after the image tokens. Interestingly, the <code>&lt;bos&gt;</code> does not attend back to the other text tokens. We can tell because the first row of the colored pattern is completely white (close to 0 attention) in middle and late layers.</p>
<p>So what is the <code>&lt;bos&gt;</code> token attending to? Mostly to image tokens. To see this, I increase the contrast of the attention patterns using the slider and compare the attentions with different destination text tokens. The <code>&lt;bos&gt;</code> token is attending uniformly to many image tokens. The images below are all from intermediate layers (layer 15).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/paligemma-layer-15-dest-token-bos-maxatt-low.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="The <bos> token attends uniformly to many image tokens"><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/paligemma-layer-15-dest-token-bos-maxatt-low.png" class="img-fluid figure-img" alt="The <bos> token attends uniformly to many image tokens"></a></p>
<figcaption>The <code>&lt;bos&gt;</code> token attends uniformly to many image tokens</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/paligemma-layer-15-dest-token-first-maxatt-low.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="The next text token attends sparsely to image tokens"><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/paligemma-layer-15-dest-token-first-maxatt-low.png" class="img-fluid figure-img" alt="The next text token attends sparsely to image tokens"></a></p>
<figcaption>The next text token attends sparsely to image tokens</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/paligemma-layer-15-dest-token-last-maxatt-low.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="The last text token also attends sparsely to image tokens, although more in patches."><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/paligemma-layer-15-dest-token-last-maxatt-low.png" class="img-fluid figure-img" alt="The last text token also attends sparsely to image tokens, although more in patches."></a></p>
<figcaption>The last text token also attends sparsely to image tokens, although more in patches.</figcaption>
</figure>
</div>
<p>This suggests a hypothesis: Namely that the visual information flows from the image tokens into the <code>&lt;bos&gt;</code> token, and then from the <code>&lt;bos&gt;</code> token to the rest of the text tokens. To quantify this, I partition the input into 3 regions: The image tokens, the <code>&lt;bos&gt;</code> token, and the rest of the text tokens. By summing up the attention of the tokens in each region, we get a measure of the attention between regions. This yields a 3x3 matrix, where each row sums up to 1.</p>
<div id="fig-blockwise-attn-sums-frisbee" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-blockwise-attn-sums-frisbee-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/blockwise-attn-sums.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;5: Self-attention over input regions. In lower layers, image tokens mostly attend to each other, despite having access to all tokens. Later, in middle and final layers, image tokens also attend to the <bos> token, (an example of information flowing back from text to image). The <bos> token increasingly attends to the image tokens as depth increases. Text tokens attend as much (or more) to the <bos>token as to the image tokens, despite their ratio being 1:256."><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/blockwise-attn-sums.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-blockwise-attn-sums-frisbee-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Self-attention over input regions. In lower layers, image tokens mostly attend to each other, despite having access to all tokens. Later, in middle and final layers, image tokens also attend to the <code>&lt;bos&gt;</code> token, (an example of information flowing back from text to image). The <code>&lt;bos&gt;</code> token increasingly attends to the image tokens as depth increases. Text tokens attend as much (or more) to the <code>&lt;bos&gt;</code>token as to the image tokens, despite their ratio being 1:256.
</figcaption>
</figure>
</div>
<p>These numbers suggest that <strong>PaliGemma2 has trained the <code>&lt;bos&gt;</code> token to be a “broker” token for visual information:</strong> The <code>&lt;bos&gt;</code> token “collects” and aggregates visual information from the image tokens into a single place, and then “serves” it back to text and image tokens. It plays a similar role as the <code>[CLS]</code> token in the ViT.</p>
</section>
<section id="do-the-numbers-generalize" class="level2">
<h2 class="anchored" data-anchor-id="do-the-numbers-generalize">Do the Numbers Generalize?</h2>
<p>To test if the hypothesis holds in general for (image, text) pairs other than the example of the dog with the frisbee, I ran the analysis on the first 1000 distinct images from the VQA dataset (train) and their corresponding questions. The dataset has multiple questions per image, but I used only the first question so as to have the most visual variation within the 1000 samples.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/vqa-grid-of-img-question-answer.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="VQA images, questions, and model answers. The images are diverse real-life images, and the questions are short open- and closed-ended questions. Note that the questions are formatted in the PaliGemma2 QA format (“Answer en ”)."><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/vqa-grid-of-img-question-answer.png" class="img-fluid figure-img" alt="VQA images, questions, and model answers. The images are diverse real-life images, and the questions are short open- and closed-ended questions. Note that the questions are formatted in the PaliGemma2 QA format (“Answer en ”)."></a></p>
<figcaption>VQA images, questions, and model answers. The images are diverse real-life images, and the questions are short open- and closed-ended questions. Note that the questions are formatted in the PaliGemma2 QA format (“Answer en <question>”).</question></figcaption>
</figure>
</div>
<p>I computed the self-attention matrix over regions for each (image, question) pair and computed the average and the standard deviation over the 1000 pairs. We observe that the standard deviations are very small, indicating that <strong>the “broker” role of the <code>&lt;bos&gt;</code> token is robust and independent of the image and question.</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/blockwise-attn-sums-vqa1000.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Self-attention over input regions for 1000 VQA samples. The standard deviations are small. The numbers for previous example (purple frisbee) in Figure&nbsp;5 are mostly within the 1-\sigma confidence interval here, suggesting it is a typical example."><img src="https://tomasruizt.github.io/posts/multimodal-attn/images/blockwise-attn-sums-vqa1000.png" class="img-fluid figure-img" alt="Self-attention over input regions for 1000 VQA samples. The standard deviations are small. The numbers for previous example (purple frisbee) in Figure&nbsp;5 are mostly within the 1-\sigma confidence interval here, suggesting it is a typical example."></a></p>
<figcaption>Self-attention over input regions for 1000 VQA samples. The standard deviations are small. The numbers for previous example (purple frisbee) in Figure&nbsp;5 are mostly within the 1-<img src="https://latex.codecogs.com/png.latex?%5Csigma"> confidence interval here, suggesting it is a typical example.</figcaption>
</figure>
</div>
</section>
</section>
<section id="conclusion-and-outlook" class="level1">
<h1>Conclusion and Outlook</h1>
<p>I showed how to visualize multimodal attention patterns using the new module for <code>circuitsviz</code>, which is useful for exploratory work in interpretability. I used PaliGemma2 as an interesting case study, because of its prefix-attention mask. After inspecting the attention patterns, I hypothesized that the <code>&lt;bos&gt;</code> token is trained to be a “broker” token for visual information, and I showed that this phenomenon is independent of the input image and question on VQA.</p>
<p><strong>Yet, more analysis remains to be done:</strong> If the token is truly a “broker” token, then visual information flow should be disrupted if this token is causally intervened on (patching). It is also possible that the “broker” role is not tied to the <code>&lt;bos&gt;</code> token specifically, but to the first text token in the input (whatever it is). Finding key tokens in VLMs has been useful to improve their efficiency because the less important tokens can be pruned away and don’t have to be computed <span class="citation" data-cites="chen2024imageworth12tokens">(Chen et al. 2024)</span> <span class="citation" data-cites="wang2024clstokentellsneeded">(Wang et al. 2024)</span>. We saw in our example that the image tokens outnumber the text tokens (around 256 to 15). This problem is worsened by the quadratic growth of the attention pattern, so pruning image tokens greatly reduces the compute and memory footprint of the model.</p>
<p>Finally, by understanding the mechanisms by which VLMs process visual information, as well as their information bottlenecks, we can monitor them better and make their usage more reliable and safe. We can also control them more easily, for example by intervening on the activations of key tokens when necessary, ultimately improving their safety once deployed.</p>
<section id="acknowledgement" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgement">Acknowledgement</h2>
<p>This is the final project for the course “Artificial Intelligence Safety Fundamentals” <a href="https://aisafetyfundamentals.com/">(AISF)</a> by BlueDot Impact. The author is funded by the Bavarian Research Institute for Digital Transformation (bidt) and the Ludwig Maximilian University of Munich.</p>
</section>
</section>
<section id="references" class="level1">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-beyer2024paligemmaversatile3bvlm" class="csl-entry">
Beyer, Lucas, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, et al. 2024. <span>“PaliGemma: A Versatile 3B VLM for Transfer.”</span> <a href="https://arxiv.org/abs/2407.07726">https://arxiv.org/abs/2407.07726</a>.
</div>
<div id="ref-chen2024imageworth12tokens" class="csl-entry">
Chen, Liang, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. 2024. <span>“An Image Is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models.”</span> <a href="https://arxiv.org/abs/2403.06764">https://arxiv.org/abs/2403.06764</a>.
</div>
<div id="ref-dosovitskiy2021imageworth16x16words" class="csl-entry">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.
</div>
<div id="ref-liu2023visualinstructiontuning" class="csl-entry">
Liu, Haotian, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. <span>“Visual Instruction Tuning.”</span> <a href="https://arxiv.org/abs/2304.08485">https://arxiv.org/abs/2304.08485</a>.
</div>
<div id="ref-olsson2022incontextlearninginductionheads" class="csl-entry">
Olsson, Catherine, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, et al. 2022. <span>“In-Context Learning and Induction Heads.”</span> <a href="https://arxiv.org/abs/2209.11895">https://arxiv.org/abs/2209.11895</a>.
</div>
<div id="ref-radford2021learningtransferablevisualmodels" class="csl-entry">
Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. <span>“Learning Transferable Visual Models from Natural Language Supervision.”</span> <a href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a>.
</div>
<div id="ref-steiner2024paligemma2familyversatile" class="csl-entry">
Steiner, Andreas, André Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, et al. 2024. <span>“PaliGemma 2: A Family of Versatile VLMs for Transfer.”</span> <a href="https://arxiv.org/abs/2412.03555">https://arxiv.org/abs/2412.03555</a>.
</div>
<div id="ref-wang2024clstokentellsneeded" class="csl-entry">
Wang, Ao, Fengyuan Sun, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding. 2024. <span>“[CLS] Token Tells Everything Needed for Training-Free Efficient MLLMs.”</span> <a href="https://arxiv.org/abs/2412.05819">https://arxiv.org/abs/2412.05819</a>.
</div>
<div id="ref-zhai2023sigmoid" class="csl-entry">
Zhai, Xiaohua, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. <span>“Sigmoid Loss for Language Image Pre-Training.”</span> In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 11975–86.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{ruiz2025,
  author = {Ruiz, Tomas},
  title = {Drilling {Down} into {Multimodal} {Attention}},
  date = {2025-02-01},
  url = {https://tomasruizt.github.io/posts/multimodal-attn/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-ruiz2025" class="csl-entry quarto-appendix-citeas">
Ruiz, Tomas. 2025. <span>“Drilling Down into Multimodal
Attention.”</span> February 1, 2025. <a href="https://tomasruizt.github.io/posts/multimodal-attn/">https://tomasruizt.github.io/posts/multimodal-attn/</a>.
</div></div></section></div> ]]></description>
  <category>Transformers</category>
  <category>Attention</category>
  <guid>https://tomasruizt.github.io/posts/multimodal-attn/</guid>
  <pubDate>Fri, 31 Jan 2025 23:00:00 GMT</pubDate>
</item>
<item>
  <title>How Does Tiling Speed Up Matrix Multiplications on GPUs?</title>
  <dc:creator>Tomas Ruiz</dc:creator>
  <link>https://tomasruizt.github.io/posts/tiling-for-matrix-mult/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="tiled-matrix-multiplication.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Tiled MatMul according to ChatGPT"><img src="https://tomasruizt.github.io/posts/tiling-for-matrix-mult/tiled-matrix-multiplication.webp" class="img-fluid figure-img" style="width:100.0%" data-text-align="center" alt="Tiled Matrix Multiplication"></a></p>
<figcaption>Tiled MatMul according to ChatGPT</figcaption>
</figure>
</div>
<p><strong>TL;DR:</strong> Tiling is a technique used to reduce the number of memory accesses performed during matrix multiplication. We see how it improves compute intensity and how it speeds up the matrix multiplication operation, not only on CPUs, but also on GPUs. I also provide a simple implementation of tiling in CUDA C.</p>
<section id="matrix-multiplication-recap" class="level1">
<h1>Matrix Multiplication Recap</h1>
<p>Matrix multiplication, where <img src="https://latex.codecogs.com/png.latex?AB%20=%20C">, computes each element of <img src="https://latex.codecogs.com/png.latex?C"> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D%20=%20%5Csum_%7Bk=1%7D%5En%20A_%7Bik%7D%20B_%7Bkj%7D"></p>
<p>For simplicity, assume <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> are square matrices of size <img src="https://latex.codecogs.com/png.latex?n">. Below is basic pseudocode for the operation, which involves <img src="https://latex.codecogs.com/png.latex?2n%5E3"> floating-point operations (flops), because of the triple nested loop:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n):</span>
<span id="cb1-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n):</span>
<span id="cb1-3">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n):</span>
<span id="cb1-4">            C[i][j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> A[i][k] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> B[k][j]</span></code></pre></div>
<p>The code is correct, but it is inefficient in terms of its memory access pattern. Let’s see why.</p>
</section>
<section id="compute-intensity" class="level1">
<h1>Compute Intensity</h1>
<p>Besides the count of flops, the performance of a matmul is also determined by the memory access pattern. The matmul has to fetch data from main memory into a fast cache (L1 or L2), compute, and then return the result to main memory. This roundtrip is time-consuming, and the cores can become idle waiting for the data from main memory. If so, the <strong>memory bandwidth</strong> becomes the bottleneck of the algorithm.</p>
<p>Key Concept: <strong>Compute Intensity</strong>. The compute intensity ratio, defined as flops per memory transfer, indicates whether an algorithm is limited by memory bandwidth or compute power. This concept originates from the Roofline model <span class="citation" data-cites="williams2009roofline">(Williams, Waterman, and Patterson 2009)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>To boost performance, maximize compute intensity by reducing memory transfers per flop.</p>
</div>
</div>
</section>
<section id="memory-access-analysis" class="level1">
<h1>Memory Access Analysis</h1>
<section id="naive-memory-access" class="level2">
<h2 class="anchored" data-anchor-id="naive-memory-access">Naive Memory Access</h2>
<p>In the naive approach, computing <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D"> requires fetching <img src="https://latex.codecogs.com/png.latex?n"> elements from <img src="https://latex.codecogs.com/png.latex?A"> (a row) and <img src="https://latex.codecogs.com/png.latex?n"> elements from <img src="https://latex.codecogs.com/png.latex?B"> (a column). That makes <img src="https://latex.codecogs.com/png.latex?2n"> memory accesses. <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D"> is computed as the dot product of the row and the column, which requires <img src="https://latex.codecogs.com/png.latex?2n"> flops (1 mult and 1 add, <img src="https://latex.codecogs.com/png.latex?n"> times). Thus, the compute intensity is: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BCompute%20Intensity%7D%20=%20%5Cfrac%7B%5Ctext%7Bflops%7D%7D%7B%5Ctext%7Bmemory%20transfers%7D%7D%20=%20%5Cfrac%7B2n%7D%7B2n%7D%20=%201%0A"></p>
<p>Can we do better? In this naive implementation, we are performing redundant memory transfers: For example, to compute <img src="https://latex.codecogs.com/png.latex?C_%7B11%7D">, and <img src="https://latex.codecogs.com/png.latex?C_%7B12%7D">, we fetch the first row of <img src="https://latex.codecogs.com/png.latex?A"> twice from main memory.</p>
</section>
<section id="optimal-memory-access" class="level2">
<h2 class="anchored" data-anchor-id="optimal-memory-access">Optimal Memory Access</h2>
<p>If our cache was (theoretically) large enough, we would transfer all elements of <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> into the cache at once (<img src="https://latex.codecogs.com/png.latex?2n%5E2"> transfers) and perform the full matrix multiplication (<img src="https://latex.codecogs.com/png.latex?2n%5E3"> flops).</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BCompute%20Intensity%7D%20=%20%5Cfrac%7B%5Ctext%7Bflops%7D%7D%7B%5Ctext%7Bmemory%20transfers%7D%7D%20=%20%5Cfrac%7B2n%5E3%7D%7B2n%5E2%7D%20=%20n%0A"> The larger the matrices, the higher the compute intensity.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>Assuming a matrix size of <img src="https://latex.codecogs.com/png.latex?n=10,000">, the naive approach does 1 flop per memory transfer, while the optimal approach does 10,000 flops per memory transfer. This would result in a 10,000x speedup, assuming memory bandwidth remained the bottleneck.</p>
</div>
</div>
</section>
<section id="a-middle-ground-tiling" class="level2">
<h2 class="anchored" data-anchor-id="a-middle-ground-tiling">A Middle Ground: Tiling</h2>
<p>Since caching entire matrices is impractical, we divide matrices <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> into smaller square blocks of size <img src="https://latex.codecogs.com/png.latex?r">, called <strong>tiles</strong>, perform block-wise multiplications, and aggregate the results.</p>
<p>But how does block-wise matrix multiplication work? Let’s go through an example to gain some intuition. We break the matrices <img src="https://latex.codecogs.com/png.latex?A">, <img src="https://latex.codecogs.com/png.latex?B">, and <img src="https://latex.codecogs.com/png.latex?C"> into 4 blocks each (2x2). Each of these blocks has size <img src="https://latex.codecogs.com/png.latex?n/2">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0AA_%7B11%7D%20&amp;%20A_%7B12%7D%20%5C%5C%0AA_%7B21%7D%20&amp;%20A_%7B22%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0AB_%7B11%7D%20&amp;%20B_%7B12%7D%20%5C%5C%0AB_%7B21%7D%20&amp;%20B_%7B22%7D%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0AC_%7B11%7D%20&amp;%20C_%7B12%7D%20%5C%5C%0AC_%7B21%7D%20&amp;%20C_%7B22%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>To compute an submatrix <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D">, we multiply the corresponding blocks of <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> and sum the results. For example:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AC_%7B11%7D%20=%20A_%7B11%7DB_%7B11%7D%20+%20A_%7B12%7DB_%7B21%7D%0A"></p>
<p>In pseudocode this translates to the code below.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1">n_blocks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> r</span>
<span id="cb2-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_blocks):</span>
<span id="cb2-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_blocks):</span>
<span id="cb2-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_blocks):</span>
<span id="cb2-5">            C[i][j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> A[i][k] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> B[k][j]</span></code></pre></div>
<p>Note that the entry <code>C[i][j]</code> is now a block matrix rather than a scalar, and the <code>@</code> operator denotes matrix multiplication, rather than scalar multiplication. Line 5 of the code above is loading blocks of size <img src="https://latex.codecogs.com/png.latex?r%5E2"> from <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> into the cache, which takes <img src="https://latex.codecogs.com/png.latex?2r%5E2"> memory transfers. Then, it multiplies the two blocks, which requires <img src="https://latex.codecogs.com/png.latex?2r%5E3"> flops.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BCompute%20Intensity%7D%20=%20%5Cfrac%7B%5Ctext%7Bflops%7D%7D%7B%5Ctext%7Bmemory%20transfers%7D%7D%20=%20%5Cfrac%7B2r%5E3%7D%7B2r%5E2%7D%20=%20r%0A"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>Assuming a block size of <img src="https://latex.codecogs.com/png.latex?r=100">, the naive approach does 1 flop per memory transfer, while the tiling approach does 100 flops per memory transfer. This would result in a 100x speedup, assuming memory bandwidth remained the bottleneck.</p>
</div>
</div>
<p>It should be clear that <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20r%20%5Cleq%20n">. Setting <img src="https://latex.codecogs.com/png.latex?r=1">, we recover the naive approach, while setting <img src="https://latex.codecogs.com/png.latex?r=n"> we recover the optimal approach. The table below compares the performance of the different methods. Note that for the tiled method, the flops and memory transfers are measured per block, rather than per matrix.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 12%">
<col style="width: 29%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Flops</th>
<th>Memory Transfers</th>
<th>Flops/Memory Transfer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Naive</td>
<td><img src="https://latex.codecogs.com/png.latex?2n%5E3"></td>
<td><img src="https://latex.codecogs.com/png.latex?2n%20%5Ccdot%20n%5E2"></td>
<td><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B2n%5E3%7D%7B2n%20%5Ccdot%20n%5E2%7D%20=%201"></td>
</tr>
<tr class="even">
<td>Tiling</td>
<td><img src="https://latex.codecogs.com/png.latex?2r%5E3"></td>
<td><img src="https://latex.codecogs.com/png.latex?2r%5E2"></td>
<td><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Br%5E3%7D%7Br%5E2%7D%20=%20r"></td>
</tr>
<tr class="odd">
<td>Optimal (Theoretical)</td>
<td><img src="https://latex.codecogs.com/png.latex?2n%5E3"></td>
<td><img src="https://latex.codecogs.com/png.latex?2n%5E2"></td>
<td><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bn%5E3%7D%7Bn%5E2%7D%20=%20n"></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="tiling-on-gpus-cuda" class="level1">
<h1>Tiling on GPUs &amp; CUDA</h1>
<p>GPUs have a large number of cores, which can quickly process the data arriving from memory. Therefore, the memory bandwidth is likely to be a bottleneck in matrix multiplication on GPUs. To relieve the pressure on the memory bandwidth, it’s necessary to reduce the number of memory transfers by using the tiling technique.</p>
<p>How is this actually implemented on a GPU? The code block below shows a simplified tiled matrix multiplication in CUDA C. The key idea is that the cache is explicitly written and read using CUDA <strong>shared memory</strong>. This is the equivalent to a user-managed L1 cache. The keyword <code>__shared__</code> defines an array used as cache. The variables <code>row</code> and <code>col</code> indicate what element of the matrix <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D"> the function computes. Line 13 is the loop over blocks. Lines 15-16 load the data from main memory into the cache. Lines 22-24 perform the dot product and accumulate the result. Finally, line 31 writes the result back to global memory. The function <code>__syncthreads()</code> is a CUDA synchronization primitive to avoid a race condition between threads.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode numberSource c number-lines code-with-copy"><code class="sourceCode c"><span id="cb3-1">__global__ <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">void</span> tiledMatMul<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>A<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>B<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>C<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb3-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Define shared memory arrays (cache)</span></span>
<span id="cb3-3">    __shared__ <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span> A_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>TILE_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">][</span>TILE_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">];</span></span>
<span id="cb3-4">    __shared__ <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span> B_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>TILE_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">][</span>TILE_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">];</span></span>
<span id="cb3-5">    </span>
<span id="cb3-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// CUDA thread variables</span></span>
<span id="cb3-7">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> blockIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> blockDim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb3-8">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> blockIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> blockDim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb3-9">    </span>
<span id="cb3-10">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span> C_ij <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">f</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb3-11">    </span>
<span id="cb3-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Loop over the blocks</span></span>
<span id="cb3-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> TILE_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> t<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">++)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb3-14">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Transfer data from main memory into the cache</span></span>
<span id="cb3-15">        A_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">][</span>threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> A<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> TILE_SIZE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">];</span></span>
<span id="cb3-16">        B_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">][</span>threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> B<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[(</span>t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> TILE_SIZE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> col<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">];</span></span>
<span id="cb3-17">        </span>
<span id="cb3-18">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Ensure data transfer is complete before proceeding</span></span>
<span id="cb3-19">        __syncthreads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">();</span></span>
<span id="cb3-20">        </span>
<span id="cb3-21">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Matrix multiply both blocks</span></span>
<span id="cb3-22">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> TILE_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">++)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb3-23">            C_ij <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> A_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">][</span>k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> B_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">][</span>threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">];</span></span>
<span id="cb3-24">        <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb3-25">        </span>
<span id="cb3-26">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Finish multiplying the blocks before overwriting the cache next iteration</span></span>
<span id="cb3-27">        __syncthreads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">();</span></span>
<span id="cb3-28">    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb3-29">    </span>
<span id="cb3-30">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Transfer the result back to global memory</span></span>
<span id="cb3-31">    C<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> col<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> C_ij<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb3-32"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
<p>The CUDA code is more complicated than the pseudocode, but it makes the cache usage explicit. To run the code look at <a href="https://github.com/tomasruizt/Code-Along/blob/main/cuda-pmpp/03_matrix_multiplication/main.cu">my example on Github</a>, which has Makefile for compilation and execution. If you want to see a more complete version, you can find it in the book by <span class="citation" data-cites="kirk2016programming">(Kirk and Wen-Mei 2016)</span>. They describe other techniques to optimize algorithms on GPUs, like memory coalescing, minimizing control divergence, and thread coarsening.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Tiling is a practical optimization technique for improving matrix multiplication performance by minimizing memory transfers and maximizing compute intensity. We saw how the memory access pattern affects the performance of matrix multiplication, and how tiling is implemented concretely in CUDA C.</p>
<p><strong>Acknowledgement:</strong> The author is funded by the Bavarian Research Institute for Digital Transformation (bidt) and the Ludwig Maximilian University of Munich.</p>
</section>
<section id="references" class="level1">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-kirk2016programming" class="csl-entry">
Kirk, David B, and W Hwu Wen-Mei. 2016. <em>Programming Massively Parallel Processors: A Hands-on Approach</em>. Morgan kaufmann.
</div>
<div id="ref-williams2009roofline" class="csl-entry">
Williams, Samuel, Andrew Waterman, and David Patterson. 2009. <span>“Roofline: An Insightful Visual Performance Model for Multicore Architectures.”</span> <em>Communications of the ACM</em> 52 (4): 65–76.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{ruiz2024,
  author = {Ruiz, Tomas},
  title = {How {Does} {Tiling} {Speed} {Up} {Matrix} {Multiplications}
    on {GPUs?}},
  date = {2024-12-23},
  url = {https://tomasruizt.github.io/posts/tiling-for-matrix-mult/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-ruiz2024" class="csl-entry quarto-appendix-citeas">
Ruiz, Tomas. 2024. <span>“How Does Tiling Speed Up Matrix
Multiplications on GPUs?”</span> December 23, 2024. <a href="https://tomasruizt.github.io/posts/tiling-for-matrix-mult/">https://tomasruizt.github.io/posts/tiling-for-matrix-mult/</a>.
</div></div></section></div> ]]></description>
  <category>Mathematics</category>
  <category>GPUs</category>
  <guid>https://tomasruizt.github.io/posts/tiling-for-matrix-mult/</guid>
  <pubDate>Sun, 22 Dec 2024 23:00:00 GMT</pubDate>
  <media:content url="https://tomasruizt.github.io/posts/tiling-for-matrix-mult/tiled-matrix-multiplication-squared.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Grokking an Inner Product Inequality With Python on WebAssembly</title>
  <dc:creator>Tomas Ruiz</dc:creator>
  <link>https://tomasruizt.github.io/posts/viz-inequalities-inner-prod-wasm/</link>
  <description><![CDATA[ 





<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>The purpose of this post is two-fold:</p>
<ol type="1">
<li>To showcase Python running directly in your browser <em>without</em> any server behind it, like JavaScript. I will even import libraries like <code>numpy</code> and <code>matplotlib</code>. The underlying technologies that power this are <a href="https://en.wikipedia.org/wiki/WebAssembly">WebAssembly</a> and <a href="https://pyodide.org/">Pyodide</a>, which I encourage you to check out.</li>
<li>To get you excited about a simple inequality and its application to vectors, functions &amp; matrices. These are our objects of study, and we will inspect and visualize them interactively using Python.</li>
</ol>
<p>I created this document using <a href="https://r-wasm.github.io/quarto-live/">Quarto Live</a>. Big thanks to <a href="https://renatatopinkova.github.io/">Renata Topinkova</a> for showing me this tool!</p>
</section>
<section id="the-inequality" class="level2">
<h2 class="anchored" data-anchor-id="the-inequality">The Inequality</h2>
<p>Michael Steele presents in his book this simple inequality based only on the fact that <img src="https://latex.codecogs.com/png.latex?(x-y)%5E2"> is always positive<sup>1</sup>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A0%20&amp;%5Cleq%20%20(x%20-%20y)%5E2%20%5C%5C%0A%5Cimplies%200%20&amp;%5Cleq%20x%5E2%20-2xy%20+y%5E2%20%5C%5C%0A%E2%9F%B9%20xy%20&amp;%E2%89%A4%20%5Cfrac%7B1%7D%7B2%7D%20(x%5E2%20+%20y%5E2)%0A%5Cend%7Balign%7D%0A"></p>
<p>The last inequality above is not intuitively obvious to me. I’m the kind of person that likes numerical proof to internalize these results, and this is where Python comes in handy. Change the variables <code>x</code> and <code>y</code> in the code below to see if the inequality holds.</p>
<div>
<div id="pyodide-1">

</div>
<script type="pyodide-1-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsInJ1bmJ1dHRvbiI6ZmFsc2UsImV2YWwiOnRydWV9LCJjb2RlIjoieCA9IDNcbnkgPSA0XG5wcmludChmXCJ7eCp5fSDiiaQgezAuNSAqICh4KnggKyB5KnkpfVwiKSJ9
</script>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This Python code is running <em>in your browser</em>. There is no juypter notebook, nor any deployment or any client-server communication behind it! 🤯🚀</p>
</div>
</div>
</section>
<section id="generalizing-to-vectors" class="level2">
<h2 class="anchored" data-anchor-id="generalizing-to-vectors">Generalizing to Vectors</h2>
<p>What happens when we apply this inequality to more than just scalars? It also applies to sequences of numbers, e.g: <img src="https://latex.codecogs.com/png.latex?%0Ax_1%20y_1%20+%20x_2%20y_2%20%3C=%20%5Cfrac%7B1%7D%7B2%7D%20(x_1%5E2%20+%20x_2%5E2)%20+%20%5Cfrac%7B1%7D%7B2%7D%20(y_1%5E2%20+%20y_2%5E2)%0A"> You might recognize that this is equivalent to an inner product: <img src="https://latex.codecogs.com/png.latex?%20x%5ET%20y%20%E2%89%A4%20%5Cfrac%7B1%7D%7B2%7D%20(x%5ET%20x%20+%20y%5ET%20y)"> where <img src="https://latex.codecogs.com/png.latex?x%20=%20%5Bx_1,%20%5Cdots,%20x_n%5D"> and <img src="https://latex.codecogs.com/png.latex?y%20=%20%5By_1,%20%5Cdots,%20y_n%5D">.</p>
<p>The inequality is asserting that the vector product <img src="https://latex.codecogs.com/png.latex?x%5ETy"> of <em>any</em> two vectors <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y"> has an upper bound given by the average of <img src="https://latex.codecogs.com/png.latex?x%5ETx"> and <img src="https://latex.codecogs.com/png.latex?y%5ETy">.</p>
<p>Once again, I’m not intuitively convinced until I see code running. Notice how we import <code>numpy</code>, which calls compiled C routines under the hood (<em>but our runtime is the browser now</em>).</p>
<div>
<div id="pyodide-2">

</div>
<script type="pyodide-2-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsInJ1bmJ1dHRvbiI6ZmFsc2UsImV2YWwiOnRydWV9LCJjb2RlIjoiaW1wb3J0IG51bXB5IGFzIG5wXG5cbnggPSBucC5hcnJheShbMSwgMiwgM10pXG55ID0gbnAuYXJyYXkoWzQsIDUsIDZdKVxuXG5wcmludChmXCJ7eCBAIHl9IOKJpCB7MC41ICogKHggQCB4ICsgeSBAIHkpfVwiKSJ9
</script>
</div>
</section>
<section id="generalizing-to-functions" class="level2">
<h2 class="anchored" data-anchor-id="generalizing-to-functions">Generalizing to Functions</h2>
<p>You might have heard that functions are infinite-dimensional vectors (🤯). In that case, the inequality also applies! But how does the inner product <img src="https://latex.codecogs.com/png.latex?x%5ETy"> for two functions look like?</p>
<p>The convention is to use the bracket notation <img src="https://latex.codecogs.com/png.latex?%E2%9F%A8%20x,%20y%20%E2%9F%A9"> rather than <img src="https://latex.codecogs.com/png.latex?x%5ETy">. To sum over the infinite individual entries of the (function) vector, we use the integral:</p>
<p><img src="https://latex.codecogs.com/png.latex?%E2%9F%A8%20f,%20g%20%E2%9F%A9%20=%20%5Cint%20f(x)%20g(x)%20dx"> Using this definition, the inequality holds for functions as well: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%E2%9F%A8%20f,%20g%20%E2%9F%A9%20&amp;%E2%89%A4%20%5Cfrac%7B1%7D%7B2%7D%20(%E2%9F%A8%20f,%20f%20%E2%9F%A9%20+%20%E2%9F%A8%20g,%20g%20%E2%9F%A9)%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20%5Cleft(%20%5Cint%20f(x)%5E2%20dx%20+%20%5Cint%20g(x)%5E2%20dx%20%5Cright)%0A%5Cend%7Balign%7D%0A"></p>
<p>Let’s take two concrete functions <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Ccos(x)"> and <img src="https://latex.codecogs.com/png.latex?g(x)%20=%20%5Csin(4x)">. I choose these arbitrarily because plotting them looks nice. Feel free to use different functions <code>f</code> and <code>g</code> in the code.</p>
<script type="exercise-setup-ex_funcs-contents">
eyJhdHRyIjp7ImV4ZXJjaXNlIjoiZXhfZnVuY3MiLCJzZXR1cCI6dHJ1ZSwiZXZhbCI6dHJ1ZX0sImNvZGUiOiJpbXBvcnQgbWF0cGxvdGxpYi5weXBsb3QgYXMgcGx0XG5cbmRlZiBwbG90KHgsIGYsIGcpOlxuICAgIHBsdC5wbG90KHgsIGYqZywgbGFiZWw9XCIkZiBcXFxcY2RvdCBnJFwiLCBjb2xvcj1cImdyZWVuXCIpXG4gICAgcGx0LnBsb3QoeCwgZipmLCBsYWJlbD1cIiRmXjIkXCIsIGNvbG9yPVwibGlnaHRibHVlXCIpXG4gICAgcGx0LnBsb3QoeCwgZypnLCBsYWJlbD1cIiRnXjIkXCIsIGNvbG9yPVwibGlnaHRibHVlXCIpXG4gICAgcGx0LnBsb3QoeCwgMC41ICogKGYqZiArIGcqZyksIGxhYmVsPVwiJDAuNSAoZl4yICsgZ14yKSRcIiwgY29sb3I9XCJyZWRcIilcbiAgICBwbHQudGl0bGUoXCJVcHBlciBib3VuZCBmb3IgJFxcXFxsYW5nbGUgZiwgZyBcXFxccmFuZ2xlJFwiKVxuICAgIHBsdC5sZWdlbmQoKVxuICAgIHBsdC5ncmlkKClcbiAgICBwbHQuc2hvdygpIn0=
</script>
<div>
<div id="pyodide-4">

</div>
<script type="pyodide-4-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsInJ1bmJ1dHRvbiI6ZmFsc2UsImF1dG9ydW4iOnRydWUsImV4ZXJjaXNlIjoiZXhfZnVuY3MiLCJldmFsIjp0cnVlfSwiY29kZSI6ImltcG9ydCBudW1weSBhcyBucFxuXG54ID0gbnAubGluc3BhY2UoMCwgMiwgMTAwKVxuZiA9IG5wLmNvcyh4KVxuZyA9IG5wLnNpbig0KngpXG5wbG90KHgsIGYsIGcpIn0=
</script>
</div>
<p>In the plot above, the individual functions <img src="https://latex.codecogs.com/png.latex?f%5E2"> and <img src="https://latex.codecogs.com/png.latex?g%5E2"> are plotted with light-blue lines. Their average is the red line, and the product <img src="https://latex.codecogs.com/png.latex?f%20%E2%8B%85%20g"> is the green line. The red line is an upper bound for the green one. We see that the green line crosses over the two blue lines at different points but never crosses over the red line.</p>
<p><strong>About the integral:</strong> Perhaps you noticed that I formulated the inequality on inner-products, but I’m plotting the functions pointwise. The missing step is the integral, which is evaluated in Python using the numpy function <code>np.trapz()</code>. As we can confirm below, the inequality holds:</p>
<div>
<div id="pyodide-5">

</div>
<script type="pyodide-5-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsInJ1bmJ1dHRvbiI6ZmFsc2UsImV2YWwiOnRydWV9LCJjb2RlIjoiaW1wb3J0IG51bXB5IGFzIG5wXG5cbnggPSBucC5saW5zcGFjZSgwLCAyLCAxMDApXG5mID0gbnAuY29zKHgpXG5nID0gbnAuc2luKDQqeClcblxucHJpbnQoZlwie25wLnRyYXB6KGYqZywgeCl9IOKJpCB7MC41ICogKG5wLnRyYXB6KGYqZiwgeCkgKyBucC50cmFweihnKmcsIHgpKX1cIikifQ==
</script>
</div>
</section>
<section id="generalizing-to-matrices" class="level2">
<h2 class="anchored" data-anchor-id="generalizing-to-matrices">Generalizing to Matrices</h2>
<p>Will the inequality also apply to matrices? The inner product of two matrices <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> (also called <a href="https://en.wikipedia.org/wiki/Frobenius_inner_product">Frobenius inner product</a>) is defined as: <img src="https://latex.codecogs.com/png.latex?%E2%9F%A8A,%20B%E2%9F%A9%20=%20%5Ctext%7Btr%7D(A%5ET%20B)"> where <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Btr%7D"> is the trace operator.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Beware that this inner product is different from matrix multiplication <img src="https://latex.codecogs.com/png.latex?%E2%9F%A8A,%20B%E2%9F%A9%20=%20tr(A%5ET%20B)%20%E2%89%A0%20AB"></p>
</div>
</div>
<p>The inequality for matrices then reads:</p>
<p><img src="https://latex.codecogs.com/png.latex?tr(A%5ET%20B)%20%E2%89%A4%20%5Cfrac%7B1%7D%7B2%7D%20(tr(A%5ET%20A)%20+%20tr(B%5ET%20B))"></p>
<p>It’s easy to convince yourself that the inequality holds for matrices by writing down the trace as a sum of scalars. As we verified before, inequality holds for each scalar in the sum:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Atr(A%5ET%20B)%0A&amp;=%20%5Csum_%7Bi,j%7D%20A_%7Bij%7D%20B_%7Bij%7D%20&amp;%20%5Ctext%7B(definition)%7D%5C%5C%0A&amp;%E2%89%A4%20%5Cfrac%7B1%7D%7B2%7D%20%5Cleft(%20%5Csum_%7Bi,j%7D%20A_%7Bij%7D%5E2%20+%20%5Csum_%7Bi,j%7D%20B_%7Bij%7D%5E2%20%5Cright)%20&amp;%5Ctext%7B(applied%20by%20scalar)%7D%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20(tr(A%5ET%20A)%20+%20tr(B%5ET%20B))%0A%5Cend%7Balign%7D%0A"></p>
<p>Let’s check the inequality with random matrices. You can use the <code>"Start Over"</code> button to re-run the code with new matrices.</p>
<div>
<div id="pyodide-6">

</div>
<script type="pyodide-6-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsInJ1bmJ1dHRvbiI6ZmFsc2UsImV2YWwiOnRydWV9LCJjb2RlIjoiaW1wb3J0IG51bXB5IGFzIG5wXG5cbmRpbSA9IDJcbkEgPSBucC5yYW5kb20ucmFuZG4oZGltLCBkaW0pXG5CID0gbnAucmFuZG9tLnJhbmRuKGRpbSwgZGltKVxuXG5kZWYgaXAoWCwgWSk6ICAjIGlubmVyIHByb2R1Y3RcbiAgcmV0dXJuIG5wLnRyYWNlKFguVCBAIFkpXG5cbnByaW50KGZcIntpcChBLCBCKX0g4omkIHswLjUgKiAoaXAoQSwgQSkgKyBpcChCLCBCKSl9XCIpIn0=
</script>
</div>
<p>The inequality holds, but I have no geometric intuition about the trace of a matrix, or how this inequality could be visualized for matrices. If you have an idea, please let me know! 🙏</p>
</section>
<section id="further-sources" class="level2">
<h2 class="anchored" data-anchor-id="further-sources">Further Sources</h2>
<p>If you found the mathematics interesting, particularly the generalization of inner products, I recommend MIT’s course <a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/">Matrix Calculus for Machine Learning and Beyond</a>, which covers this topic in more detail, and goes <em>much</em> further 😄.</p>


<script type="pyodide-data">
eyJwYWNrYWdlcyI6eyJwa2dzIjpbInB5b2RpZGVfaHR0cCIsIm1pY3JvcGlwIiwiaXB5dGhvbiJdfSwib3B0aW9ucyI6eyJpbmRleFVSTCI6Imh0dHBzOi8vY2RuLmpzZGVsaXZyLm5ldC9weW9kaWRlL3YwLjI2LjEvZnVsbC8ifX0=
</script>
<script type="ojs-module-contents">
{"contents":[{"cellName":"pyodide-6","inline":false,"source":"viewof _pyodide_editor_6 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-6-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-6-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_6 = pyodideOjs.process(_pyodide_editor_6, {});\n","methodName":"interpret"},{"cellName":"pyodide-5","inline":false,"source":"viewof _pyodide_editor_5 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-5-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-5-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_5 = pyodideOjs.process(_pyodide_editor_5, {});\n","methodName":"interpret"},{"cellName":"pyodide-4","inline":false,"source":"viewof _pyodide_editor_4 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-4-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  // Default exercise configuration\n  const options = Object.assign(\n    {\n      id: \"pyodide-4-contents\",\n      envir: `exercise-env-${block.attr.exercise}`,\n      error: false,\n      autorun: false,\n      caption: 'Exercise',\n    },\n    block.attr\n  );\n\n  const editor = new PyodideExerciseEditor(pyodideOjs.pyodidePromise, block.code, options);\n  return editor.container;\n}\nviewof _pyodide_value_4 = pyodideOjs.process(_pyodide_editor_4, {});\n_pyodide_feedback_4 = {\n  const { PyodideGrader } = window._exercise_ojs_runtime;\n  const emptyFeedback = document.createElement('div');\n\n  const grader = new PyodideGrader(_pyodide_value_4.evaluator);\n  const feedback = await grader.gradeExercise();\n  if (!feedback) return emptyFeedback;\n  return feedback;\n}\n","methodName":"interpret"},{"cellName":"pyodide-2","inline":false,"source":"viewof _pyodide_editor_2 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-2-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_2 = pyodideOjs.process(_pyodide_editor_2, {});\n","methodName":"interpret"},{"cellName":"pyodide-1","inline":false,"source":"viewof _pyodide_editor_1 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-1-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_1 = pyodideOjs.process(_pyodide_editor_1, {});\n","methodName":"interpret"},{"cellName":"pyodide-prelude","inline":false,"source":"pyodideOjs = {\n  const {\n    PyodideEvaluator,\n    PyodideEnvironmentManager,\n    setupPython,\n    startPyodideWorker,\n    b64Decode,\n    collapsePath,\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // Pyodide supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"pyodide-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  let pyodidePromise = (async () => {\n    statusText.textContent = `Downloading Pyodide`;\n    const pyodide = await startPyodideWorker(data.options);\n\n    statusText.textContent = `Downloading package: micropip`;\n    await pyodide.loadPackage(\"micropip\");\n    const micropip = await pyodide.pyimport(\"micropip\");\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return micropip.install(pkg);\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n    await micropip.destroy();\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await pyodide.FS.mkdir(path);\n        } catch (e) {\n          if (e.name !== \"ErrnoError\") throw e;\n          if (e.errno !== 20) {\n            const errorTextPtr = await pyodide._module._strerror(e.errno);\n            const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n            throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      try {\n        return await pyodide.FS.writeFile(file, new Uint8Array(data));\n      } catch (e) {\n        if (e.name !== \"ErrnoError\") throw e;\n        const errorTextPtr = await pyodide._module._strerror(e.errno);\n        const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n        throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n      }\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Pyodide environment setup`;\n    await setupPython(pyodide);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return pyodide;\n  })().catch((err) => {\n    statusText.style.color = \"var(--exercise-editor-hl-er, #AD0000)\";\n    statusText.textContent = err.message;\n    //indicatorContainer.querySelector(\".spinner-grow\").classList.add(\"d-none\");\n    throw err;\n  });\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const pyodide = await pyodidePromise;\n    const evaluator = new PyodideEvaluator(pyodide, context);\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    pyodidePromise,\n    renderedOjs,\n    process,\n  };\n}\n","methodName":"interpretQuiet"}]}
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>“The Cauchy-Schwarz Master Class: An Introduction to the Art of Mathematical Inequalities” by J. Michael Steele.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Mathematics</category>
  <category>Python</category>
  <guid>https://tomasruizt.github.io/posts/viz-inequalities-inner-prod-wasm/</guid>
  <pubDate>Wed, 11 Sep 2024 22:00:00 GMT</pubDate>
  <media:content url="https://tomasruizt.github.io/posts/viz-inequalities-inner-prod-wasm/upper-bound-img.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>A Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification</title>
  <dc:creator>Tomas Ruiz</dc:creator>
  <link>https://tomasruizt.github.io/posts/linear-adaptation/</link>
  <description><![CDATA[ 





<section id="summary" class="level1">
<h1>Summary</h1>
<p>In this post I show how to linearly fine-tune a large language model (LLM) using a closed-form solution, based on the Moore-Penrose Inverse. I will focus on the special case of binary classification because the changes in output are easier to interpret. The new linear transformation <img src="https://latex.codecogs.com/png.latex?W"> is shown in figure 1 (green).</p>
<div id="fig-llm" class="lightbox quarto-float quarto-figure quarto-figure-center anchored" data-text-align="center" alt="Learned Linear Transformation">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-llm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="linear-adaptation/linear-adaptation.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: The learned transformation W (green) is applied in parallel to the existing linear layer (blue), preserving the existing knowledge of the model. Both are the summed to make the logits, which are passed to the softmax function."><img src="https://tomasruizt.github.io/posts/linear-adaptation/linear-adaptation/linear-adaptation.png" class="img-fluid figure-img" style="width:75.0%" data-text-align="center" alt="Learned Linear Transformation"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-llm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The learned transformation W (green) is applied in parallel to the existing linear layer (blue), preserving the existing knowledge of the model. Both are the summed to make the logits, which are passed to the softmax function.
</figcaption>
</figure>
</div>
</section>
<section id="about-fine-tuning" class="level1">
<h1>About Fine-Tuning</h1>
<p>Large Language Models (LLMs) are great baseline models for <strong>zero-shot</strong> classification, i.e.&nbsp;without any labeled examples. However, one often has a small labeled dataset <img src="https://latex.codecogs.com/png.latex?D"> and is interested in improving the performance over this baseline. In the <strong>few-shot</strong> setting, some labeled examples are provided in the prompt for the model to learn in context. To improve upon this setting, the next step is to <strong>fine-tune</strong> the model on the labeled dataset.</p>
<p>There are different ways to fine-tune LLMs. For example: optimizing all network parameters, optimizing only the parameters of the final layer, or freezing all parameters but introduce a new smaller set of tunable parameters <span class="citation" data-cites="hu2021lora">(Hu et al. 2021)</span>. In this post, I focus on the simple case of fine-tuning the last linear transformation, because I’m interested in interpreting the changes to individual logits and probabilities.</p>
</section>
<section id="binary-classification" class="level1">
<h1>Binary Classification</h1>
<p>I also focus on binary classification specifically. This means that the model must only answer <code>yes/no</code> or <code>0/1</code> to the prompt. This setting is easier to interpret with metrics like precision, recall or the <img src="https://latex.codecogs.com/png.latex?F1"> score. Furthermore, in the binary case we can interpret how the fine-tuning procedure affected the model by inspecting the answers that flipped between <code>yes/no</code> and vice-versa <span class="citation" data-cites="dutta2024accuracy">(Dutta et al. 2024)</span>.</p>
<p>In terms of computation, we will see that the problem structure of binary classification can be leveraged to compute a closed-form solution efficiently. As shown in Figure&nbsp;1, I add an additional linear transformation <img src="https://latex.codecogs.com/png.latex?W"> before the softmax, and solve for it using the Moore-Penrose Inverse. This is mathematically equivalent to training <img src="https://latex.codecogs.com/png.latex?W"> with gradient descent, but without all the iteration.</p>
</section>
<section id="closed-form-solution" class="level1">
<h1>Closed-Form Solution</h1>
<p>In underbraces, I’ve written the dimension of the matrices and vectors. In the original language model, the probability vector <img src="https://latex.codecogs.com/png.latex?y"> has the same size of the vocabulary <img src="https://latex.codecogs.com/png.latex?V">, and is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cunderbrace%7Bp(y%20%7C%20x_%7B1:t%7D)%7D_%7B(1,V)%7D%0A&amp;=%20%5Ctext%7Bsoftmax%7D(%5Cunderbrace%7B%5Ctext%7Blogits%7D_t%7D_%7B(1,V)%7D)%20%5C%5C%0A&amp;=%20%5Ctext%7Bsoftmax%7D(%5Cunderbrace%7Bz_t%7D_%7B(1,d)%7D%20%5Cunderbrace%7BA%7D_%7B(d,V)%7D)%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?z_t"> is the hidden state for the last token, and <img src="https://latex.codecogs.com/png.latex?A"> is the weights the last linear layer (no bias is included as in <span class="citation" data-cites="chowdhery2023palm">Chowdhery et al. (2023)</span>). The loss for this model is defined as the distance between these probabilites and our true labels, which is a set of binary labels <img src="https://latex.codecogs.com/png.latex?D%20=%20%5Cbegin%7Bbmatrix%7D%20d_1%20%5C%5C%20%5Cvdots%20%5C%5C%20d_N%20%5Cend%7Bbmatrix%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B(N,%202)%7D"></p>
<p>With fine-tuning, we modify the probabilities that the LLM assigns to the tokens for <code>yes/no</code> from <img src="https://latex.codecogs.com/png.latex?p"> to <img src="https://latex.codecogs.com/png.latex?p_a"> (<em>adapted probabilities</em>). The role of <img src="https://latex.codecogs.com/png.latex?W"> is change the logits that are passed to the softmax function. We tweak <img src="https://latex.codecogs.com/png.latex?W"> to approximate the dataset <img src="https://latex.codecogs.com/png.latex?D"> with the adapted probabilities <img src="https://latex.codecogs.com/png.latex?p_a">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Ap_a(y%20%7C%20x_%7B1:t%7D)%20&amp;=%20%5Ctext%7Bsoftmax%7D(%5Ctext%7Blogits%7D_t%20+%20z_t%20W)%20%5C%5C%0A%5Cimplies%20%5Cunderbrace%7B%5Clog%20p_a%7D_%7B(1,%20V)%7D%0A&amp;=%20%5Cunderbrace%7B%5Ctext%7Blogits%7D_t%7D_%7B(1,V)%7D%20+%20%5Cunderbrace%7Bz_t%7D_%7B(1,d)%7D%20%5Cunderbrace%7BW%7D_%7B(d,V)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>In vectorized form (one row per datapoint <img src="https://latex.codecogs.com/png.latex?N">) this can be written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cunderbrace%7B%5Clog%20P_a%7D_%7B(N,V)%7D%0A&amp;=%20%5Cunderbrace%7BL_t%7D_%7B(N,V)%7D%20+%20%5Cunderbrace%7BZ_t%7D_%7B(N,d)%7D%20%5Cunderbrace%7BW%7D_%7B(d,V)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>Solving for <img src="https://latex.codecogs.com/png.latex?W"> exactly is only possible for squared invertible matrices <img src="https://latex.codecogs.com/png.latex?Z_t">. However, <img src="https://latex.codecogs.com/png.latex?W"> is rectangular (size <img src="https://latex.codecogs.com/png.latex?(d,%20V)">), so this problem is solved approximately by minimizing the squared distance:</p>
<p><img src="https://latex.codecogs.com/png.latex?W%20=%20%5Carg%20%5Cmin_W%20%7C%7C%20(%5Clog%20P_a%20-%20L_t)%20-%20Z_t%20W%20%7C%7C%5E2_2%20%5Cqquad%20(1)%20"></p>
<p>This is a least squares problem, whose solution is given by the <strong>Moore-Penrose Inverse</strong>.</p>
<p><img src="https://latex.codecogs.com/png.latex?W%20=%20(Z_t%5ET%20Z_t)%5E%7B-1%7D%20Z_t%5ET%20(%5Clog%20P_a%20-%20L_t)"></p>
<p>Or equivalently, by solving the following linear system of equations with <img src="https://latex.codecogs.com/png.latex?V"> columns (<em>But see note on numerical stability <sup>1</sup></em>).</p>
<p><img src="https://latex.codecogs.com/png.latex?W%20=%20%5Ctext%7Blinsolve%7D(%5Cunderbrace%7BZ_t%5ET%20Z_t%7D_%7B(d,d)%7D,%20%5Cspace%20%5Cunderbrace%7BZ_t%5ET%20(%5Clog%20P_a%20-%20L_t)%7D_%7B(d,V)%7D)%20%5Cqquad%20(2)%20"></p>
</section>
<section id="runtime" class="level1">
<h1>Runtime</h1>
<p>Each linear system takes <img src="https://latex.codecogs.com/png.latex?O(d%5E3)"> to solve, so solving <img src="https://latex.codecogs.com/png.latex?V"> of these systems is prohibitively expensive (<img src="https://latex.codecogs.com/png.latex?V=128k,%20d=4k"> in LLama3 8B). (<em>But see note on repeated linear solves<sup>2</sup></em>). However, we can exploit the structure of the binary classification problem, by only evaluating the logits <img src="https://latex.codecogs.com/png.latex?L_t"> and probabilities <img src="https://latex.codecogs.com/png.latex?P_a"> for the <code>yes/no</code> tokens. This reduces the size of the probability matrix <img src="https://latex.codecogs.com/png.latex?P_a"> by <em>4 to 5 orders of magnitude</em>, from <img src="https://latex.codecogs.com/png.latex?(N,V)"> to <img src="https://latex.codecogs.com/png.latex?(N,2)">. Similarly, the learned matrix <img src="https://latex.codecogs.com/png.latex?W"> shrinks from size <img src="https://latex.codecogs.com/png.latex?(d,V)"> to <img src="https://latex.codecogs.com/png.latex?(d,2)">.</p>
<p>As a result, we need to solve only 2 linear systems, each with runtime constant in the vocabulary size <img src="https://latex.codecogs.com/png.latex?V"> and in the number of datapoints in our dataset <img src="https://latex.codecogs.com/png.latex?N">, but proportional to <img src="https://latex.codecogs.com/png.latex?O(d%5E3)">. As an added benefit of evaluating only the <code>yes/no</code> logits, the output of the fine-tuned model is compliant by design, as it cannot output any other logits other than for <code>yes/no</code>.</p>
<p>To solve for <img src="https://latex.codecogs.com/png.latex?W"> either using eq (1) or eq (2), we plug in our dataset <img src="https://latex.codecogs.com/png.latex?D"> for <img src="https://latex.codecogs.com/png.latex?P_a">, since both matrices have the same size.</p>
</section>
<section id="inference" class="level1">
<h1>Inference</h1>
<p>At inference time, the matrix <img src="https://latex.codecogs.com/png.latex?W"> stays constant, while the logits change for each input.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Ap_a(y%7Cx_%7B1:t%7D)%0A&amp;=%20%5Ctext%7Bsoftmax%7D%20%5C%7B%20%5Ctext%7Blogits%7D_t%20+%20z_t%20W%20%5C%7D%20%5C%5C%0A&amp;=%20%5Ctext%7Bsoftmax%7D%20%5C%7B%20z_t%20A%20+%20z_t%20W%20%5C%7D%0A%5Cend%7Baligned%7D%0A"></p>
</section>
<section id="next-steps" class="level1">
<h1>Next Steps</h1>
<p>In the next post, I will show an implementation of this method in PyTorch, and interpret how linear fine-tuning changes the outputs of the original LLM. I am interested in the flips between <code>yes/no</code> outside of the small fine-tuning dataset <img src="https://latex.codecogs.com/png.latex?D">, and particularly on the boundaries of the dataset, and how this pertains to generalization. Stay tuned! :)</p>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-chowdhery2023palm" class="csl-entry">
Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al. 2023. <span>“Palm: Scaling Language Modeling with Pathways.”</span> <em>Journal of Machine Learning Research</em> 24 (240): 1–113.
</div>
<div id="ref-dutta2024accuracy" class="csl-entry">
Dutta, Abhinav, Sanjeev Krishnan, Nipun Kwatra, and Ramachandran Ramjee. 2024. <span>“Accuracy Is Not All You Need.”</span> <em>arXiv Preprint arXiv:2407.09141</em>.
</div>
<div id="ref-hu2021lora" class="csl-entry">
Hu, Edward J, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. <span>“Lora: Low-Rank Adaptation of Large Language Models.”</span> <em>arXiv Preprint arXiv:2106.09685</em>.
</div>
<div id="ref-watkins2004fundamentals" class="csl-entry">
Watkins, David S. 2004. <em>Fundamentals of Matrix Computations</em>. John Wiley &amp; Sons.
</div>
</div>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>2024-09: The matrix <img src="https://latex.codecogs.com/png.latex?Z%5ET%20Z"> is positive definite, so it is in theory efficiently invertible using the Cholesky decomposition. However, its condition number <img src="https://latex.codecogs.com/png.latex?%5Ckappa%20(Z%5ET%20Z)"> is squarely proportional to the condition number of <img src="https://latex.codecogs.com/png.latex?%5Ckappa(Z)">. This can lead to numerical instability when solving the linear system. In fact, I stumbled upon numerical instability while implementing this in linear system in PyTorch, which lead me to use an <img src="https://latex.codecogs.com/png.latex?L_2"> regularization term. See <a href="https://tobydriscoll.net/fnc-julia/leastsq/normaleqns.html#conditioning-and-stability">Source</a>.↩︎</p></li>
<li id="fn2"><p>2024-09: It turns out that solving a linear system with <img src="https://latex.codecogs.com/png.latex?V"> columns on the right-hand side can be done cheaper than in <img src="https://latex.codecogs.com/png.latex?V%20%5Ccdot%20%5Cfrac%7B2%7D%7B3%7D%20d%5E3"> flops. To solve <img src="https://latex.codecogs.com/png.latex?A%20X%20=%20B"> (with <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd,d%7D"> and <img src="https://latex.codecogs.com/png.latex?X,%20B%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd,V%7D">) the factorization of <img src="https://latex.codecogs.com/png.latex?A"> requires <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B2%7D%7B3%7D%20d%5E3"> flops, but it only has to be done once. After that, solving for each of the <img src="https://latex.codecogs.com/png.latex?V"> columns of <img src="https://latex.codecogs.com/png.latex?B"> costs <img src="https://latex.codecogs.com/png.latex?2%20d%5E2"> flops each. So the total flop count is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B2%7D%7B3%7D%20d%5E3%20+%20V%20%5Ccdot%202%20d%5E2">. This is a significant improvement over the naive approach. See <span class="citation" data-cites="watkins2004fundamentals">(Watkins 2004, 77–78)</span>.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Machine Learning</category>
  <guid>https://tomasruizt.github.io/posts/linear-adaptation/</guid>
  <pubDate>Thu, 01 Aug 2024 22:00:00 GMT</pubDate>
  <media:content url="https://tomasruizt.github.io/posts/linear-adaptation/linear-adaptation/linear-adaptation.png" medium="image" type="image/png" height="128" width="144"/>
</item>
</channel>
</rss>
