<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>All Posts</title>
<link>https://tomasruizt.github.io/</link>
<atom:link href="https://tomasruizt.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Tomas Ruiz&#39;s blog</description>
<generator>quarto-1.5.57</generator>
<lastBuildDate>Wed, 07 Jan 2026 05:00:00 GMT</lastBuildDate>
<item>
  <title>PyTorch and CPU-GPU Synchronizations</title>
  <dc:creator>Tomas Ruiz</dc:creator>
  <link>https://tomasruizt.github.io/posts/08_cpu_gpu_synchronization/</link>
  <description><![CDATA[ 





<p><strong>TL;DR:</strong> This post is a guide to understand and prevent CPU-GPU synchronizations, which will help you write fast and efficient PyTorch programs üöÄ. I explain the concept with concrete PyTorch examples from <a href="https://gist.github.com/tomasruizt/4281c61bfaead3e61211d052d3971c90">this Github Gist</a>, and profiles from NVIDIA Nsight Systems.</p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>CPU-GPU synchronizations (also known as <em>host-device</em> synchronizations) are a subtle but important mechanism to write fast and efficient PyTorch programs. The CPU-GPU synchronization is a blocking operation that prevents the CPU from scheduling new work (PyTorch ops) on the GPU. The <a href="https://docs.pytorch.org/tutorials/recipes/recipes/tuning_guide.html#avoid-unnecessary-cpu-gpu-synchronization">PyTorch Tuning Guide</a> makes some basic suggestions about how to avoid CPU-GPU synchronizations, e.g.&nbsp;by avoiding calls to <code>tensor.item()</code>, printing a device tensor, or calling <code>tensor.cpu()</code> / <code>tensor.cuda()</code>, but doesn‚Äôt explain in depth what is happening under the hood. Furthermore, there are other subtle ways to run into CPU-GPU synchronizations. In this post I dive deeper into synchronizations and explain how they slow down a PyTorch program. I show how to use the NVIDIA <strong>Nsight Systems</strong> profiler to identify and diagnose CPU-GPU synchronization issues. Finally, I also show how to use the experimental PyTorch API <code>torch.cuda.set_sync_debug_mode</code> in <strong>unit tests</strong> to verify the absence of CPU-GPU synchronizations in production code.</p>
<div id="fig-profile-do-print-overview" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-profile-do-print-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://tomasruizt.github.io/posts/08_cpu_gpu_synchronization/imgs/example-code-do-print.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-profile-do-print-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Profile showing CPU-GPU synchronization issues (here triggered by printing a CUDA tensor). A deep dive into the profile details is in Section&nbsp;4.
</figcaption>
</figure>
</div>
</section>
<section id="what-is-a-cpu-gpu-synchronization" class="level1">
<h1>What is a CPU-GPU Synchronization?</h1>
<p>To fully take advantage of the speed of a GPU, it must be kept busy (high GPU utilization). This depends on the CPU scheduling enough work to keep the GPU busy. Typically, in a well-performing PyTorch program, the CPU schedules many instructions quickly, which are executed by the GPU. The CPU is said to <em>run ahead</em> of the GPU, because it issues instructions without waiting for the previous ones to complete. This is why PyTorch is said to be <em>asynchronous</em>. If the CPU fails to schedule enough work, the GPU will sit idle waiting for work.</p>
<p><strong>The intuition:</strong> As an analogy to understand CPU-GPU synchronizations, think of a restaurant where a chef (CPU) schedules all the steps (PyTorch ops) for a big dinner party in advance. A CPU-GPU synchronization would be like the chef waiting to observe how a specific step in a specific dish turned out <em>during the dinner</em>, before scheduling the rest of the dishes and telling the cooks what to do. Obviously, this latency in communication will lead to the cooks being idle, waiting for the chef to schedule the dishes. Instead, the chef should schedule the entire dinner plan in advance (<em>run ahead</em>).</p>
</section>
<section id="example" class="level1">
<h1>Example</h1>
<p>Let‚Äôs look at a concrete example. Below we have a PyTorch program that executes a <code>slow_operation()</code> on the GPU, followed by a <code>quick_operation()</code> on the GPU. The quick operation just counts the number of 1s in the result tensor. The result of the quick operation is gathered in <code>results</code> tensor. Both functions are executed in a loop, to simulate a long-running program like LLM inference, where the slow operation could be the LLM forward pass, and the quick operation could be bookkeeping with the sampled tokens (a real vLLM example is explained in Section&nbsp;7). To warm up the torch code, I run both the <code>slow_operation()</code> and <code>quick_operation()</code> once before the hot loop. I annotated regions of code with <code>nvtx.annotate()</code>, to keep track of GPU and CPU runtime during profiling.</p>
<script src="https://gist.github.com/tomasruizt/4281c61bfaead3e61211d052d3971c90.js?file=cpu_gpu_sync_example.py"></script>
</section>
<section id="sec-profiling-analysis" class="level1">
<h1>Profiling Analysis</h1>
<p>We can run the NVIDIA Nsight Systems profiler with the command below to produce the timelines shown in Figure&nbsp;2 and Figure&nbsp;3. The script takes a flag <code>--do-print</code> to include a CPU-GPU synchronization in the quick operation: a <code>print</code> of a GPU tensor, which forces data back from the GPU to the CPU.</p>
<p><strong>Without</strong> CPU-GPU synchronization:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode numberSource shell number-lines code-with-copy"><code class="sourceCode"><span id="cb1-1">nsys profile \</span>
<span id="cb1-2">  -o profile-no-print.nsys-rep \</span>
<span id="cb1-3">  python cpu_gpu_sync_example.py</span></code></pre></div>
<p><strong>With</strong> CPU-GPU synchronization:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode numberSource shell number-lines code-with-copy"><code class="sourceCode"><span id="cb2-1">nsys profile \</span>
<span id="cb2-2">  -o profile-do-print.nsys-rep \</span>
<span id="cb2-3">  python cpu_gpu_sync_example.py --do-print</span></code></pre></div>
<p>The resulting profiles are shown below and annotated. I ran the code on an NVIDIA RTX3090 GPU and Nsight Systems 2025.3.1.90.</p>
<p>To inspect the report:</p>
<ul>
<li>Open the <code>.nsys-rep</code> in the Nsight Systems UI</li>
<li>Zoom into the NVTX-annotated region.</li>
<li>Look for long CPU-side CUDA API calls like <code>cudaStreamSynchronize</code> and correlate them with gaps in GPU utilization.</li>
</ul>
<section id="run-with-cpu-gpu-synchronization" class="level3">
<h3 class="anchored" data-anchor-id="run-with-cpu-gpu-synchronization">Run With CPU-GPU Synchronization</h3>
<div id="fig-profile-do-print" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-profile-do-print-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://tomasruizt.github.io/posts/08_cpu_gpu_synchronization/imgs/example-code-do-print-annotated.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-profile-do-print-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Profile with CPU-GPU synchronization.
</figcaption>
</figure>
</div>
<ul>
<li>We see <strong>gaps</strong> in the GPU utilization (first blue bar from top to bottom). These gaps indicate that the GPU has idle times and waits for work.</li>
<li>We observe that the runtimes of the GPU and CPU are similarly long, as seen both timelines being equal in length in the horizontal axis. In a well-performing program, the CPU runtime should be shorter than the GPU runtime (CPU <em>runs ahead</em> of the GPU).</li>
<li>In the CPU ops timeline, we see long green bars (<code>cudaStreamSynchronize</code>), which mean that the CPU is <strong>blocked</strong> waiting for the GPU to return some data.</li>
<li>On the GPU ops timeline we observe that the slow operation takes longer than the quick operation, while in the CPU ops timeline we observe the reverse. This means that the quick operation is the one blocking the CPU.</li>
</ul>
</section>
<section id="healthy-run" class="level3">
<h3 class="anchored" data-anchor-id="healthy-run">Healthy Run</h3>
<div id="fig-profile-no-print" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-profile-no-print-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://tomasruizt.github.io/posts/08_cpu_gpu_synchronization/imgs/example-code-no-print-annotated.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-profile-no-print-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Profile without CPU-GPU synchronization.
</figcaption>
</figure>
</div>
<ul>
<li>We observe that the CPU <strong>runs ahead</strong> of the GPU, so it dispatches all the work quickly and finishes way before the GPU.</li>
<li>The GPU utilization does not have any gaps. The blue line showing utilization is continuous.</li>
<li>In the GPU ops timeline, the slow operation is slower than the quick operation.</li>
<li>The full region <code>interleaved-code</code> runs faster than with the CPU-GPU synchronization (4.434 ms vs 4.827 ms).</li>
</ul>
</section>
</section>
<section id="dynamic-shapes-can-trigger-synchronizations" class="level1">
<h1>Dynamic Shapes Can Trigger Synchronizations</h1>
<p>Besides the code mentioned in the <a href="https://docs.pytorch.org/tutorials/recipes/recipes/tuning_guide.html#avoid-unnecessary-cpu-gpu-synchronization">PyTorch Tuning Guide</a> there is another common pattern that triggers CPU-GPU synchronizations, namely <strong>dynamic shapes</strong>. One example of dynamic shapes is boolean indexing <code>x = t[bool_mask]</code>, where <code>bool_mask</code> is a boolean tensor on GPU. The size of the tensor <code>x</code> cannot be determined by the CPU alone, because it depends on the number of true values in <code>bool_mask</code>. Therefore, PyTorch typically needs to fetch data from the GPU to determine the amount of memory to allocate for <code>x</code>. This creates a CPU-GPU synchronization.</p>
<p>Another example of this same problem is slicing <code>x = t[:gpu_index]</code>, where <code>gpu_index</code> is a scalar integer tensor on GPU. Once again, the size of the tensor <code>x</code> cannot be determined by the CPU alone, because it depends on the value of <code>gpu_index</code>.</p>
<p>There are other ways to slice a torch tensor, e.g.&nbsp;<code>x = t.narrow(dim=0, start, length)</code> (<a href="https://docs.pytorch.org/docs/stable/generated/torch.narrow.html">link</a>). However, this operation requires <code>length</code> to be a Python <code>int</code>. Passing a GPU tensor as <code>length</code> will typically trigger a cast to Python <code>int</code> on CPU, which introduces a CPU-GPU synchronization.</p>
<p>A similar situation arises with <code>x = torch.repeat_interleave(y, repeats)</code>. But interestingly, this API exposes an optional argument <code>output_size</code> that can be used to prevent the synchronization (<a href="https://docs.pytorch.org/docs/stable/generated/torch.repeat_interleave.html">link</a>).</p>
<p>What all these synchronization triggers have in common is that the length of the resulting tensor depends on data residing on the GPU. This is why <strong>dynamic shapes</strong> are so problematic. Instead, if the length of the resulting tensor can be known <em>statically</em> on the CPU side, then we can find a way to avoid the CPU-GPU synchronization. PyTorch might not provide the precise API you need to avoid the synchronization. In that case, you might want to write a custom kernel in <strong>Triton</strong> to avoid the synchronization. The advantage of doing this is that you can fuse many sequential PyTorch operations into a single Triton kernel, which reduces the overhead of dispatching many small kernels on the CPU-side (around 2¬µs to 3¬µs per kernel). A concrete example is explained in Section&nbsp;7. The accompanying Github Gist also uses <a href="https://gist.github.com/tomasruizt/4281c61bfaead3e61211d052d3971c90#file-test_tokengroup-py-L299">Triton kernels</a> to fuse together PyTorch ops, and prevent CPU-GPU synchronizations.</p>
</section>
<section id="unit-testing-for-cpu-gpu-synchronizations" class="level1">
<h1>Unit Testing for CPU-GPU Synchronizations</h1>
<p>So we need to prevent synchronization in our code. How can we do this without running a profiler on each code section we are interested in? PyTorch provides an experimental feature that can raise warnings or errors when CPU-GPU synchronizations occur: The <code>torch.cuda.set_sync_debug_mode()</code> function <a href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.set_sync_debug_mode.html">(link)</a>. Since it raises errors immediately, it can be used in unit tests to verify that the code is free of CPU-GPU synchronizations. For example it can be used in a context manager:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> functools</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> contextlib <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> contextmanager</span>
<span id="cb3-3"></span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb3-5"></span>
<span id="cb3-6"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@contextmanager</span></span>
<span id="cb3-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> fail_if_gpu_cpu_synchronization(fail: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bool</span>):</span>
<span id="cb3-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Within this context, GPU-CPU synchronization raises an error if `fail` is True."""</span></span>
<span id="cb3-9"></span>
<span id="cb3-10">    new_mode <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> fail <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb3-11">    old_mode <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cuda.get_sync_debug_mode()</span>
<span id="cb3-12">    torch.cuda.set_sync_debug_mode(new_mode)</span>
<span id="cb3-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb3-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">yield</span></span>
<span id="cb3-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">finally</span>:</span>
<span id="cb3-16">        torch.cuda.set_sync_debug_mode(old_mode)</span>
<span id="cb3-17"></span>
<span id="cb3-18"></span>
<span id="cb3-19">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cuda"</span>)</span>
<span id="cb3-20"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> fail_if_gpu_cpu_synchronization(fail<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>):</span>
<span id="cb3-21">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(x)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Raises an error</span></span></code></pre></div>
<p>The context manager can be used in a decorator as well, to decorate functions that should fail if they contain CPU-GPU synchronizations.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> on_gpu_cpu_synchronization(fail: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bool</span>):</span>
<span id="cb4-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb4-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Wrap a function to raise an error on GPU-CPU synchronization if `fail` is True.</span></span>
<span id="cb4-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb4-5"></span>
<span id="cb4-6">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> decorator(fn):</span>
<span id="cb4-7">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@functools.wraps</span>(fn)</span>
<span id="cb4-8">        <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> wrapper(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>args, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>kwargs):</span>
<span id="cb4-9">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> fail_if_gpu_cpu_synchronization(fail):</span>
<span id="cb4-10">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> fn(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>args, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>kwargs)</span>
<span id="cb4-11"></span>
<span id="cb4-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> wrapper</span>
<span id="cb4-13"></span>
<span id="cb4-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> decorator</span>
<span id="cb4-15"></span>
<span id="cb4-16"></span>
<span id="cb4-17"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@on_gpu_cpu_synchronization</span>(fail<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-18"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> test_should_not_sync():</span>
<span id="cb4-19">    x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cuda"</span>)</span>
<span id="cb4-20">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span></code></pre></div>
<p>Note that the decorator is used <strong>only on test functions</strong>, meaning that the PyTorch API to raise synchronization errors is never applied to production code, but only to the test code. This keeps this experimental feature isolated from production code while still providing quick and valuable feedback on CPU-GPU synchronizations. I used this pattern to verify the absence of synchronizations in my Github Gist about <a href="https://gist.github.com/tomasruizt/4281c61bfaead3e61211d052d3971c90#file-test_tokengroup-py">vLLM TokenGroup</a>. The mechanism itself of raising errors is also unit tested in the <a href="https://gist.github.com/tomasruizt/4281c61bfaead3e61211d052d3971c90#file-test_helpers-py">test_helpers.py file</a> in the Github Gist.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>This PyTorch API is experimental as of today, and does not cover <em>all</em> CPU-GPU synchronizations. For example, it does not cover the <code>torch.distributed</code> and <code>torch.sparse</code> namespaces (see <a href="https://docs.pytorch.org/docs/stable/generated/torch.cuda.set_sync_debug_mode.html">docs</a>).</p>
</div>
</div>
</section>
<section id="sec-in-practice" class="level1">
<h1>In Practice: vLLM</h1>
<p>I encountered CPU-GPU synchronizations in the context of contributing speculative decoding to vLLM (<a href="https://github.com/vllm-project/vllm/pull/24322">PR #24322</a>). In particular, the LLM forward passes are heavy operations, and preparing the input tokens for the forward pass requires lots of small PyTorch operations, which can introduce CPU-GPU synchronizations, if not carefully written. Senior NVIDIA engineer <a href="https://www.linkedin.com/in/benjamin-chislett-05502818a/">Benjamin Chislett</a> helped me understand what causes the synchronizations, in particular dynamic shapes, and the value of writing custom kernels in Triton to fuse together multiple sequential PyTorch operations (example: <a href="https://github.com/vllm-project/vllm/pull/28597">PR #28597</a>). I thank him for his support and feedback! üí™</p>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<p>In this post, I explained what CPU-GPU synchronizations are, and how to identify and diagnose them with the NVIDIA Nsight Systems profiler. I discussed dynamic shapes as a common trigger for CPU-GPU synchronizations, and the value of writing custom kernels in Triton to fuse together multiple sequential PyTorch operations. I also showed how to use the experimental PyTorch API <code>torch.cuda.set_sync_debug_mode</code> in unit tests to verify the absence of CPU-GPU synchronizations in production code. This guide aims to help engineers prevent CPU-GPU synchronizations, which is key to write fast and efficient PyTorch programs.</p>
</section>
<section id="further-references" class="level1">
<h1>Further References</h1>
<p>For a deep dive into kernel benchmarking in practice, I recommend this <a href="https://www.youtube.com/watch?v=CtrqBmYtSEk">YouTube lecture</a> by NVIDIA engineering manager <a href="https://www.linkedin.com/in/g-evtushenko/">Georgii Evtushenko</a>.</p>
<!-- 
  - I learned a lot about synchronizing operations while developing the vLLM TokenGroup abstraction in this Github gist: https://gist.github.com/tomasruizt/4281c61bfaead3e61211d052d3971c90

  - `torch.tensor([1, 2, 3], device=‚Äùcuda‚Äù)` syncs, while `torch.tensor([1, 2, 3]).to(‚Äùcuda‚Äù, non_blocking=True)` does not.
  - Dynamic shapes sync. E.g. boolean indexing `t[bool_mask]` or `t[:gpu_index]` because torch needs to know how large the tensor is to issue the mem-alloc.

  - Scalar ops can be very easy to write in Triton, not only vectorized programs. In particular, multiple torch ops can be fused into a single Triton op. This is useful to reduce the dispatching overhead on the CPU-side, and the kernel launch overhead on the GPU-side.
-->


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{ruiz2026,
  author = {Ruiz, Tomas},
  title = {PyTorch and {CPU-GPU} {Synchronizations}},
  date = {2026-01-07},
  url = {https://tomasruizt.github.io/posts/08_cpu_gpu_synchronization/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-ruiz2026" class="csl-entry quarto-appendix-citeas">
Ruiz, Tomas. 2026. <span>‚ÄúPyTorch and CPU-GPU Synchronizations.‚Äù</span>
January 7, 2026. <a href="https://tomasruizt.github.io/posts/08_cpu_gpu_synchronization/">https://tomasruizt.github.io/posts/08_cpu_gpu_synchronization/</a>.
</div></div></section></div> ]]></description>
  <category>GPUs</category>
  <category>PyTorch</category>
  <category>Triton</category>
  <guid>https://tomasruizt.github.io/posts/08_cpu_gpu_synchronization/</guid>
  <pubDate>Wed, 07 Jan 2026 05:00:00 GMT</pubDate>
  <media:content url="https://tomasruizt.github.io/posts/08_cpu_gpu_synchronization/imgs/example-code-do-print-annotated.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>A Guide to Classify 35,000 Videos with a 72B Multimodal LLM on the vLLM Engine</title>
  <dc:creator>Tomas Ruiz</dc:creator>
  <link>https://tomasruizt.github.io/posts/05_mm-inference-on-vllm/</link>
  <description><![CDATA[ 





<section id="summary" class="level1">
<h1>Summary</h1>
<p>In this post, I describe how we used the vLLM inference engine to classify 35k videos collected from TikTok for a research project. I share lessons learned about computing on SLURM, parallelism strategies (tensor- and data-parallelism), and practical tips for fault tolerance and video inference with vLLM. I also dive into concrete token throughput statistics of our workload, which is extremely prefill-heavy. Finally, I show how we corrected for the bias of the Qwen2.5-VL-72B model to make statistically valid statements about our dataset. As a bonus, I also share Python code for video classification using the vLLM library in Section&nbsp;13, as well as the SLURM file for our workload in Section&nbsp;12.</p>
</section>
<section id="motivation" class="level1">
<h1>Motivation</h1>
<p>For this research project, I collaborated with my colleague at LMU Munich, <a href="https://renatatopinkova.github.io/">Renata Topinkova</a>, a computational social scientist. Our goal was to detect subtle product sales by influencers on the platform. We presented a <a href="./other-files/poster_ic2s2_25.pdf">poster with our findings</a> at the IC2S2 conference in 2025.</p>
<p>This post focuses mainly on our large <strong>inference workload</strong>, which required running a large multimodal LLM (72B parameters) on 35k videos as quickly as possible before the deadline. The only way to get this done in reasonable time was with an <em>inference engine</em>. I used vLLM <span class="citation" data-cites="DBLP:conf/sosp/KwonLZ0ZY0ZS23">(Kwon et al. 2023)</span>, because it‚Äôs the engine I am most familiar with. There are other alternative engines like <em>SGLang</em> <span class="citation" data-cites="zheng2024sglang">(Zheng et al. 2024)</span> or <a href="https://github.com/ai-dynamo/dynamo">NVIDIA Dynamo</a> that you can try.</p>
</section>
<section id="dataset-and-model" class="level1">
<h1>Dataset and Model</h1>
<p>Our dataset consists of 35,000 multimodal TikTok posts made up of videos, images, audio and text. The size of the dataset is around 180GB, the majority of which is from the videos. The dataset is a special subsample of a much bigger and more general dataset of ~1M posts we have.</p>
<p>To analyze this dataset, we needed a multimodal model capable of consuming inputs with multiple modalities. On top of that, the cues we looked for in the videos were often subtle and implicit: E.g.,</p>
<blockquote class="blockquote">
<p><em>‚ÄúDoes the post suggest a follow-up transaction or sales interaction?‚Äù</em></p>
</blockquote>
<p>So we also needed the model to grasp subtle communication cues in the videos. Larger models, typically in the range of 70B+ parameters, are much more competent at this than the widely used 8B models.</p>
<p>We decided to use the <code>Qwen2.5-VL-72B-Instruct</code> model by the Qwen team <span class="citation" data-cites="bai2025qwen25vltechnicalreport">(Bai et al. 2025)</span> <a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct">(Hugging Face link)</a>. Other multimodal models require you to cut up videos into individual frames and pass a video as a ‚Äústack of images‚Äù, but the Qwen2.5-VL model instead natively consumes the videos directly. The key innovation to do this is called multidimensional RoPE (mRoPE) embeddings. mRoPE encodes the time dimension of the video into a separate dimension <span class="citation" data-cites="wang2024qwen2vlenhancingvisionlanguagemodels">(Wang et al. 2024)</span>. Thus, Qwen2.5-VL understands the temporal relationship between individual video frames, unlike other multimodal models.</p>
<p><strong>Audio:</strong> An important shortcoming of this model is that it does not process audio natively. To alleviate this, we transcribed the audio using Whisper <span class="citation" data-cites="pmlr-v202-radford23a">(Radford et al. 2023)</span> and included the transcript in the text input. Some models that are capable of processing audio natively have been released recently: Qwen2.5-Omni or Phi-4 <span class="citation" data-cites="xu2025qwen25omnitechnicalreport abdin2024phi4technicalreport">(Xu et al. 2025; Abdin et al. 2024)</span>. However, the support for vLLM was limited until recently, the models remain relatively small in terms of size, and often output speech, which is not something we needed. Nevertheless, they are a very promising direction to explore.</p>
<p><strong>Video Input:</strong> Feeding a video to the model through vLLM requires a specific setup: Many examples for image analysis with vLLM / OpenAI API serialize the image into base64 encoding and send it in the request payload (e.g., OpenAI <a href="https://platform.openai.com/docs/guides/images-vision?api-mode=chat&amp;format=base64-encoded#analyze-images">docs</a>, Google <a href="https://ai.google.dev/gemini-api/docs/video-understanding">docs</a>, even vLLM <a href="https://docs.vllm.ai/en/stable/examples/online_serving/openai_chat_completion_client_for_multimodal.html">examples</a>). However, I found that this was not scalable for videos, because it blocked the server while it processed the payloads, and left the GPU idle. Instead, we only send the local file path <em>referencing</em> the video in the payload. vLLM reads the file from disk, processes it, and feeds it to the model. This removes the serialization and deserialization overhead. For this to work, we must give the vLLM server explicit permission to read from a specific directory (and subdirectories) using <a href="https://docs.vllm.ai/en/latest/configuration/engine_args.html#-allowed-local-media-path">this option</a>:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode numberSource shell number-lines code-with-copy"><code class="sourceCode"><span id="cb1-1">vllm serve ... --allowed-local-media-path=/path/to/your/files/</span></code></pre></div>
<p>The way to include a video file path in the payload is shown below. Note that the prefix <code>file://</code> is important. Both this file path and the path passed to <code>allowed-local-media-path</code> must be absolute paths. A full example can be found in their documentation: <a href="https://github.com/QwenLM/Qwen3-VL/blob/6e98a0a62bce167c5802ae6f5f95fcd97d2634cf/README.md?plain=1#L286">link</a>.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1">filepath <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/path/to/your/files/my-video.mp4"</span></span>
<span id="cb2-2">messages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb2-3">  {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: [</span>
<span id="cb2-4">    {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"type"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"What is in the video?"</span>},</span>
<span id="cb2-5">    {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"type"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"video_url"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"video_url"</span>: {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"url"</span>: <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"file://</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>filepath<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>}}</span>
<span id="cb2-6">  ]}</span>
<span id="cb2-7">]</span></code></pre></div>
</section>
<section id="gpu-cluster" class="level1">
<h1>GPU Cluster</h1>
<p>For our workload, we needed to run multiple instances of this large 72B model in parallel to process the dataset quickly (i.e.&nbsp;within a few hours). To this end, we used the <a href="https://doku.lrz.de/lrz-ai-systems-11484278.html">LRZ AI Systems cluster</a>. We used the BayernKI partition, which has 30 nodes, each with 4 Hopper H100 GPUs (94 GB VRAM per GPU) connected over NVLink. These are beefy GPUs, but nevertheless, the 72B model would not fit on a single GPU, even in bfloat16 precision, since only the model weights require around 144 GB VRAM. The model would fit on a single Blackwell B200 GPU (180 GB VRAM), but those are not available on our cluster. Beyond the model weights, the model also requires space for the KV cache, model activations, etc. Therefore, we allocated 4 GPUs, totaling 376 GB VRAM, per model instance. To launch jobs, the cluster uses the SLURM job scheduler <span class="citation" data-cites="yoo2003slurm">(Yoo, Jette, and Grondona 2003)</span>.</p>
<p><strong>Containers:</strong> On the LRZ AI Systems, one cannot install and customize a Python environment on the worker nodes directly. Instead, one has to use NVIDIA <a href="https://github.com/NVIDIA/enroot">enroot containers</a>. These are conceptually similar to Docker containers, and the LRZ has introductory documentation about enroot <a href="https://doku.lrz.de/4-1-enroot-introduction-1895502566.html">here</a>. There are a couple of useful commands to create and start containers including <code>enroot import</code>, <code>enroot create</code> and <code>enroot start</code>. The code in Section&nbsp;12 shows how we used these commands in our SLURM file.</p>
<p><strong>Data Transfer:</strong> To move your data into the LRZ AI System, they integrated a tool called <a href="https://doku.lrz.de/dss-how-globus-data-transfer-and-globus-sharing-for-dss-works-11484489.html">Globus Data Transfer</a>. This is special software to transfer large amounts of data between different servers. It is preinstalled on the LRZ AI Systems, and we installed it on our server as well. Then we used the <a href="https://app.globus.org/">web interface</a> to transfer our dataset to the GPU cluster. The transfer is asynchronous and will email you when it‚Äôs done. It also works the other way around, to transfer data back from the LRZ to your server.</p>
</section>
<section id="parallelism-strategies" class="level1">
<h1>Parallelism Strategies</h1>
<p>vLLM supports distributing the model on multiple GPUs, a strategy called <strong>Tensor Parallelism</strong>. In essence, this means that each model tensor is split up across multiple GPUs so that no single GPU must hold the entire model in memory. The drawback of this is that the GPUs must communicate with each other to generate the model outputs. Nevertheless, with the high-bandwidth NVLink, this overhead is acceptable for the ability to run larger models. The option to use multiple GPUs in vLLM is very simple: <code>vllm serve ... --tensor-parallel-size=4</code>.</p>
<p>In our workload, each model instance processed only a portion of the full dataset, a strategy called <strong>Data Parallelism</strong>. I implemented data parallelism on the client side, outside vLLM, which means that the client decides what data to send to the vLLM server based on its own <em>rank</em> within the full workload. SLURM supports a feature called <a href="https://slurm.schedmd.com/job_array.html">Job Arrays</a>, which allow you to submit a collection of similar jobs. Each job in the collection gets a unique serial ID (e.g., 0, 1, ‚Ä¶, 100), which SLURM sets in the environment variable <code>SLURM_ARRAY_TASK_ID</code>. The model instance can read this ID (its <em>rank</em>) and understand which partition of the dataset it is assigned to process. I found the descriptions of these parallelism strategies on the <a href="https://huggingface.co/docs/transformers/main/perf_train_gpu_many">Hugging Face Parallelism Methods page</a> useful, for starters. The code in Section&nbsp;12 shows how we used <code>SLURM_ARRAY_TASK_ID</code> in our SLURM file.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/parallelism-strategies.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Different Parallelism Strategies: Data Parallelism partitions the dataset so that N different model instances process different parts of the dataset. Tensor Parallelism distributes a model instance across K separate GPUs. In our workload N=10 and K=4."><img src="https://tomasruizt.github.io/posts/05_mm-inference-on-vllm/imgs/parallelism-strategies.png" class="img-fluid figure-img" style="width:40.0%" data-text-align="left" alt="Different Parallelism Strategies"></a></p>
<figcaption>Different Parallelism Strategies: Data Parallelism partitions the dataset so that <img src="https://latex.codecogs.com/png.latex?N"> different model instances process different parts of the dataset. Tensor Parallelism distributes a model instance across <img src="https://latex.codecogs.com/png.latex?K"> separate GPUs. In our workload <img src="https://latex.codecogs.com/png.latex?N=10"> and <img src="https://latex.codecogs.com/png.latex?K=4">.</figcaption>
</figure>
</div>
</section>
<section id="fault-tolerance" class="level1">
<h1>Fault Tolerance</h1>
<p>The vLLM library supports different inference modes: It is possible to use <em>batch processing</em>, which takes a batch of requests at once, processes them and returns the results. Alternatively, it‚Äôs possible to spin up an inference server behind an OpenAI-compatible API, which takes requests <em>asynchronously</em>, and returns the results individually as they become available. For our large workload, we expected the batch processing to be more efficient, but at the time of writing, the batch processing code <strong>blocked</strong> until all requests were completed before returning. If any error occurred during processing, the entire batch would fail, and failures were more common than one would expect: e.g., an exception was raised mid-workload because a long video did not fit in the context window.</p>
<p>Therefore, I opted for the asynchronous inference mode, which offered better <strong>fault tolerance</strong>. As soon as a request completed, we dumped its result to a file with JSON-lines format. If the workload failed halfway, we still had all previous results on disk. If a single request failed (e.g.&nbsp;video too long), it was also logged to the results file. This approach made it possible to restart a failed workload later, by determining which requests were still missing from the results file. The JSON-lines file format allows for efficient append-only incremental writes, but is not a very efficient data format for downstream use, because it takes up a lot of disk space and is slow to read. Therefore, once result files were complete, we converted them to the <a href="https://github.com/apache/parquet-format">Parquet format</a>, which has great lossless compression and is super fast to read.</p>
<p>Sending thousands of multimodal requests to the server <strong>concurrently</strong> choked it, because the server accepted all requests and started processing all videos (e.g., loading from disk), but neglected to generate tokens. On top of this, processing too many concurrent requests filled up the KV cache very quickly. You could observe this in the vLLM logs: the KV cache utilization spiked to 100% and stayed there for most of the workload. To cope with this, the server evicted blocks from the KV cache, and put most requests back in the queue, where they effectively sat idle. To prevent this situation, I implemented a client-side throttling mechanism, which limited the number of requests sent to the server at the same time. We set 16 concurrent requests for Qwen2.5-VL-72B. Getting this to work properly with Python‚Äôs <code>asyncio</code> library and correct timeouts was tricky, but now you can just <a href="https://github.com/tomasruizt/llm_app/blob/8e122aed8f81880c4299416ffacc562c4b5f150f/llmlib/llmlib/openai/openai_completion.py#L174">copy the code from my repo</a> or use the Python code from Section&nbsp;13 directly.</p>
<section id="preventing-clashes" class="level2">
<h2 class="anchored" data-anchor-id="preventing-clashes">Preventing Clashes</h2>
<p>Multiple model instances might be co-located on the same node, e.g., an 8 GPU node can host 2√ó4 GPU jobs. To prevent the jobs from clashing over the same vLLM port, we set different ports for each model instance based on its rank (<code>SLURM_ARRAY_TASK_ID</code>). We also gave each model instance a unique enroot container name based on the <code>SLURM_ARRAY_TASK_ID</code>, to prevent co-located jobs from clashing over the same container name. Also, we deleted the created enroot container after the job finished. To diagnose the failure of an individual job task within a job array, it‚Äôs useful to pipe <code>stderr</code> and <code>stdout</code> of each job task to different files, as shown in lines 5 and 6 of our SLURM file in Section&nbsp;12. It‚Äôs possible to restart individual job tasks within a job array with SLURM: <code>sbatch &lt;slurm-file&gt; --array=&lt;failed-task-id&gt;</code>.</p>
</section>
</section>
<section id="prefill-decode-ratio" class="level1">
<h1>Prefill-Decode Ratio</h1>
<p>We processed over 552 million input tokens, and generated over 9 million output tokens. Per request, we processed 15.8k input tokens and produced over 250 output tokens, on average. The ratio of input tokens to output tokens is very skewed (61 to 1). It means that most compute in our workload was not spent decoding long answers, but rather processing the many input tokens. The workload is therefore called ‚Äúprefill-heavy‚Äù. This contrasts to ‚Äúdecode-heavy‚Äù workloads, like those involved in solving mathematical problems that generate a long answer including reasoning. The prefill step is generally speaking compute-bound rather than memory-bound, so our workload likely exploits the full compute capacity of the GPUs. Figure&nbsp;1 and Figure&nbsp;2 show the distribution of input and output tokens for the entire workload.</p>
<div id="fig-input-tokens" class="lightbox quarto-float quarto-figure quarto-figure-center anchored" data-text-align="left" alt="Number of input tokens">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-input-tokens-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="imgs/input_tokens.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;1: Distribution of Input Tokens. The distribution is skewed to the left, and most requests required between 5k and 20k input tokens. There were a few outliers with as much as 123k input tokens."><img src="https://tomasruizt.github.io/posts/05_mm-inference-on-vllm/imgs/input_tokens.png" class="img-fluid figure-img" style="width:80.0%" data-text-align="left" alt="Number of input tokens"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-input-tokens-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>Distribution of Input Tokens</strong>. The distribution is skewed to the left, and most requests required between 5k and 20k input tokens. There were a few outliers with as much as 123k input tokens.
</figcaption>
</figure>
</div>
<div id="fig-output-tokens" class="lightbox quarto-float quarto-figure quarto-figure-center anchored" data-text-align="left" alt="Number of output tokens">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-output-tokens-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="imgs/output_tokens.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;2: Distribution of Output Tokens. It‚Äôs important to note that the x-scale is 100 times smaller than for input tokens. Most requests produced between 50 and 600 output tokens. The bimodal shape is likely caused by our prompt, which instructed the model to respond to follow-up questions only in specific cases."><img src="https://tomasruizt.github.io/posts/05_mm-inference-on-vllm/imgs/output_tokens.png" class="img-fluid figure-img" style="width:80.0%" data-text-align="left" alt="Number of output tokens"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-output-tokens-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>Distribution of Output Tokens</strong>. It‚Äôs important to note that the x-scale is <em>100 times smaller</em> than for input tokens. Most requests produced between 50 and 600 output tokens. The bimodal shape is likely caused by our prompt, which instructed the model to respond to follow-up questions only in specific cases.
</figcaption>
</figure>
</div>
</section>
<section id="throughput-stats" class="level1">
<h1>Throughput Stats</h1>
<p>I estimated from the logs that the <strong>throughput</strong> per model instance was ~2.95 seconds per request (~3 for simplicity). Dividing the ~15k input tokens and ~250 output tokens per request gives a throughput of ~5k input tokens and ~83 output tokens per second per model instance (‚âà5,080 tokens/s total throughput).</p>
<p>This is a high throughput compared to other benchmarks: The vLLM documentation reports a <a href="https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen2.5-VL.html#qwen25vl-72b-benchmark-on-random-synthetic-dataset">synthetic benchmark</a> with a ratio of ~9:1 input tokens per output token, which achieves a lower throughput of 1461 tokens/s. The higher throughput of our workload might be just because we use better GPUs (H100s vs A100s). NVIDIA has also reported <a href="https://docs.nvidia.com/nim/benchmarking/llm/latest/performance.html#llama-3-3-70b-instruct-results">performance benchmarks</a> for the similarly-sized model Llama-3.3-70b-instruct running on 4 H100 GPUs and their <a href="https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html">NIM</a> inference framework. However, the peak throughputs reported by NVIDIA vary <em>a lot</em> depending not only on the ratio of input to output tokens, but also the total number of tokens per request, with throughput peaks ranging from 349 to 6085 tokens/s (I report their best results in Table&nbsp;1). The throughput we achieved is competitive without being ‚Äútoo good to be true‚Äù üöÄ.</p>
<div id="tbl-throughput-stats" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-throughput-stats-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>Throughput Comparison.</strong> All benchmarks use instruct models. Input and output tokens are per request. The token ratio tells if the workload is prefill- or decode-heavy. The NVIDIA numbers are the most optimistic from their benchmarks.
</figcaption>
<div aria-describedby="tbl-throughput-stats-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>Benchmark</th>
<th>Model</th>
<th>In. Toks</th>
<th>Out. Toks</th>
<th>Tok. Ratio</th>
<th>GPUs</th>
<th>Throughput <img src="https://latex.codecogs.com/png.latex?%5Cuparrow"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>NVIDIA NIM</td>
<td>Llama-3.3-70b</td>
<td>1000</td>
<td>1000</td>
<td>1:1</td>
<td>4 H100 80GB</td>
<td><strong>6085 tok/s</strong></td>
</tr>
<tr class="even">
<td>Ours (Videos)</td>
<td>Qwen2.5-VL-72B</td>
<td>15000</td>
<td>250</td>
<td>61:1</td>
<td>4 H100 96GB</td>
<td>5020 tok/s</td>
</tr>
<tr class="odd">
<td>vLLM (Synthetic)</td>
<td>Qwen2.5-VL-72B</td>
<td>8000</td>
<td>900</td>
<td>9:1</td>
<td>4 A100 80GB</td>
<td>1461 tok/s</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>All benchmarks used different GPUs. I searched for the difference between the H100 80GB and the H100 96GB GPUs, and found <a href="https://www.ionos.com/digitalguide/server/know-how/nvidia-h100/">this comparison</a> which says the former card has more bfloat16 compute power (1979 vs 1671 TFLOPS), while the latter has higher memory bandwidth (3.9 vs 3.35 TB/s), so it‚Äôs not clear if they either one is better.</p>
</section>
<section id="costs" class="level1">
<h1>Costs</h1>
<p>In terms of total runtime, the workload required 3s/post * 35k posts = 105k s <img src="https://latex.codecogs.com/png.latex?%5Capprox"> 29hr running a single model instance (4 GPUs). The cost of renting an equivalent H100 GPU on <a href="https://www.runpod.io/">Runpod.io</a> is $3.07/hr per GPU (as of 2025-09-24). The total cost of the workload would be $3.07 * 4 GPUs * 29hr = $356.12 (not a very expensive workload). Nevertheless, this does not include the time spent setting up the workload, debugging failures, and tweaking parameters, which required running at least one model instance (~$12/hr).</p>
<div id="fig-h100-nvl-cost-per-hour" class="lightbox quarto-float quarto-figure quarto-figure-center anchored" alt="H100 NVL Renting Costs">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-h100-nvl-cost-per-hour-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="imgs/h100-nvl-cost-per-hour.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;3: H100 NVL Rent Cost. The cost shown is per GPU per hour. The workload required 4 GPUs per model instance for 29 hours."><img src="https://tomasruizt.github.io/posts/05_mm-inference-on-vllm/imgs/h100-nvl-cost-per-hour.png" class="img-fluid figure-img" style="width:60.0%" alt="H100 NVL Renting Costs"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-h100-nvl-cost-per-hour-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <strong>H100 NVL Rent Cost.</strong> The cost shown is per GPU per hour. The workload required 4 GPUs per model instance for 29 hours.
</figcaption>
</figure>
</div>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<p>In terms of social science, we found interesting results, e.g., that a lot of posts promoted a product (42%), and that the most common product sold was dietary supplements (16%). For quality control, we compared the Qwen labels with human labels in a random subset of ~150 posts on a specific product detection question:</p>
<blockquote class="blockquote">
<p><em>‚ÄúDoes the post mention a product or service that is being offered, promoted, or recommended? Answer with ‚Äòyes‚Äô or ‚Äòno‚Äô.‚Äù</em></p>
</blockquote>
<p>On this subset, we also used Gemini-2.5-Pro for classification, intended as an upper performance reference. We compared both models in terms of precision, recall, and F1 scores in Table&nbsp;2. In plain words, the table shows that Qwen was too strict about what constitutes a product mention, while Gemini was too lax, but achieved a better balance.</p>
<div id="tbl-model-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-model-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: <strong>Model Comparison.</strong> Gemini outperformed Qwen in recall, and in F1, but Qwen had higher precision.
</figcaption>
<div aria-describedby="tbl-model-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Precision</th>
<th>Recall</th>
<th>F1 Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Qwen2.5-VL-72B</td>
<td><strong>0.90</strong></td>
<td>0.69</td>
<td>0.78</td>
</tr>
<tr class="even">
<td>Gemini-2.5-Pro</td>
<td>0.81</td>
<td><strong>0.98</strong></td>
<td><strong>0.89</strong></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><strong>Correcting for AI Bias:</strong> The numbers showed that both models had a bias in a specific direction that needed to be corrected to answer a question like the following:</p>
<blockquote class="blockquote">
<p><em>‚ÄúIf humans were to label all 35k posts, what fraction would they label as mentioning a product?‚Äù</em></p>
</blockquote>
<p>It‚Äôs possible to combine the few human labels with the thousands of structurally biased AI labels to answer this question. We followed a method described in <em>Confidence-Driven Inference</em> by <span class="citation" data-cites="gligoric-etal-2025-unconfident">Gligoric et al. (2025)</span> (also <a href="https://sites.google.com/view/ic2s2-bridging-human/home">presented</a> in IC2S2). When computing an average using the combined human and AI labels, the method weights the AI labels with a weight <img src="https://latex.codecogs.com/png.latex?%5Clambda%20%5Cin%20%5B0,1%5D">. The weight is 0 when the AI labels are completely uncorrelated to the human labels, and 1 when they are completely correlated. This method allowed us to correct the Qwen model bias in our estimates, and also to make statements about confidence intervals like the following:</p>
<blockquote class="blockquote">
<p><em>‚ÄúThe percentage of the 35k posts mentioning a product is ~53%, with the 95% confidence interval being 47% to 59%.‚Äù</em></p>
</blockquote>
<p>The confidence intervals are useful for hypothesis testing, and for making statements about statistical significance. More details in our <a href="./other-files/poster_ic2s2_25.pdf">IC2S2 poster</a>.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>I showed a practical recipe to classify a dataset of 35,000 videos with a 72B parameter model. We went over the infrastructure needed in terms of GPUs, tensor- and data-parallelism strategies, SLURM commands, concrete precautions for fault tolerance, and tips on video inference with vLLM. Our workload was a prefill-heavy workload, and I showed how the throughput we achieved is competitive with existing benchmarks by NVIDIA and vLLM. Finally, I explained how to use thousands of potentially biased AI labels to make statistically valid arguments. I hope this guide is useful to other researchers aiming to analyze large video datasets with multimodal LLMs that require multiple GPUs.</p>
<p><strong>Outlook:</strong> In the future, I hope to scale this pipeline to 1M videos and push the throughput limits with quantization methods and speculative decoding. Testing audio-capable models is also high on my agenda. If you are interested in collaborating on large-scale inference, reach out to me! üöÄ</p>
</section>
<section id="sec-slurm-file" class="level1">
<h1>SLURM File</h1>
<p>Below is a GitHub Gist of the SLURM file to submit our workload on the LRZ AI Systems. It shows how to request multiple GPUs, configure SLURM job arrays, set vLLM ports, and set up and tear down the enroot container, etc. Unlike the Python code, this will not run without the dataset, but it shows the general workflow.</p>
<script src="https://gist.github.com/tomasruizt/6b48b51c343b48ee2f1a3ebb9a6ea129.js"></script>
</section>
<section id="sec-python-code" class="level1">
<h1>Python Code</h1>
<p>The code below shows how to use Python to (1) start up the vLLM server, and (2) send concurrent requests to the server. It uses a library I wrote for this purpose: <a href="https://github.com/tomasruizt/llm_app">GitHub/tomasruizt/llm_app</a>. The snippet below is in the repo‚Äôs <code>examples/</code> directory.</p>
<script src="https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Ftomasruizt%2Fllm_app%2Fblob%2Fmain%2Fexamples%2Fvllm-video-inference.py&amp;style=github&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on"></script>
</section>
<section id="references" class="level1">

<!-- 
What to write about
‚úÖ The model, how many GPUs are needed (Tensor Parallelism)
‚úÖ that we transcribe using whisper
‚úÖ How I split the dataset (Data Parallelism)
‚úÖ The LRZ, what GPUs and nodes are available
   ‚úÖ how these are connected to each other
‚úÖ SLURM, how array jobs work
  ‚úÖ preventing port clashes
‚úÖ The posts are multimodal: videos, imgs, text
‚úÖ Incremental progress by splitting requests from backend server
‚úÖ Client-side throttling, since different models have different reqs

‚úÖ How much storage is 35k videos in TB
‚úÖ Write about enroot containers
‚úÖ make a picture display tensor parallelism & data parallelism
‚úÖ Include a snippet about how to use your llm_app library.
Results: 
  ‚úÖ How many tokens did we process?
  ‚úÖHow many posts actually promote a product?
  ‚úÖHow long did it take to process the dataset?
-->



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-abdin2024phi4technicalreport" class="csl-entry">
Abdin, Marah, Jyoti Aneja, Harkirat Behl, S√©bastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, et al. 2024. <span>‚ÄúPhi-4 Technical Report.‚Äù</span> <a href="https://arxiv.org/abs/2412.08905">https://arxiv.org/abs/2412.08905</a>.
</div>
<div id="ref-bai2025qwen25vltechnicalreport" class="csl-entry">
Bai, Shuai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, et al. 2025. <span>‚ÄúQwen2.5-VL Technical Report.‚Äù</span> <a href="https://arxiv.org/abs/2502.13923">https://arxiv.org/abs/2502.13923</a>.
</div>
<div id="ref-gligoric-etal-2025-unconfident" class="csl-entry">
Gligoric, Kristina, Tijana Zrnic, Cinoo Lee, Emmanuel Candes, and Dan Jurafsky. 2025. <span>‚ÄúCan Unconfident <span>LLM</span> Annotations Be Used for Confident Conclusions?‚Äù</span> In <em>Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>, edited by Luis Chiruzzo, Alan Ritter, and Lu Wang, 3514‚Äì33. Albuquerque, New Mexico: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2025.naacl-long.179">https://doi.org/10.18653/v1/2025.naacl-long.179</a>.
</div>
<div id="ref-DBLP:conf/sosp/KwonLZ0ZY0ZS23" class="csl-entry">
Kwon, Woosuk, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. <span>‚ÄúEfficient Memory Management for Large Language Model Serving with PagedAttention.‚Äù</span> In <em>SOSP</em>, 611‚Äì26. <a href="https://doi.org/10.1145/3600006.3613165">https://doi.org/10.1145/3600006.3613165</a>.
</div>
<div id="ref-pmlr-v202-radford23a" class="csl-entry">
Radford, Alec, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever. 2023. <span>‚ÄúRobust Speech Recognition via Large-Scale Weak Supervision.‚Äù</span> In <em>Proceedings of the 40th International Conference on Machine Learning</em>, edited by Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, 202:28492‚Äì518. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v202/radford23a.html">https://proceedings.mlr.press/v202/radford23a.html</a>.
</div>
<div id="ref-wang2024qwen2vlenhancingvisionlanguagemodels" class="csl-entry">
Wang, Peng, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, et al. 2024. <span>‚ÄúQwen2-VL: Enhancing Vision-Language Model‚Äôs Perception of the World at Any Resolution.‚Äù</span> <a href="https://arxiv.org/abs/2409.12191">https://arxiv.org/abs/2409.12191</a>.
</div>
<div id="ref-xu2025qwen25omnitechnicalreport" class="csl-entry">
Xu, Jin, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, et al. 2025. <span>‚ÄúQwen2.5-Omni Technical Report.‚Äù</span> <a href="https://arxiv.org/abs/2503.20215">https://arxiv.org/abs/2503.20215</a>.
</div>
<div id="ref-yoo2003slurm" class="csl-entry">
Yoo, Andy B, Morris A Jette, and Mark Grondona. 2003. <span>‚ÄúSlurm: Simple Linux Utility for Resource Management.‚Äù</span> In <em>Workshop on Job Scheduling Strategies for Parallel Processing</em>, 44‚Äì60. Springer.
</div>
<div id="ref-zheng2024sglang" class="csl-entry">
Zheng, Lianmin, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, et al. 2024. <span>‚Äú<span>SGL</span>ang: Efficient Execution of Structured Language Model Programs.‚Äù</span> In <em>The Thirty-Eighth Annual Conference on Neural Information Processing Systems</em>. <a href="https://openreview.net/forum?id=VqkAKQibpq">https://openreview.net/forum?id=VqkAKQibpq</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{ruiz2025,
  author = {Ruiz, Tomas},
  title = {A {Guide} to {Classify} 35,000 {Videos} with a {72B}
    {Multimodal} {LLM} on the {vLLM} {Engine}},
  date = {2025-09-22},
  url = {https://tomasruizt.github.io/posts/05_mm-inference-on-vllm/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-ruiz2025" class="csl-entry quarto-appendix-citeas">
Ruiz, Tomas. 2025. <span>‚ÄúA Guide to Classify 35,000 Videos with a 72B
Multimodal LLM on the vLLM Engine.‚Äù</span> September 22, 2025. <a href="https://tomasruizt.github.io/posts/05_mm-inference-on-vllm/">https://tomasruizt.github.io/posts/05_mm-inference-on-vllm/</a>.
</div></div></section></div> ]]></description>
  <category>GPUs</category>
  <category>vLLM</category>
  <category>Transformers</category>
  <guid>https://tomasruizt.github.io/posts/05_mm-inference-on-vllm/</guid>
  <pubDate>Mon, 22 Sep 2025 05:00:00 GMT</pubDate>
  <media:content url="https://tomasruizt.github.io/posts/05_mm-inference-on-vllm/imgs/parallelism-strategies-square.png" medium="image" type="image/png" height="140" width="144"/>
</item>
<item>
  <title>Drilling Down into Multimodal Attention</title>
  <dc:creator>Tomas Ruiz</dc:creator>
  <link>https://tomasruizt.github.io/posts/04_multimodal-attn/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/multimodal-attn.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Visualizing Multimodal Attention Patterns"><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/multimodal-attn.png" class="img-fluid figure-img" style="width:70.0%" data-text-align="left" alt="Visualizing Multimodal Attention"></a></p>
<figcaption>Visualizing Multimodal Attention Patterns</figcaption>
</figure>
</div>
<section id="summary" class="level1">
<h1>Summary</h1>
<p>This post explains how to inspect the attention patterns of a vision-language models (VLMs) using a new module I created <a href="https://github.com/tomasruizt/CircuitsVis">on a fork</a> of the <code>circuitsviz</code> library. To interact with an example, <a href="https://storage.googleapis.com/tomas-ruiz-multimodal-attn/2025-02-03-visualization/PaliGemma2_layer_25_attention_heads.html">click here</a>. My analysis suggests that the PaliGemma2 model, which uses a prefix-attention mask, has trained its <code>&lt;bos&gt;</code> token to be a ‚Äúbroker‚Äù token for visual information. Finding key tokens like this has important implications for making VLMs more compute efficient and interpretable. All the code to reproduce the analysis is <a href="https://github.com/tomasruizt/visualizing-multimodal-attn">on Github</a>.</p>
</section>
<section id="mechanistic-interpretability" class="level1">
<h1>Mechanistic Interpretability</h1>
<p>Large language models (LLMs) are notoriously difficult to interpret (black-box). One approach to shed light on LLMs is mechanistic interpretability, which aims to understand the inner workings of the model by breaking down its components. The <a href="https://distill.pub/">distill.pub journal</a> hosted early works on this topic, the team at <a href="https://transformer-circuits.pub/">Anthropic</a> continued the tradition, and today researchers actively contribute to the field.</p>
</section>
<section id="attention-patterns" class="level1">
<h1>Attention Patterns</h1>
<p>The central component of the Transformer architecture is the attention mechanism, which allows the LLM to focus on different parts of the input sequence. Most interpretability research on attention has focused on text-only models, finding e.g.&nbsp;‚Äúinduction heads‚Äù. These are heads that learn to copy part of the input sequence into the output, and form an important mechanism for in-context learning <span class="citation" data-cites="olsson2022incontextlearninginductionheads">(Olsson et al. 2022)</span>.</p>
<p>To find such attention patterns, it is essential to have effective visualization tools like the <code>circuitsviz</code> library. The examples below show two different modules in the library to visualize attention over tokens. Each token in the input sequence attends to the all other tokens (therefore the squared shape of the pattern). The attention mechanism determines the color intensity: dark fields mean high attention, white fields means low attention, and gray fields are inactive. Click on any image in this post to see a larger version.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-induction-head" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-induction-head-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/attn-pattern-induction-head.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;1: (Example 1) Induction Head Pattern. The top-right triangle is inactive due to the ‚Äúcausal attention mask‚Äù of the Transformer, which prevents tokens from attending to future tokens. We observe a diagonal pattern in blue, which shows the induction head ‚Äúcopying‚Äù the sequence, because the tokens are repeating the sequence."><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/attn-pattern-induction-head.png" class="img-fluid figure-img" style="width:80.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-induction-head-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: (Example 1) Induction Head Pattern. The top-right triangle is inactive due to the ‚Äúcausal attention mask‚Äù of the Transformer, which prevents tokens from attending to future tokens. We observe a diagonal pattern in blue, which shows the induction head ‚Äúcopying‚Äù the sequence, because the tokens are repeating the sequence.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/many-txt-attention-heads.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="(Example 2) Multiple Attention Heads. Each head in the multi-head-attention block of the Transformer learn to attend to different elements in the inputs, and therefore has different attention patterns."><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/many-txt-attention-heads.png" class="img-fluid figure-img" style="width:120.0%" alt="(Example 2) Multiple Attention Heads. Each head in the multi-head-attention block of the Transformer learn to attend to different elements in the inputs, and therefore has different attention patterns."></a></p>
<figcaption>(Example 2) Multiple Attention Heads. Each head in the multi-head-attention block of the Transformer learn to attend to different elements in the inputs, and therefore has different attention patterns.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="multimodal-tokens" class="level1">
<h1>Multimodal Tokens</h1>
<p>But how are images turned into tokens? In contrast to text-only LLMs, VLMs can also process images. A VLM consists of a vision encoder, an LLM and a linear layer to combine both. The vision encoder is a vision transformer (ViT) <span class="citation" data-cites="dosovitskiy2021imageworth16x16words">(Dosovitskiy et al. 2021)</span> that has been pre-trained with (image, text) pairs, like CLIP <span class="citation" data-cites="radford2021learningtransferablevisualmodels">(Radford et al. 2021)</span> or SigLIP <span class="citation" data-cites="zhai2023sigmoid">(Zhai et al. 2023)</span>. The VLM converts the image into a sequence of image tokens in two steps:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/vision-transformer.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="1. First, The image is cropped into a square and broken down into 16x16=256 non-overlapping patches, which are projected into 256 tokens. The ViT adds one additional token named [CLS] to hold global information about the entire image, for a total of 257 tokens. All patches are then passed through the ViT to create visual embeddings. Image taken from [@dosovitskiy2021imageworth16x16words]"><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/vision-transformer.png" class="img-fluid figure-img" style="width:60.0%" alt="1. First, The image is cropped into a square and broken down into 16x16=256 non-overlapping patches, which are projected into 256 tokens. The ViT adds one additional token named [CLS] to hold global information about the entire image, for a total of 257 tokens. All patches are then passed through the ViT to create visual embeddings. Image taken from (Dosovitskiy et al. 2021)"></a></p>
<figcaption>1. First, The image is cropped into a square and broken down into 16x16=256 non-overlapping patches, which are projected into 256 tokens. The ViT adds one additional token named <code>[CLS]</code> to hold global information about the entire image, for a total of 257 tokens. All patches are then passed through the ViT to create visual embeddings. Image taken from <span class="citation" data-cites="dosovitskiy2021imageworth16x16words">(Dosovitskiy et al. 2021)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/llava-architecture.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="2. Second, the visual embeddings are concatenated with the text embeddings and fed into the LLM. Image taken from [@liu2023visualinstructiontuning]"><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/llava-architecture.png" class="img-fluid figure-img" style="width:60.0%" alt="2. Second, the visual embeddings are concatenated with the text embeddings and fed into the LLM. Image taken from (Liu et al. 2023)"></a></p>
<figcaption>2. Second, the visual embeddings are concatenated with the text embeddings and fed into the LLM. Image taken from <span class="citation" data-cites="liu2023visualinstructiontuning">(Liu et al. 2023)</span></figcaption>
</figure>
</div>
<p>In theory, we could visualize the multimodal attention patterns in with the same approach as the text-only pattern, like in Figure&nbsp;1. But the input sequence is very long now (257 tokens + text tokens), and the pattern grows quadratically with the number of tokens. Also, the image tokens are concatenated by row by row, so their vertical spatial structure is lost in the naive text-only visualization.</p>
</section>
<section id="visualizing-multimodal-attention" class="level1">
<h1>Visualizing Multimodal Attention</h1>
<p>This is where the new visualization shines: It overlays the attention pattern over the image, so we can appreciate the spatial structure of the attention over the image. The main visualization is split in two attention grids: The left grid shows <strong>only a single row</strong> of the image self-attention pattern, rearranged spatially on top of the image. The right grid is the classic self-attention of the text tokens.</p>
<p>By clicking on any token on either grid, the token is selected as the ‚Äúdestination‚Äù token, and the left grid switches to that row of the attention pattern. It is possible to tune the contrast of the attention with a slider, to see patterns with lower attention values. See the video below as an example.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/oIhhqn1tDhk" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="case-study-paligemma2" class="level1">
<h1>Case Study: PaliGemma2</h1>
<p>I use the PaliGemma2 VLM <span class="citation" data-cites="steiner2024paligemma2familyversatile">(Steiner et al. 2024)</span> by Google as my case study, because it does not use a causal attention mask, but a prefix-attention mask. This means that the attention pattern is not triangular, and it means that early tokens can attend to the later tokens. In particular, the image tokens, which are concatenated first in the sequence, can attend to text tokens in the prompt. In contrast to other VLMs, the PaliGemma2 model does not use the <code>[CLS]</code> token of the ViT. However, PaliGemma2 prepends the text prompt with a <code>&lt;bos&gt;</code> (beginning of sentence) token, so the <code>&lt;bos&gt;</code> token becomes the first text token in the input sequence.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/paligemma2-architecture.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="PaliGemma2 architecture. The VLM consists of a vision encoder (SigLIP), an LLM (Gemma2) and a linear layer to combine both. Google released 3 sizes of the model, and with 3 different image resolutions. from [@steiner2024paligemma2familyversatile]"><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/paligemma2-architecture.png" class="img-fluid figure-img" style="width:120.0%" alt="PaliGemma2 architecture. The VLM consists of a vision encoder (SigLIP), an LLM (Gemma2) and a linear layer to combine both. Google released 3 sizes of the model, and with 3 different image resolutions. from (Steiner et al. 2024)"></a></p>
<figcaption>PaliGemma2 architecture. The VLM consists of a vision encoder (SigLIP), an LLM (Gemma2) and a linear layer to combine both. Google released 3 sizes of the model, and with 3 different image resolutions. from <span class="citation" data-cites="steiner2024paligemma2familyversatile">(Steiner et al. 2024)</span></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/prefix-attn.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="PaliGemma Prefix Attention Pattern. Note that the image tokens and the text tokens (Prefix) fully attend to each other. Only the output tokens (Suffix/Target) have a causal mask. The <eos> token is the ‚Äúend of sentence‚Äù token. Image from [@beyer2024paligemmaversatile3bvlm]"><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/prefix-attn.png" class="img-fluid figure-img" style="width:80.0%" alt="PaliGemma Prefix Attention Pattern. Note that the image tokens and the text tokens (Prefix) fully attend to each other. Only the output tokens (Suffix/Target) have a causal mask. The <eos> token is the ‚Äúend of sentence‚Äù token. Image from (Beyer et al. 2024)"></a></p>
<figcaption>PaliGemma Prefix Attention Pattern. Note that the image tokens and the text tokens (Prefix) fully attend to each other. Only the output tokens (Suffix/Target) have a causal mask. The <code>&lt;eos&gt;</code> token is the ‚Äúend of sentence‚Äù token. Image from <span class="citation" data-cites="beyer2024paligemmaversatile3bvlm">(Beyer et al. 2024)</span></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>PaliGemma2 uses a special syntax for the prompt: For the model to answer a question in the english (en) language, we must prefix the text question with <code>"Answer en &lt;question&gt;"</code>. For example, given the image of the dog with the frisbee, the model can correctly answer the question <code>"Answer en what is the color of the frisbee?"</code> with <code>"purple"</code>.</p>
<section id="paligemma2s-attention-patterns" class="level2">
<h2 class="anchored" data-anchor-id="paligemma2s-attention-patterns">PaliGemma2‚Äôs Attention Patterns</h2>
<p>We now drill down into PaliGemma2‚Äôs attention patterns. When looking at the attention patterns, the first thing that jumps out is that the text tokens are not attending to the image tokens very much, because the image is almost completely white (even at zero attention, the image remains visible to prevent it from dissapearing completely). This effect is consistent across layers (See Figure&nbsp;2, Figure&nbsp;3, Figure&nbsp;4). This is surprising, because the question can only be answered by attending to the image. How does then PaliGemma2 answer the question?</p>
<div id="fig-paligemma-layer-00" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-paligemma-layer-00-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/paligemma-layer-00-heads.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;2: Layer 0: Link to full visualization"><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/paligemma-layer-00-heads.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-paligemma-layer-00-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <a href="https://storage.googleapis.com/tomas-ruiz-multimodal-attn/2025-02-03-visualization/PaliGemma2_layer_0_attention_heads.html">Layer 0: Link to full visualization</a>
</figcaption>
</figure>
</div>
<div id="fig-paligemma-layer-15" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-paligemma-layer-15-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/paligemma-layer-15-heads.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;3: Layer 15: Link to full visualization Dark vertical bars, but first row (<bos> token) is white"><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/paligemma-layer-15-heads.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-paligemma-layer-15-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <a href="https://storage.googleapis.com/tomas-ruiz-multimodal-attn/2025-02-03-visualization/PaliGemma2_layer_15_attention_heads.html">Layer 15: Link to full visualization</a> Dark vertical bars, but first row (<code>&lt;bos&gt;</code> token) is white
</figcaption>
</figure>
</div>
<div id="fig-paligemma-layer-25" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-paligemma-layer-25-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/paligemma-layer-25-heads.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;4: Layer 25: Link to full visualization"><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/paligemma-layer-25-heads.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-paligemma-layer-25-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <a href="https://storage.googleapis.com/tomas-ruiz-multimodal-attn/2025-02-03-visualization/PaliGemma2_layer_25_attention_heads.html">Layer 25: Link to full visualization</a>
</figcaption>
</figure>
</div>
<p>In the middle layers (layer 15), vertical bars are visible in almost every head. This indicates that most text tokens are attending to the <code>&lt;bos&gt;</code> token, which is the first token after the image tokens. Interestingly, the <code>&lt;bos&gt;</code> does not attend back to the other text tokens. We can tell because the first row of the colored pattern is completely white (close to 0 attention) in middle and late layers.</p>
<p>So what is the <code>&lt;bos&gt;</code> token attending to? Mostly to image tokens. To see this, I increase the contrast of the attention patterns using the slider and compare the attentions with different destination text tokens. The <code>&lt;bos&gt;</code> token is attending uniformly to many image tokens. The images below are all from intermediate layers (layer 15).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/paligemma-layer-15-dest-token-bos-maxatt-low.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="The <bos> token attends uniformly to many image tokens"><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/paligemma-layer-15-dest-token-bos-maxatt-low.png" class="img-fluid figure-img" alt="The <bos> token attends uniformly to many image tokens"></a></p>
<figcaption>The <code>&lt;bos&gt;</code> token attends uniformly to many image tokens</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/paligemma-layer-15-dest-token-first-maxatt-low.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="The next text token attends sparsely to image tokens"><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/paligemma-layer-15-dest-token-first-maxatt-low.png" class="img-fluid figure-img" alt="The next text token attends sparsely to image tokens"></a></p>
<figcaption>The next text token attends sparsely to image tokens</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/paligemma-layer-15-dest-token-last-maxatt-low.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="The last text token also attends sparsely to image tokens, although more in patches."><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/paligemma-layer-15-dest-token-last-maxatt-low.png" class="img-fluid figure-img" alt="The last text token also attends sparsely to image tokens, although more in patches."></a></p>
<figcaption>The last text token also attends sparsely to image tokens, although more in patches.</figcaption>
</figure>
</div>
<p>This suggests a hypothesis: Namely that the visual information flows from the image tokens into the <code>&lt;bos&gt;</code> token, and then from the <code>&lt;bos&gt;</code> token to the rest of the text tokens. To quantify this, I partition the input into 3 regions: The image tokens, the <code>&lt;bos&gt;</code> token, and the rest of the text tokens. By summing up the attention of the tokens in each region, we get a measure of the attention between regions. This yields a 3x3 matrix, where each row sums up to 1.</p>
<div id="fig-blockwise-attn-sums-frisbee" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-blockwise-attn-sums-frisbee-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/blockwise-attn-sums.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;5: Self-attention over input regions. In lower layers, image tokens mostly attend to each other, despite having access to all tokens. Later, in middle and final layers, image tokens also attend to the <bos> token, (an example of information flowing back from text to image). The <bos> token increasingly attends to the image tokens as depth increases. Text tokens attend as much (or more) to the <bos>token as to the image tokens, despite their ratio being 1:256."><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/blockwise-attn-sums.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-blockwise-attn-sums-frisbee-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Self-attention over input regions. In lower layers, image tokens mostly attend to each other, despite having access to all tokens. Later, in middle and final layers, image tokens also attend to the <code>&lt;bos&gt;</code> token, (an example of information flowing back from text to image). The <code>&lt;bos&gt;</code> token increasingly attends to the image tokens as depth increases. Text tokens attend as much (or more) to the <code>&lt;bos&gt;</code>token as to the image tokens, despite their ratio being 1:256.
</figcaption>
</figure>
</div>
<p>These numbers suggest that <strong>PaliGemma2 has trained the <code>&lt;bos&gt;</code> token to be a ‚Äúbroker‚Äù token for visual information:</strong> The <code>&lt;bos&gt;</code> token ‚Äúcollects‚Äù and aggregates visual information from the image tokens into a single place, and then ‚Äúserves‚Äù it back to text and image tokens. It plays a similar role as the <code>[CLS]</code> token in the ViT.</p>
</section>
<section id="do-the-numbers-generalize" class="level2">
<h2 class="anchored" data-anchor-id="do-the-numbers-generalize">Do the Numbers Generalize?</h2>
<p>To test if the hypothesis holds in general for (image, text) pairs other than the example of the dog with the frisbee, I ran the analysis on the first 1000 distinct images from the VQA dataset (train) and their corresponding questions. The dataset has multiple questions per image, but I used only the first question so as to have the most visual variation within the 1000 samples.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/vqa-grid-of-img-question-answer.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="VQA images, questions, and model answers. The images are diverse real-life images, and the questions are short open- and closed-ended questions. Note that the questions are formatted in the PaliGemma2 QA format (‚ÄúAnswer en ‚Äù)."><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/vqa-grid-of-img-question-answer.png" class="img-fluid figure-img" alt="VQA images, questions, and model answers. The images are diverse real-life images, and the questions are short open- and closed-ended questions. Note that the questions are formatted in the PaliGemma2 QA format (‚ÄúAnswer en ‚Äù)."></a></p>
<figcaption>VQA images, questions, and model answers. The images are diverse real-life images, and the questions are short open- and closed-ended questions. Note that the questions are formatted in the PaliGemma2 QA format (‚ÄúAnswer en <question>‚Äù).</question></figcaption>
</figure>
</div>
<p>I computed the self-attention matrix over regions for each (image, question) pair and computed the average and the standard deviation over the 1000 pairs. We observe that the standard deviations are very small, indicating that <strong>the ‚Äúbroker‚Äù role of the <code>&lt;bos&gt;</code> token is robust and independent of the image and question.</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/blockwise-attn-sums-vqa1000.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Self-attention over input regions for 1000 VQA samples. The standard deviations are small. The numbers for previous example (purple frisbee) in Figure&nbsp;5 are mostly within the 1-\sigma confidence interval here, suggesting it is a typical example."><img src="https://tomasruizt.github.io/posts/04_multimodal-attn/images/blockwise-attn-sums-vqa1000.png" class="img-fluid figure-img" alt="Self-attention over input regions for 1000 VQA samples. The standard deviations are small. The numbers for previous example (purple frisbee) in Figure&nbsp;5 are mostly within the 1-\sigma confidence interval here, suggesting it is a typical example."></a></p>
<figcaption>Self-attention over input regions for 1000 VQA samples. The standard deviations are small. The numbers for previous example (purple frisbee) in Figure&nbsp;5 are mostly within the 1-<img src="https://latex.codecogs.com/png.latex?%5Csigma"> confidence interval here, suggesting it is a typical example.</figcaption>
</figure>
</div>
</section>
</section>
<section id="conclusion-and-outlook" class="level1">
<h1>Conclusion and Outlook</h1>
<p>I showed how to visualize multimodal attention patterns using the new module for <code>circuitsviz</code>, which is useful for exploratory work in interpretability. I used PaliGemma2 as an interesting case study, because of its prefix-attention mask. After inspecting the attention patterns, I hypothesized that the <code>&lt;bos&gt;</code> token is trained to be a ‚Äúbroker‚Äù token for visual information, and I showed that this phenomenon is independent of the input image and question on VQA.</p>
<p><strong>Yet, more analysis remains to be done:</strong> If the token is truly a ‚Äúbroker‚Äù token, then visual information flow should be disrupted if this token is causally intervened on (patching). It is also possible that the ‚Äúbroker‚Äù role is not tied to the <code>&lt;bos&gt;</code> token specifically, but to the first text token in the input (whatever it is). Finding key tokens in VLMs has been useful to improve their efficiency because the less important tokens can be pruned away and don‚Äôt have to be computed <span class="citation" data-cites="chen2024imageworth12tokens">(Chen et al. 2024)</span> <span class="citation" data-cites="wang2024clstokentellsneeded">(Wang et al. 2024)</span>. We saw in our example that the image tokens outnumber the text tokens (around 256 to 15). This problem is worsened by the quadratic growth of the attention pattern, so pruning image tokens greatly reduces the compute and memory footprint of the model.</p>
<p>Finally, by understanding the mechanisms by which VLMs process visual information, as well as their information bottlenecks, we can monitor them better and make their usage more reliable and safe. We can also control them more easily, for example by intervening on the activations of key tokens when necessary, ultimately improving their safety once deployed.</p>
<section id="acknowledgement" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgement">Acknowledgement</h2>
<p>This is the final project for the course ‚ÄúArtificial Intelligence Safety Fundamentals‚Äù <a href="https://aisafetyfundamentals.com/">(AISF)</a> by BlueDot Impact. The author is funded by the Bavarian Research Institute for Digital Transformation (bidt) and the Ludwig Maximilian University of Munich.</p>
</section>
</section>
<section id="references" class="level1">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-beyer2024paligemmaversatile3bvlm" class="csl-entry">
Beyer, Lucas, Andreas Steiner, Andr√© Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, et al. 2024. <span>‚ÄúPaliGemma: A Versatile 3B VLM for Transfer.‚Äù</span> <a href="https://arxiv.org/abs/2407.07726">https://arxiv.org/abs/2407.07726</a>.
</div>
<div id="ref-chen2024imageworth12tokens" class="csl-entry">
Chen, Liang, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. 2024. <span>‚ÄúAn Image Is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models.‚Äù</span> <a href="https://arxiv.org/abs/2403.06764">https://arxiv.org/abs/2403.06764</a>.
</div>
<div id="ref-dosovitskiy2021imageworth16x16words" class="csl-entry">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>‚ÄúAn Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.‚Äù</span> <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.
</div>
<div id="ref-liu2023visualinstructiontuning" class="csl-entry">
Liu, Haotian, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. <span>‚ÄúVisual Instruction Tuning.‚Äù</span> <a href="https://arxiv.org/abs/2304.08485">https://arxiv.org/abs/2304.08485</a>.
</div>
<div id="ref-olsson2022incontextlearninginductionheads" class="csl-entry">
Olsson, Catherine, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, et al. 2022. <span>‚ÄúIn-Context Learning and Induction Heads.‚Äù</span> <a href="https://arxiv.org/abs/2209.11895">https://arxiv.org/abs/2209.11895</a>.
</div>
<div id="ref-radford2021learningtransferablevisualmodels" class="csl-entry">
Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. <span>‚ÄúLearning Transferable Visual Models from Natural Language Supervision.‚Äù</span> <a href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a>.
</div>
<div id="ref-steiner2024paligemma2familyversatile" class="csl-entry">
Steiner, Andreas, Andr√© Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, et al. 2024. <span>‚ÄúPaliGemma 2: A Family of Versatile VLMs for Transfer.‚Äù</span> <a href="https://arxiv.org/abs/2412.03555">https://arxiv.org/abs/2412.03555</a>.
</div>
<div id="ref-wang2024clstokentellsneeded" class="csl-entry">
Wang, Ao, Fengyuan Sun, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding. 2024. <span>‚Äú[CLS] Token Tells Everything Needed for Training-Free Efficient MLLMs.‚Äù</span> <a href="https://arxiv.org/abs/2412.05819">https://arxiv.org/abs/2412.05819</a>.
</div>
<div id="ref-zhai2023sigmoid" class="csl-entry">
Zhai, Xiaohua, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. <span>‚ÄúSigmoid Loss for Language Image Pre-Training.‚Äù</span> In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 11975‚Äì86.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{ruiz2025,
  author = {Ruiz, Tomas},
  title = {Drilling {Down} into {Multimodal} {Attention}},
  date = {2025-02-01},
  url = {https://tomasruizt.github.io/posts/04_multimodal-attn/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-ruiz2025" class="csl-entry quarto-appendix-citeas">
Ruiz, Tomas. 2025. <span>‚ÄúDrilling Down into Multimodal
Attention.‚Äù</span> February 1, 2025. <a href="https://tomasruizt.github.io/posts/04_multimodal-attn/">https://tomasruizt.github.io/posts/04_multimodal-attn/</a>.
</div></div></section></div> ]]></description>
  <category>Transformers</category>
  <category>Attention</category>
  <guid>https://tomasruizt.github.io/posts/04_multimodal-attn/</guid>
  <pubDate>Sat, 01 Feb 2025 05:00:00 GMT</pubDate>
</item>
<item>
  <title>How Does Tiling Speed Up Matrix Multiplications on GPUs?</title>
  <dc:creator>Tomas Ruiz</dc:creator>
  <link>https://tomasruizt.github.io/posts/03_tiling-for-matrix-mult/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="tiled-matrix-multiplication.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Tiled MatMul according to ChatGPT"><img src="https://tomasruizt.github.io/posts/03_tiling-for-matrix-mult/tiled-matrix-multiplication.webp" class="img-fluid figure-img" style="width:100.0%" data-text-align="center" alt="Tiled Matrix Multiplication"></a></p>
<figcaption>Tiled MatMul according to ChatGPT</figcaption>
</figure>
</div>
<p><strong>TL;DR:</strong> Tiling is a technique used to reduce the number of memory accesses performed during matrix multiplication. We see how it improves compute intensity and how it speeds up the matrix multiplication operation, not only on CPUs, but also on GPUs. I also provide a simple implementation of tiling in CUDA C.</p>
<section id="matrix-multiplication-recap" class="level1">
<h1>Matrix Multiplication Recap</h1>
<p>Matrix multiplication, where <img src="https://latex.codecogs.com/png.latex?AB%20=%20C">, computes each element of <img src="https://latex.codecogs.com/png.latex?C"> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D%20=%20%5Csum_%7Bk=1%7D%5En%20A_%7Bik%7D%20B_%7Bkj%7D"></p>
<p>For simplicity, assume <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> are square matrices of size <img src="https://latex.codecogs.com/png.latex?n">. Below is basic pseudocode for the operation, which involves <img src="https://latex.codecogs.com/png.latex?2n%5E3"> floating-point operations (flops), because of the triple nested loop:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n):</span>
<span id="cb1-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n):</span>
<span id="cb1-3">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n):</span>
<span id="cb1-4">            C[i][j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> A[i][k] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> B[k][j]</span></code></pre></div>
<p>The code is correct, but it is inefficient in terms of its memory access pattern. Let‚Äôs see why.</p>
</section>
<section id="compute-intensity" class="level1">
<h1>Compute Intensity</h1>
<p>Besides the count of flops, the performance of a matmul is also determined by the memory access pattern. The matmul has to fetch data from main memory into a fast cache (L1 or L2), compute, and then return the result to main memory. This roundtrip is time-consuming, and the cores can become idle waiting for the data from main memory. If so, the <strong>memory bandwidth</strong> becomes the bottleneck of the algorithm.</p>
<p>Key Concept: <strong>Compute Intensity</strong>. The compute intensity ratio, defined as flops per memory transfer, indicates whether an algorithm is limited by memory bandwidth or compute power. This concept originates from the Roofline model <span class="citation" data-cites="williams2009roofline">(Williams, Waterman, and Patterson 2009)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>To boost performance, maximize compute intensity by reducing memory transfers per flop.</p>
</div>
</div>
</section>
<section id="memory-access-analysis" class="level1">
<h1>Memory Access Analysis</h1>
<section id="naive-memory-access" class="level2">
<h2 class="anchored" data-anchor-id="naive-memory-access">Naive Memory Access</h2>
<p>In the naive approach, computing <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D"> requires fetching <img src="https://latex.codecogs.com/png.latex?n"> elements from <img src="https://latex.codecogs.com/png.latex?A"> (a row) and <img src="https://latex.codecogs.com/png.latex?n"> elements from <img src="https://latex.codecogs.com/png.latex?B"> (a column). That makes <img src="https://latex.codecogs.com/png.latex?2n"> memory accesses. <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D"> is computed as the dot product of the row and the column, which requires <img src="https://latex.codecogs.com/png.latex?2n"> flops (1 mult and 1 add, <img src="https://latex.codecogs.com/png.latex?n"> times). Thus, the compute intensity is: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BCompute%20Intensity%7D%20=%20%5Cfrac%7B%5Ctext%7Bflops%7D%7D%7B%5Ctext%7Bmemory%20transfers%7D%7D%20=%20%5Cfrac%7B2n%7D%7B2n%7D%20=%201%0A"></p>
<p>Can we do better? In this naive implementation, we are performing redundant memory transfers: For example, to compute <img src="https://latex.codecogs.com/png.latex?C_%7B11%7D">, and <img src="https://latex.codecogs.com/png.latex?C_%7B12%7D">, we fetch the first row of <img src="https://latex.codecogs.com/png.latex?A"> twice from main memory.</p>
</section>
<section id="optimal-memory-access" class="level2">
<h2 class="anchored" data-anchor-id="optimal-memory-access">Optimal Memory Access</h2>
<p>If our cache was (theoretically) large enough, we would transfer all elements of <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> into the cache at once (<img src="https://latex.codecogs.com/png.latex?2n%5E2"> transfers) and perform the full matrix multiplication (<img src="https://latex.codecogs.com/png.latex?2n%5E3"> flops).</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BCompute%20Intensity%7D%20=%20%5Cfrac%7B%5Ctext%7Bflops%7D%7D%7B%5Ctext%7Bmemory%20transfers%7D%7D%20=%20%5Cfrac%7B2n%5E3%7D%7B2n%5E2%7D%20=%20n%0A"> The larger the matrices, the higher the compute intensity.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>Assuming a matrix size of <img src="https://latex.codecogs.com/png.latex?n=10,000">, the naive approach does 1 flop per memory transfer, while the optimal approach does 10,000 flops per memory transfer. This would result in a 10,000x speedup, assuming memory bandwidth remained the bottleneck.</p>
</div>
</div>
</section>
<section id="a-middle-ground-tiling" class="level2">
<h2 class="anchored" data-anchor-id="a-middle-ground-tiling">A Middle Ground: Tiling</h2>
<p>Since caching entire matrices is impractical, we divide matrices <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> into smaller square blocks of size <img src="https://latex.codecogs.com/png.latex?r">, called <strong>tiles</strong>, perform block-wise multiplications, and aggregate the results.</p>
<p>But how does block-wise matrix multiplication work? Let‚Äôs go through an example to gain some intuition. We break the matrices <img src="https://latex.codecogs.com/png.latex?A">, <img src="https://latex.codecogs.com/png.latex?B">, and <img src="https://latex.codecogs.com/png.latex?C"> into 4 blocks each (2x2). Each of these blocks has size <img src="https://latex.codecogs.com/png.latex?n/2">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0AA_%7B11%7D%20&amp;%20A_%7B12%7D%20%5C%5C%0AA_%7B21%7D%20&amp;%20A_%7B22%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0AB_%7B11%7D%20&amp;%20B_%7B12%7D%20%5C%5C%0AB_%7B21%7D%20&amp;%20B_%7B22%7D%0A%5Cend%7Bbmatrix%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0AC_%7B11%7D%20&amp;%20C_%7B12%7D%20%5C%5C%0AC_%7B21%7D%20&amp;%20C_%7B22%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>To compute an submatrix <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D">, we multiply the corresponding blocks of <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> and sum the results. For example:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AC_%7B11%7D%20=%20A_%7B11%7DB_%7B11%7D%20+%20A_%7B12%7DB_%7B21%7D%0A"></p>
<p>In pseudocode this translates to the code below.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1">n_blocks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> r</span>
<span id="cb2-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_blocks):</span>
<span id="cb2-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_blocks):</span>
<span id="cb2-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_blocks):</span>
<span id="cb2-5">            C[i][j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> A[i][k] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> B[k][j]</span></code></pre></div>
<p>Note that the entry <code>C[i][j]</code> is now a block matrix rather than a scalar, and the <code>@</code> operator denotes matrix multiplication, rather than scalar multiplication. Line 5 of the code above is loading blocks of size <img src="https://latex.codecogs.com/png.latex?r%5E2"> from <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> into the cache, which takes <img src="https://latex.codecogs.com/png.latex?2r%5E2"> memory transfers. Then, it multiplies the two blocks, which requires <img src="https://latex.codecogs.com/png.latex?2r%5E3"> flops.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BCompute%20Intensity%7D%20=%20%5Cfrac%7B%5Ctext%7Bflops%7D%7D%7B%5Ctext%7Bmemory%20transfers%7D%7D%20=%20%5Cfrac%7B2r%5E3%7D%7B2r%5E2%7D%20=%20r%0A"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>Assuming a block size of <img src="https://latex.codecogs.com/png.latex?r=100">, the naive approach does 1 flop per memory transfer, while the tiling approach does 100 flops per memory transfer. This would result in a 100x speedup, assuming memory bandwidth remained the bottleneck.</p>
</div>
</div>
<p>It should be clear that <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20r%20%5Cleq%20n">. Setting <img src="https://latex.codecogs.com/png.latex?r=1">, we recover the naive approach, while setting <img src="https://latex.codecogs.com/png.latex?r=n"> we recover the optimal approach. The table below compares the performance of the different methods. Note that for the tiled method, the flops and memory transfers are measured per block, rather than per matrix.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 12%">
<col style="width: 29%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Flops</th>
<th>Memory Transfers</th>
<th>Flops/Memory Transfer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Naive</td>
<td><img src="https://latex.codecogs.com/png.latex?2n%5E3"></td>
<td><img src="https://latex.codecogs.com/png.latex?2n%20%5Ccdot%20n%5E2"></td>
<td><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B2n%5E3%7D%7B2n%20%5Ccdot%20n%5E2%7D%20=%201"></td>
</tr>
<tr class="even">
<td>Tiling</td>
<td><img src="https://latex.codecogs.com/png.latex?2r%5E3"></td>
<td><img src="https://latex.codecogs.com/png.latex?2r%5E2"></td>
<td><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Br%5E3%7D%7Br%5E2%7D%20=%20r"></td>
</tr>
<tr class="odd">
<td>Optimal (Theoretical)</td>
<td><img src="https://latex.codecogs.com/png.latex?2n%5E3"></td>
<td><img src="https://latex.codecogs.com/png.latex?2n%5E2"></td>
<td><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bn%5E3%7D%7Bn%5E2%7D%20=%20n"></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="tiling-on-gpus-cuda" class="level1">
<h1>Tiling on GPUs &amp; CUDA</h1>
<p>GPUs have a large number of cores, which can quickly process the data arriving from memory. Therefore, the memory bandwidth is likely to be a bottleneck in matrix multiplication on GPUs. To relieve the pressure on the memory bandwidth, it‚Äôs necessary to reduce the number of memory transfers by using the tiling technique.</p>
<p>How is this actually implemented on a GPU? The code block below shows a simplified tiled matrix multiplication in CUDA C. The key idea is that the cache is explicitly written and read using CUDA <strong>shared memory</strong>. This is the equivalent to a user-managed L1 cache. The keyword <code>__shared__</code> defines an array used as cache. The variables <code>row</code> and <code>col</code> indicate what element of the matrix <img src="https://latex.codecogs.com/png.latex?C_%7Bij%7D"> the function computes. Line 13 is the loop over blocks. Lines 15-16 load the data from main memory into the cache. Lines 22-24 perform the dot product and accumulate the result. Finally, line 31 writes the result back to global memory. The function <code>__syncthreads()</code> is a CUDA synchronization primitive to avoid a race condition between threads.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode numberSource c number-lines code-with-copy"><code class="sourceCode c"><span id="cb3-1">__global__ <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">void</span> tiledMatMul<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>A<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>B<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>C<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb3-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Define shared memory arrays (cache)</span></span>
<span id="cb3-3">    __shared__ <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span> A_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>TILE_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">][</span>TILE_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">];</span></span>
<span id="cb3-4">    __shared__ <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span> B_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>TILE_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">][</span>TILE_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">];</span></span>
<span id="cb3-5">    </span>
<span id="cb3-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// CUDA thread variables</span></span>
<span id="cb3-7">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> blockIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> blockDim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb3-8">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> blockIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> blockDim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb3-9">    </span>
<span id="cb3-10">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">float</span> C_ij <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">f</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb3-11">    </span>
<span id="cb3-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Loop over the blocks</span></span>
<span id="cb3-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> TILE_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> t<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">++)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb3-14">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Transfer data from main memory into the cache</span></span>
<span id="cb3-15">        A_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">][</span>threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> A<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> TILE_SIZE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">];</span></span>
<span id="cb3-16">        B_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">][</span>threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> B<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[(</span>t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> TILE_SIZE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> col<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">];</span></span>
<span id="cb3-17">        </span>
<span id="cb3-18">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Ensure data transfer is complete before proceeding</span></span>
<span id="cb3-19">        __syncthreads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">();</span></span>
<span id="cb3-20">        </span>
<span id="cb3-21">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Matrix multiply both blocks</span></span>
<span id="cb3-22">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> TILE_SIZE<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">++)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb3-23">            C_ij <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> A_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">][</span>k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> B_block<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">][</span>threadIdx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">];</span></span>
<span id="cb3-24">        <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb3-25">        </span>
<span id="cb3-26">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Finish multiplying the blocks before overwriting the cache next iteration</span></span>
<span id="cb3-27">        __syncthreads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">();</span></span>
<span id="cb3-28">    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb3-29">    </span>
<span id="cb3-30">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Transfer the result back to global memory</span></span>
<span id="cb3-31">    C<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[</span>row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> col<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">]</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> C_ij<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb3-32"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
<p>The CUDA code is more complicated than the pseudocode, but it makes the cache usage explicit. To run the code look at <a href="https://github.com/tomasruizt/Code-Along/blob/main/cuda-pmpp/03_matrix_multiplication/main.cu">my example on Github</a>, which has Makefile for compilation and execution. If you want to see a more complete version, you can find it in the book by <span class="citation" data-cites="kirk2016programming">(Kirk and Wen-Mei 2016)</span>. They describe other techniques to optimize algorithms on GPUs, like memory coalescing, minimizing control divergence, and thread coarsening.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Tiling is a practical optimization technique for improving matrix multiplication performance by minimizing memory transfers and maximizing compute intensity. We saw how the memory access pattern affects the performance of matrix multiplication, and how tiling is implemented concretely in CUDA C.</p>
<p><strong>Acknowledgement:</strong> The author is funded by the Bavarian Research Institute for Digital Transformation (bidt) and the Ludwig Maximilian University of Munich.</p>
</section>
<section id="references" class="level1">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-kirk2016programming" class="csl-entry">
Kirk, David B, and W Hwu Wen-Mei. 2016. <em>Programming Massively Parallel Processors: A Hands-on Approach</em>. Morgan kaufmann.
</div>
<div id="ref-williams2009roofline" class="csl-entry">
Williams, Samuel, Andrew Waterman, and David Patterson. 2009. <span>‚ÄúRoofline: An Insightful Visual Performance Model for Multicore Architectures.‚Äù</span> <em>Communications of the ACM</em> 52 (4): 65‚Äì76.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{ruiz2024,
  author = {Ruiz, Tomas},
  title = {How {Does} {Tiling} {Speed} {Up} {Matrix} {Multiplications}
    on {GPUs?}},
  date = {2024-12-23},
  url = {https://tomasruizt.github.io/posts/03_tiling-for-matrix-mult/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-ruiz2024" class="csl-entry quarto-appendix-citeas">
Ruiz, Tomas. 2024. <span>‚ÄúHow Does Tiling Speed Up Matrix
Multiplications on GPUs?‚Äù</span> December 23, 2024. <a href="https://tomasruizt.github.io/posts/03_tiling-for-matrix-mult/">https://tomasruizt.github.io/posts/03_tiling-for-matrix-mult/</a>.
</div></div></section></div> ]]></description>
  <category>Mathematics</category>
  <category>GPUs</category>
  <guid>https://tomasruizt.github.io/posts/03_tiling-for-matrix-mult/</guid>
  <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
  <media:content url="https://tomasruizt.github.io/posts/03_tiling-for-matrix-mult/tiled-matrix-multiplication-squared.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Grokking an Inner Product Inequality With Python on WebAssembly</title>
  <dc:creator>Tomas Ruiz</dc:creator>
  <link>https://tomasruizt.github.io/posts/02_viz-inequalities-inner-prod-wasm/</link>
  <description><![CDATA[ 





<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>The purpose of this post is two-fold:</p>
<ol type="1">
<li>To showcase Python running directly in your browser <em>without</em> any server behind it, like JavaScript. I will even import libraries like <code>numpy</code> and <code>matplotlib</code>. The underlying technologies that power this are <a href="https://en.wikipedia.org/wiki/WebAssembly">WebAssembly</a> and <a href="https://pyodide.org/">Pyodide</a>, which I encourage you to check out.</li>
<li>To get you excited about a simple inequality and its application to vectors, functions &amp; matrices. These are our objects of study, and we will inspect and visualize them interactively using Python.</li>
</ol>
<p>I created this document using <a href="https://r-wasm.github.io/quarto-live/">Quarto Live</a>. Big thanks to <a href="https://renatatopinkova.github.io/">Renata Topinkova</a> for showing me this tool!</p>
</section>
<section id="the-inequality" class="level2">
<h2 class="anchored" data-anchor-id="the-inequality">The Inequality</h2>
<p>Michael Steele presents in his book this simple inequality based only on the fact that <img src="https://latex.codecogs.com/png.latex?(x-y)%5E2"> is always positive<sup>1</sup>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A0%20&amp;%5Cleq%20%20(x%20-%20y)%5E2%20%5C%5C%0A%5Cimplies%200%20&amp;%5Cleq%20x%5E2%20-2xy%20+y%5E2%20%5C%5C%0A%E2%9F%B9%20xy%20&amp;%E2%89%A4%20%5Cfrac%7B1%7D%7B2%7D%20(x%5E2%20+%20y%5E2)%0A%5Cend%7Balign%7D%0A"></p>
<p>The last inequality above is not intuitively obvious to me. I‚Äôm the kind of person that likes numerical proof to internalize these results, and this is where Python comes in handy. Change the variables <code>x</code> and <code>y</code> in the code below to see if the inequality holds.</p>
<div>
<div id="pyodide-1" class="exercise-cell">

</div>
<script type="pyodide-1-contents">
eyJjb2RlIjoieCA9IDNcbnkgPSA0XG5wcmludChmXCJ7eCp5fSDiiaQgezAuNSAqICh4KnggKyB5KnkpfVwiKSIsImF0dHIiOnsiZXZhbCI6dHJ1ZSwiZWRpdCI6dHJ1ZSwicnVuYnV0dG9uIjpmYWxzZX19
</script>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This Python code is running <em>in your browser</em>. There is no juypter notebook, nor any deployment or any client-server communication behind it! ü§ØüöÄ</p>
</div>
</div>
</section>
<section id="generalizing-to-vectors" class="level2">
<h2 class="anchored" data-anchor-id="generalizing-to-vectors">Generalizing to Vectors</h2>
<p>What happens when we apply this inequality to more than just scalars? It also applies to sequences of numbers, e.g: <img src="https://latex.codecogs.com/png.latex?%0Ax_1%20y_1%20+%20x_2%20y_2%20%3C=%20%5Cfrac%7B1%7D%7B2%7D%20(x_1%5E2%20+%20x_2%5E2)%20+%20%5Cfrac%7B1%7D%7B2%7D%20(y_1%5E2%20+%20y_2%5E2)%0A"> You might recognize that this is equivalent to an inner product: <img src="https://latex.codecogs.com/png.latex?%20x%5ET%20y%20%E2%89%A4%20%5Cfrac%7B1%7D%7B2%7D%20(x%5ET%20x%20+%20y%5ET%20y)"> where <img src="https://latex.codecogs.com/png.latex?x%20=%20%5Bx_1,%20%5Cdots,%20x_n%5D"> and <img src="https://latex.codecogs.com/png.latex?y%20=%20%5By_1,%20%5Cdots,%20y_n%5D">.</p>
<p>The inequality is asserting that the vector product <img src="https://latex.codecogs.com/png.latex?x%5ETy"> of <em>any</em> two vectors <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y"> has an upper bound given by the average of <img src="https://latex.codecogs.com/png.latex?x%5ETx"> and <img src="https://latex.codecogs.com/png.latex?y%5ETy">.</p>
<p>Once again, I‚Äôm not intuitively convinced until I see code running. Notice how we import <code>numpy</code>, which calls compiled C routines under the hood (<em>but our runtime is the browser now</em>).</p>
<div>
<div id="pyodide-2" class="exercise-cell">

</div>
<script type="pyodide-2-contents">
eyJjb2RlIjoiaW1wb3J0IG51bXB5IGFzIG5wXG5cbnggPSBucC5hcnJheShbMSwgMiwgM10pXG55ID0gbnAuYXJyYXkoWzQsIDUsIDZdKVxuXG5wcmludChmXCJ7eCBAIHl9IOKJpCB7MC41ICogKHggQCB4ICsgeSBAIHkpfVwiKSIsImF0dHIiOnsiZXZhbCI6dHJ1ZSwiZWRpdCI6dHJ1ZSwicnVuYnV0dG9uIjpmYWxzZX19
</script>
</div>
</section>
<section id="generalizing-to-functions" class="level2">
<h2 class="anchored" data-anchor-id="generalizing-to-functions">Generalizing to Functions</h2>
<p>You might have heard that functions are infinite-dimensional vectors (ü§Ø). In that case, the inequality also applies! But how does the inner product <img src="https://latex.codecogs.com/png.latex?x%5ETy"> for two functions look like?</p>
<p>The convention is to use the bracket notation <img src="https://latex.codecogs.com/png.latex?%E2%9F%A8%20x,%20y%20%E2%9F%A9"> rather than <img src="https://latex.codecogs.com/png.latex?x%5ETy">. To sum over the infinite individual entries of the (function) vector, we use the integral:</p>
<p><img src="https://latex.codecogs.com/png.latex?%E2%9F%A8%20f,%20g%20%E2%9F%A9%20=%20%5Cint%20f(x)%20g(x)%20dx"> Using this definition, the inequality holds for functions as well: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%E2%9F%A8%20f,%20g%20%E2%9F%A9%20&amp;%E2%89%A4%20%5Cfrac%7B1%7D%7B2%7D%20(%E2%9F%A8%20f,%20f%20%E2%9F%A9%20+%20%E2%9F%A8%20g,%20g%20%E2%9F%A9)%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20%5Cleft(%20%5Cint%20f(x)%5E2%20dx%20+%20%5Cint%20g(x)%5E2%20dx%20%5Cright)%0A%5Cend%7Balign%7D%0A"></p>
<p>Let‚Äôs take two concrete functions <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Ccos(x)"> and <img src="https://latex.codecogs.com/png.latex?g(x)%20=%20%5Csin(4x)">. I choose these arbitrarily because plotting them looks nice. Feel free to use different functions <code>f</code> and <code>g</code> in the code.</p>
<script type="exercise-setup-ex_funcs-contents">
eyJjb2RlIjoiaW1wb3J0IG1hdHBsb3RsaWIucHlwbG90IGFzIHBsdFxuXG5kZWYgcGxvdCh4LCBmLCBnKTpcbiAgICBwbHQucGxvdCh4LCBmKmcsIGxhYmVsPVwiJGYgXFxcXGNkb3QgZyRcIiwgY29sb3I9XCJncmVlblwiKVxuICAgIHBsdC5wbG90KHgsIGYqZiwgbGFiZWw9XCIkZl4yJFwiLCBjb2xvcj1cImxpZ2h0Ymx1ZVwiKVxuICAgIHBsdC5wbG90KHgsIGcqZywgbGFiZWw9XCIkZ14yJFwiLCBjb2xvcj1cImxpZ2h0Ymx1ZVwiKVxuICAgIHBsdC5wbG90KHgsIDAuNSAqIChmKmYgKyBnKmcpLCBsYWJlbD1cIiQwLjUgKGZeMiArIGdeMikkXCIsIGNvbG9yPVwicmVkXCIpXG4gICAgcGx0LnRpdGxlKFwiVXBwZXIgYm91bmQgZm9yICRcXFxcbGFuZ2xlIGYsIGcgXFxcXHJhbmdsZSRcIilcbiAgICBwbHQubGVnZW5kKClcbiAgICBwbHQuZ3JpZCgpXG4gICAgcGx0LnNob3coKSIsImF0dHIiOnsiZXZhbCI6dHJ1ZSwiZWRpdCI6dHJ1ZSwic2V0dXAiOnRydWUsImV4ZXJjaXNlIjoiZXhfZnVuY3MifX0=
</script>
<div>
<div id="pyodide-4" class="exercise-cell">

</div>
<script type="pyodide-4-contents">
eyJjb2RlIjoiaW1wb3J0IG51bXB5IGFzIG5wXG5cbnggPSBucC5saW5zcGFjZSgwLCAyLCAxMDApXG5mID0gbnAuY29zKHgpXG5nID0gbnAuc2luKDQqeClcbnBsb3QoeCwgZiwgZykiLCJhdHRyIjp7ImV2YWwiOnRydWUsImV4ZXJjaXNlIjoiZXhfZnVuY3MiLCJydW5idXR0b24iOmZhbHNlLCJhdXRvcnVuIjp0cnVlLCJlZGl0Ijp0cnVlfX0=
</script>
</div>
<p>In the plot above, the individual functions <img src="https://latex.codecogs.com/png.latex?f%5E2"> and <img src="https://latex.codecogs.com/png.latex?g%5E2"> are plotted with light-blue lines. Their average is the red line, and the product <img src="https://latex.codecogs.com/png.latex?f%20%E2%8B%85%20g"> is the green line. The red line is an upper bound for the green one. We see that the green line crosses over the two blue lines at different points but never crosses over the red line.</p>
<p><strong>About the integral:</strong> Perhaps you noticed that I formulated the inequality on inner-products, but I‚Äôm plotting the functions pointwise. The missing step is the integral, which is evaluated in Python using the numpy function <code>np.trapz()</code>. As we can confirm below, the inequality holds:</p>
<div>
<div id="pyodide-5" class="exercise-cell">

</div>
<script type="pyodide-5-contents">
eyJjb2RlIjoiaW1wb3J0IG51bXB5IGFzIG5wXG5cbnggPSBucC5saW5zcGFjZSgwLCAyLCAxMDApXG5mID0gbnAuY29zKHgpXG5nID0gbnAuc2luKDQqeClcblxucHJpbnQoZlwie25wLnRyYXB6KGYqZywgeCl9IOKJpCB7MC41ICogKG5wLnRyYXB6KGYqZiwgeCkgKyBucC50cmFweihnKmcsIHgpKX1cIikiLCJhdHRyIjp7ImV2YWwiOnRydWUsImVkaXQiOnRydWUsInJ1bmJ1dHRvbiI6ZmFsc2V9fQ==
</script>
</div>
</section>
<section id="generalizing-to-matrices" class="level2">
<h2 class="anchored" data-anchor-id="generalizing-to-matrices">Generalizing to Matrices</h2>
<p>Will the inequality also apply to matrices? The inner product of two matrices <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> (also called <a href="https://en.wikipedia.org/wiki/Frobenius_inner_product">Frobenius inner product</a>) is defined as: <img src="https://latex.codecogs.com/png.latex?%E2%9F%A8A,%20B%E2%9F%A9%20=%20%5Ctext%7Btr%7D(A%5ET%20B)"> where <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Btr%7D"> is the trace operator.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Beware that this inner product is different from matrix multiplication <img src="https://latex.codecogs.com/png.latex?%E2%9F%A8A,%20B%E2%9F%A9%20=%20tr(A%5ET%20B)%20%E2%89%A0%20AB"></p>
</div>
</div>
<p>The inequality for matrices then reads:</p>
<p><img src="https://latex.codecogs.com/png.latex?tr(A%5ET%20B)%20%E2%89%A4%20%5Cfrac%7B1%7D%7B2%7D%20(tr(A%5ET%20A)%20+%20tr(B%5ET%20B))"></p>
<p>It‚Äôs easy to convince yourself that the inequality holds for matrices by writing down the trace as a sum of scalars. As we verified before, inequality holds for each scalar in the sum:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Atr(A%5ET%20B)%0A&amp;=%20%5Csum_%7Bi,j%7D%20A_%7Bij%7D%20B_%7Bij%7D%20&amp;%20%5Ctext%7B(definition)%7D%5C%5C%0A&amp;%E2%89%A4%20%5Cfrac%7B1%7D%7B2%7D%20%5Cleft(%20%5Csum_%7Bi,j%7D%20A_%7Bij%7D%5E2%20+%20%5Csum_%7Bi,j%7D%20B_%7Bij%7D%5E2%20%5Cright)%20&amp;%5Ctext%7B(applied%20by%20scalar)%7D%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20(tr(A%5ET%20A)%20+%20tr(B%5ET%20B))%0A%5Cend%7Balign%7D%0A"></p>
<p>Let‚Äôs check the inequality with random matrices. You can use the <code>"Start Over"</code> button to re-run the code with new matrices.</p>
<div>
<div id="pyodide-6" class="exercise-cell">

</div>
<script type="pyodide-6-contents">
eyJjb2RlIjoiaW1wb3J0IG51bXB5IGFzIG5wXG5cbmRpbSA9IDJcbkEgPSBucC5yYW5kb20ucmFuZG4oZGltLCBkaW0pXG5CID0gbnAucmFuZG9tLnJhbmRuKGRpbSwgZGltKVxuXG5kZWYgaXAoWCwgWSk6ICAjIGlubmVyIHByb2R1Y3RcbiAgcmV0dXJuIG5wLnRyYWNlKFguVCBAIFkpXG5cbnByaW50KGZcIntpcChBLCBCKX0g4omkIHswLjUgKiAoaXAoQSwgQSkgKyBpcChCLCBCKSl9XCIpIiwiYXR0ciI6eyJldmFsIjp0cnVlLCJlZGl0Ijp0cnVlLCJydW5idXR0b24iOmZhbHNlfX0=
</script>
</div>
<p>The inequality holds, but I have no geometric intuition about the trace of a matrix, or how this inequality could be visualized for matrices. If you have an idea, please let me know! üôè</p>
</section>
<section id="further-sources" class="level2">
<h2 class="anchored" data-anchor-id="further-sources">Further Sources</h2>
<p>If you found the mathematics interesting, particularly the generalization of inner products, I recommend MIT‚Äôs course <a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/">Matrix Calculus for Machine Learning and Beyond</a>, which covers this topic in more detail, and goes <em>much</em> further üòÑ.</p>


<script type="pyodide-data">
eyJvcHRpb25zIjp7ImluZGV4VVJMIjoiaHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L3B5b2RpZGUvdjAuMjguMS9mdWxsLyIsImVudiI6eyJQTE9UTFlfUkVOREVSRVIiOiJwbG90bHlfbWltZXR5cGUifX0sInBhY2thZ2VzIjp7InBrZ3MiOlsicHlvZGlkZV9odHRwIiwibWljcm9waXAiLCJpcHl0aG9uIl19fQ==
</script>
<script type="ojs-module-contents">
{"contents":[{"source":"viewof _pyodide_editor_6 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-6-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-6-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_6 = pyodideOjs.process(_pyodide_editor_6, {});\n","cellName":"pyodide-6","methodName":"interpret","inline":false},{"source":"viewof _pyodide_editor_5 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-5-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-5-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_5 = pyodideOjs.process(_pyodide_editor_5, {});\n","cellName":"pyodide-5","methodName":"interpret","inline":false},{"source":"viewof _pyodide_editor_4 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-4-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  // Default exercise configuration\n  const options = Object.assign(\n    {\n      id: \"pyodide-4-contents\",\n      envir: `exercise-env-${block.attr.exercise}`,\n      error: false,\n      caption: 'Exercise',\n    },\n    block.attr\n  );\n\n  const editor = new PyodideExerciseEditor(pyodideOjs.pyodidePromise, block.code, options);\n  return editor.container;\n}\nviewof _pyodide_value_4 = pyodideOjs.process(_pyodide_editor_4, {});\n_pyodide_feedback_4 = {\n  const { PyodideGrader } = window._exercise_ojs_runtime;\n  const emptyFeedback = document.createElement('div');\n\n  const grader = new PyodideGrader(_pyodide_value_4.evaluator);\n  const feedback = await grader.gradeExercise();\n  if (!feedback) return emptyFeedback;\n  return feedback;\n}\n","cellName":"pyodide-4","methodName":"interpret","inline":false},{"source":"viewof _pyodide_editor_2 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-2-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_2 = pyodideOjs.process(_pyodide_editor_2, {});\n","cellName":"pyodide-2","methodName":"interpret","inline":false},{"source":"viewof _pyodide_editor_1 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-1-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_1 = pyodideOjs.process(_pyodide_editor_1, {});\n","cellName":"pyodide-1","methodName":"interpret","inline":false},{"source":"pyodideOjs = {\n  const {\n    PyodideEvaluator,\n    PyodideEnvironmentManager,\n    setupPython,\n    startPyodideWorker,\n    b64Decode,\n    collapsePath,\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // Make any reveal slides with live cells scrollable\n  document.querySelectorAll(\".reveal .exercise-cell\").forEach((el) => {\n    el.closest('section.slide').classList.add(\"scrollable\");\n  })\n\n  // Pyodide supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"pyodide-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  let pyodidePromise = (async () => {\n    statusText.textContent = `Downloading Pyodide`;\n    const pyodide = await startPyodideWorker(data.options);\n\n    statusText.textContent = `Downloading package: micropip`;\n    await pyodide.loadPackage(\"micropip\");\n    const micropip = await pyodide.pyimport(\"micropip\");\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return micropip.install(pkg);\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n    await micropip.destroy();\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await pyodide.FS.mkdir(path);\n        } catch (e) {\n          if (e.name !== \"ErrnoError\") throw e;\n          if (e.errno !== 20) {\n            const errorTextPtr = await pyodide._module._strerror(e.errno);\n            const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n            throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      try {\n        return await pyodide.FS.writeFile(file, new Uint8Array(data));\n      } catch (e) {\n        if (e.name !== \"ErrnoError\") throw e;\n        const errorTextPtr = await pyodide._module._strerror(e.errno);\n        const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n        throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n      }\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Pyodide environment setup`;\n    await setupPython(pyodide);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return pyodide;\n  })().catch((err) => {\n    statusText.style.color = \"var(--exercise-editor-hl-er, #AD0000)\";\n    statusText.textContent = err.message;\n    //indicatorContainer.querySelector(\".spinner-grow\").classList.add(\"d-none\");\n    throw err;\n  });\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const pyodide = await pyodidePromise;\n    const evaluator = new PyodideEvaluator(pyodide, context);\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    pyodidePromise,\n    renderedOjs,\n    process,\n  };\n}\n","cellName":"pyodide-prelude","methodName":"interpretQuiet","inline":false}]}
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>‚ÄúThe Cauchy-Schwarz Master Class: An Introduction to the Art of Mathematical Inequalities‚Äù by J. Michael Steele.‚Ü©Ô∏é</p></li>
</ol>
</section></div> ]]></description>
  <category>Mathematics</category>
  <category>Python</category>
  <guid>https://tomasruizt.github.io/posts/02_viz-inequalities-inner-prod-wasm/</guid>
  <pubDate>Thu, 12 Sep 2024 05:00:00 GMT</pubDate>
  <media:content url="https://tomasruizt.github.io/posts/02_viz-inequalities-inner-prod-wasm/upper-bound-img.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>A Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification</title>
  <dc:creator>Tomas Ruiz</dc:creator>
  <link>https://tomasruizt.github.io/posts/01_linear-adaptation/</link>
  <description><![CDATA[ 





<section id="summary" class="level1">
<h1>Summary</h1>
<p>In this post I show how to linearly fine-tune a large language model (LLM) using a closed-form solution, based on the Moore-Penrose Inverse. I will focus on the special case of binary classification because the changes in output are easier to interpret. The new linear transformation <img src="https://latex.codecogs.com/png.latex?W"> is shown in figure 1 (green).</p>
<div id="fig-llm" class="lightbox quarto-float quarto-figure quarto-figure-center anchored" alt="Learned Linear Transformation" data-text-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-llm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="linear-adaptation/linear-adaptation.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: The learned transformation W (green) is applied in parallel to the existing linear layer (blue), preserving the existing knowledge of the model. Both are the summed to make the logits, which are passed to the softmax function."><img src="https://tomasruizt.github.io/posts/01_linear-adaptation/linear-adaptation/linear-adaptation.png" class="img-fluid figure-img" style="width:75.0%" data-text-align="center" alt="Learned Linear Transformation"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-llm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The learned transformation W (green) is applied in parallel to the existing linear layer (blue), preserving the existing knowledge of the model. Both are the summed to make the logits, which are passed to the softmax function.
</figcaption>
</figure>
</div>
</section>
<section id="about-fine-tuning" class="level1">
<h1>About Fine-Tuning</h1>
<p>Large Language Models (LLMs) are great baseline models for <strong>zero-shot</strong> classification, i.e.&nbsp;without any labeled examples. However, one often has a small labeled dataset <img src="https://latex.codecogs.com/png.latex?D"> and is interested in improving the performance over this baseline. In the <strong>few-shot</strong> setting, some labeled examples are provided in the prompt for the model to learn in context. To improve upon this setting, the next step is to <strong>fine-tune</strong> the model on the labeled dataset.</p>
<p>There are different ways to fine-tune LLMs. For example: optimizing all network parameters, optimizing only the parameters of the final layer, or freezing all parameters but introduce a new smaller set of tunable parameters <span class="citation" data-cites="hu2021lora">(Hu et al. 2021)</span>. In this post, I focus on the simple case of fine-tuning the last linear transformation, because I‚Äôm interested in interpreting the changes to individual logits and probabilities.</p>
</section>
<section id="binary-classification" class="level1">
<h1>Binary Classification</h1>
<p>I also focus on binary classification specifically. This means that the model must only answer <code>yes/no</code> or <code>0/1</code> to the prompt. This setting is easier to interpret with metrics like precision, recall or the <img src="https://latex.codecogs.com/png.latex?F1"> score. Furthermore, in the binary case we can interpret how the fine-tuning procedure affected the model by inspecting the answers that flipped between <code>yes/no</code> and vice-versa <span class="citation" data-cites="dutta2024accuracy">(Dutta et al. 2024)</span>.</p>
<p>In terms of computation, we will see that the problem structure of binary classification can be leveraged to compute a closed-form solution efficiently. As shown in Figure&nbsp;1, I add an additional linear transformation <img src="https://latex.codecogs.com/png.latex?W"> before the softmax, and solve for it using the Moore-Penrose Inverse. This is mathematically equivalent to training <img src="https://latex.codecogs.com/png.latex?W"> with gradient descent, but without all the iteration.</p>
</section>
<section id="closed-form-solution" class="level1">
<h1>Closed-Form Solution</h1>
<p>In underbraces, I‚Äôve written the dimension of the matrices and vectors. In the original language model, the probability vector <img src="https://latex.codecogs.com/png.latex?y"> has the same size of the vocabulary <img src="https://latex.codecogs.com/png.latex?V">, and is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cunderbrace%7Bp(y%20%7C%20x_%7B1:t%7D)%7D_%7B(1,V)%7D%0A&amp;=%20%5Ctext%7Bsoftmax%7D(%5Cunderbrace%7B%5Ctext%7Blogits%7D_t%7D_%7B(1,V)%7D)%20%5C%5C%0A&amp;=%20%5Ctext%7Bsoftmax%7D(%5Cunderbrace%7Bz_t%7D_%7B(1,d)%7D%20%5Cunderbrace%7BA%7D_%7B(d,V)%7D)%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?z_t"> is the hidden state for the last token, and <img src="https://latex.codecogs.com/png.latex?A"> is the weights the last linear layer (no bias is included as in <span class="citation" data-cites="chowdhery2023palm">Chowdhery et al. (2023)</span>). The loss for this model is defined as the distance between these probabilites and our true labels, which is a set of binary labels <img src="https://latex.codecogs.com/png.latex?D%20=%20%5Cbegin%7Bbmatrix%7D%20d_1%20%5C%5C%20%5Cvdots%20%5C%5C%20d_N%20%5Cend%7Bbmatrix%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B(N,%202)%7D"></p>
<p>With fine-tuning, we modify the probabilities that the LLM assigns to the tokens for <code>yes/no</code> from <img src="https://latex.codecogs.com/png.latex?p"> to <img src="https://latex.codecogs.com/png.latex?p_a"> (<em>adapted probabilities</em>). The role of <img src="https://latex.codecogs.com/png.latex?W"> is change the logits that are passed to the softmax function. We tweak <img src="https://latex.codecogs.com/png.latex?W"> to approximate the dataset <img src="https://latex.codecogs.com/png.latex?D"> with the adapted probabilities <img src="https://latex.codecogs.com/png.latex?p_a">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Ap_a(y%20%7C%20x_%7B1:t%7D)%20&amp;=%20%5Ctext%7Bsoftmax%7D(%5Ctext%7Blogits%7D_t%20+%20z_t%20W)%20%5C%5C%0A%5Cimplies%20%5Cunderbrace%7B%5Clog%20p_a%7D_%7B(1,%20V)%7D%0A&amp;=%20%5Cunderbrace%7B%5Ctext%7Blogits%7D_t%7D_%7B(1,V)%7D%20+%20%5Cunderbrace%7Bz_t%7D_%7B(1,d)%7D%20%5Cunderbrace%7BW%7D_%7B(d,V)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>In vectorized form (one row per datapoint <img src="https://latex.codecogs.com/png.latex?N">) this can be written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cunderbrace%7B%5Clog%20P_a%7D_%7B(N,V)%7D%0A&amp;=%20%5Cunderbrace%7BL_t%7D_%7B(N,V)%7D%20+%20%5Cunderbrace%7BZ_t%7D_%7B(N,d)%7D%20%5Cunderbrace%7BW%7D_%7B(d,V)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>Solving for <img src="https://latex.codecogs.com/png.latex?W"> exactly is only possible for squared invertible matrices <img src="https://latex.codecogs.com/png.latex?Z_t">. However, <img src="https://latex.codecogs.com/png.latex?W"> is rectangular (size <img src="https://latex.codecogs.com/png.latex?(d,%20V)">), so this problem is solved approximately by minimizing the squared distance:</p>
<p><img src="https://latex.codecogs.com/png.latex?W%20=%20%5Carg%20%5Cmin_W%20%7C%7C%20(%5Clog%20P_a%20-%20L_t)%20-%20Z_t%20W%20%7C%7C%5E2_2%20%5Cqquad%20(1)%20"></p>
<p>This is a least squares problem, whose solution is given by the <strong>Moore-Penrose Inverse</strong>.</p>
<p><img src="https://latex.codecogs.com/png.latex?W%20=%20(Z_t%5ET%20Z_t)%5E%7B-1%7D%20Z_t%5ET%20(%5Clog%20P_a%20-%20L_t)"></p>
<p>Or equivalently, by solving the following linear system of equations with <img src="https://latex.codecogs.com/png.latex?V"> columns (<em>But see note on numerical stability <sup>1</sup></em>).</p>
<p><img src="https://latex.codecogs.com/png.latex?W%20=%20%5Ctext%7Blinsolve%7D(%5Cunderbrace%7BZ_t%5ET%20Z_t%7D_%7B(d,d)%7D,%20%5Cspace%20%5Cunderbrace%7BZ_t%5ET%20(%5Clog%20P_a%20-%20L_t)%7D_%7B(d,V)%7D)%20%5Cqquad%20(2)%20"></p>
</section>
<section id="runtime" class="level1">
<h1>Runtime</h1>
<p>Each linear system takes <img src="https://latex.codecogs.com/png.latex?O(d%5E3)"> to solve, so solving <img src="https://latex.codecogs.com/png.latex?V"> of these systems is prohibitively expensive (<img src="https://latex.codecogs.com/png.latex?V=128k,%20d=4k"> in LLama3 8B). (<em>But see note on repeated linear solves<sup>2</sup></em>). However, we can exploit the structure of the binary classification problem, by only evaluating the logits <img src="https://latex.codecogs.com/png.latex?L_t"> and probabilities <img src="https://latex.codecogs.com/png.latex?P_a"> for the <code>yes/no</code> tokens. This reduces the size of the probability matrix <img src="https://latex.codecogs.com/png.latex?P_a"> by <em>4 to 5 orders of magnitude</em>, from <img src="https://latex.codecogs.com/png.latex?(N,V)"> to <img src="https://latex.codecogs.com/png.latex?(N,2)">. Similarly, the learned matrix <img src="https://latex.codecogs.com/png.latex?W"> shrinks from size <img src="https://latex.codecogs.com/png.latex?(d,V)"> to <img src="https://latex.codecogs.com/png.latex?(d,2)">.</p>
<p>As a result, we need to solve only 2 linear systems, each with runtime constant in the vocabulary size <img src="https://latex.codecogs.com/png.latex?V"> and in the number of datapoints in our dataset <img src="https://latex.codecogs.com/png.latex?N">, but proportional to <img src="https://latex.codecogs.com/png.latex?O(d%5E3)">. As an added benefit of evaluating only the <code>yes/no</code> logits, the output of the fine-tuned model is compliant by design, as it cannot output any other logits other than for <code>yes/no</code>.</p>
<p>To solve for <img src="https://latex.codecogs.com/png.latex?W"> either using eq (1) or eq (2), we plug in our dataset <img src="https://latex.codecogs.com/png.latex?D"> for <img src="https://latex.codecogs.com/png.latex?P_a">, since both matrices have the same size.</p>
</section>
<section id="inference" class="level1">
<h1>Inference</h1>
<p>At inference time, the matrix <img src="https://latex.codecogs.com/png.latex?W"> stays constant, while the logits change for each input.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Ap_a(y%7Cx_%7B1:t%7D)%0A&amp;=%20%5Ctext%7Bsoftmax%7D%20%5C%7B%20%5Ctext%7Blogits%7D_t%20+%20z_t%20W%20%5C%7D%20%5C%5C%0A&amp;=%20%5Ctext%7Bsoftmax%7D%20%5C%7B%20z_t%20A%20+%20z_t%20W%20%5C%7D%0A%5Cend%7Baligned%7D%0A"></p>
</section>
<section id="next-steps" class="level1">
<h1>Next Steps</h1>
<p>In the next post, I will show an implementation of this method in PyTorch, and interpret how linear fine-tuning changes the outputs of the original LLM. I am interested in the flips between <code>yes/no</code> outside of the small fine-tuning dataset <img src="https://latex.codecogs.com/png.latex?D">, and particularly on the boundaries of the dataset, and how this pertains to generalization. Stay tuned! :)</p>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-chowdhery2023palm" class="csl-entry">
Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al. 2023. <span>‚ÄúPalm: Scaling Language Modeling with Pathways.‚Äù</span> <em>Journal of Machine Learning Research</em> 24 (240): 1‚Äì113.
</div>
<div id="ref-dutta2024accuracy" class="csl-entry">
Dutta, Abhinav, Sanjeev Krishnan, Nipun Kwatra, and Ramachandran Ramjee. 2024. <span>‚ÄúAccuracy Is Not All You Need.‚Äù</span> <em>arXiv Preprint arXiv:2407.09141</em>.
</div>
<div id="ref-hu2021lora" class="csl-entry">
Hu, Edward J, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. <span>‚ÄúLora: Low-Rank Adaptation of Large Language Models.‚Äù</span> <em>arXiv Preprint arXiv:2106.09685</em>.
</div>
<div id="ref-watkins2004fundamentals" class="csl-entry">
Watkins, David S. 2004. <em>Fundamentals of Matrix Computations</em>. John Wiley &amp; Sons.
</div>
</div>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>2024-09: The matrix <img src="https://latex.codecogs.com/png.latex?Z%5ET%20Z"> is positive definite, so it is in theory efficiently invertible using the Cholesky decomposition. However, its condition number <img src="https://latex.codecogs.com/png.latex?%5Ckappa%20(Z%5ET%20Z)"> is squarely proportional to the condition number of <img src="https://latex.codecogs.com/png.latex?%5Ckappa(Z)">. This can lead to numerical instability when solving the linear system. In fact, I stumbled upon numerical instability while implementing this in linear system in PyTorch, which lead me to use an <img src="https://latex.codecogs.com/png.latex?L_2"> regularization term. See <a href="https://tobydriscoll.net/fnc-julia/leastsq/normaleqns.html#conditioning-and-stability">Source</a>.‚Ü©Ô∏é</p></li>
<li id="fn2"><p>2024-09: It turns out that solving a linear system with <img src="https://latex.codecogs.com/png.latex?V"> columns on the right-hand side can be done cheaper than in <img src="https://latex.codecogs.com/png.latex?V%20%5Ccdot%20%5Cfrac%7B2%7D%7B3%7D%20d%5E3"> flops. To solve <img src="https://latex.codecogs.com/png.latex?A%20X%20=%20B"> (with <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd,d%7D"> and <img src="https://latex.codecogs.com/png.latex?X,%20B%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd,V%7D">) the factorization of <img src="https://latex.codecogs.com/png.latex?A"> requires <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B2%7D%7B3%7D%20d%5E3"> flops, but it only has to be done once. After that, solving for each of the <img src="https://latex.codecogs.com/png.latex?V"> columns of <img src="https://latex.codecogs.com/png.latex?B"> costs <img src="https://latex.codecogs.com/png.latex?2%20d%5E2"> flops each. So the total flop count is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B2%7D%7B3%7D%20d%5E3%20+%20V%20%5Ccdot%202%20d%5E2">. This is a significant improvement over the naive approach. See <span class="citation" data-cites="watkins2004fundamentals">(Watkins 2004, 77‚Äì78)</span>.‚Ü©Ô∏é</p></li>
</ol>
</section></div> ]]></description>
  <category>Machine Learning</category>
  <guid>https://tomasruizt.github.io/posts/01_linear-adaptation/</guid>
  <pubDate>Fri, 02 Aug 2024 05:00:00 GMT</pubDate>
  <media:content url="https://tomasruizt.github.io/posts/01_linear-adaptation/linear-adaptation/linear-adaptation.png" medium="image" type="image/png" height="128" width="144"/>
</item>
</channel>
</rss>
