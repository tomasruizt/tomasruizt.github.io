<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Tomas&#39; Site</title>
<link>https://tomasruizt.github.io/</link>
<atom:link href="https://tomasruizt.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Tomas Ruiz&#39;s blog</description>
<generator>quarto-1.5.57</generator>
<lastBuildDate>Wed, 11 Sep 2024 22:00:00 GMT</lastBuildDate>
<item>
  <title></title>
  <dc:creator>Tomas Ruiz</dc:creator>
  <link>https://tomasruizt.github.io/posts/viz-inequalities-inner-prod-wasm/</link>
  <description><![CDATA[ 





<section id="grokking-an-inner-product-inequality-with-python-on-webassembly" class="level1">
<h1>Grokking an Inner Product Inequality With Python on WebAssembly</h1>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>The purpose of this post is two-fold:</p>
<ol type="1">
<li>To showcase Python running directly in your browser <em>without</em> any server behind it, like JavaScript. I will even import libraries like <code>numpy</code> and <code>matplotlib</code>. The underlying technologies that power this are <a href="https://en.wikipedia.org/wiki/WebAssembly">WebAssembly</a> and <a href="https://pyodide.org/">Pyodide</a>, which I encourage you to check out.</li>
<li>To get you excited about a simple inequality and its application to vectors, functions &amp; matrices. These are our objects of study, and we will inspect and visualize them interactively using Python.</li>
</ol>
<p>I created this document using <a href="https://r-wasm.github.io/quarto-live/">Quarto Live</a>. Big thanks to <a href="https://renatatopinkova.github.io/">Renata Topinkova</a> for showing me this tool!</p>
</section>
<section id="the-inequality" class="level2">
<h2 class="anchored" data-anchor-id="the-inequality">The Inequality</h2>
<p>Michael Steele presents in his book this simple inequality based only on the fact that <img src="https://latex.codecogs.com/png.latex?(x-y)%5E2"> is always positive<sup>1</sup>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A0%20&amp;%5Cleq%20%20(x%20-%20y)%5E2%20%5C%5C%0A%5Cimplies%200%20&amp;%5Cleq%20x%5E2%20-2xy%20+y%5E2%20%5C%5C%0A%E2%9F%B9%20xy%20&amp;%E2%89%A4%20%5Cfrac%7B1%7D%7B2%7D%20(x%5E2%20+%20y%5E2)%0A%5Cend%7Balign%7D%0A"></p>
<p>The last inequality above is not intuitively obvious to me. I’m the kind of person that likes numerical proof to internalize these results, and this is where Python comes in handy. Change the variables <code>x</code> and <code>y</code> in the code below to see if the inequality holds.</p>
<div>
<div id="pyodide-1">

</div>
<script type="pyodide-1-contents">
eyJjb2RlIjoieCA9IDNcbnkgPSA0XG5wcmludChmXCJ7eCp5fSDiiaQgezAuNSAqICh4KnggKyB5KnkpfVwiKSIsImF0dHIiOnsicnVuYnV0dG9uIjpmYWxzZSwiZXZhbCI6dHJ1ZSwiZWRpdCI6dHJ1ZX19
</script>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This Python code is running <em>in your browser</em>. There is no juypter notebook, nor any deployment or any client-server communication behind it! 🤯🚀</p>
</div>
</div>
</section>
<section id="generalizing-to-vectors" class="level2">
<h2 class="anchored" data-anchor-id="generalizing-to-vectors">Generalizing to Vectors</h2>
<p>What happens when we apply this inequality to more than just scalars? It also applies to sequences of numbers, e.g: <img src="https://latex.codecogs.com/png.latex?%0Ax_1%20y_1%20+%20x_2%20y_2%20%3C=%20%5Cfrac%7B1%7D%7B2%7D%20(x_1%5E2%20+%20x_2%5E2)%20+%20%5Cfrac%7B1%7D%7B2%7D%20(y_1%5E2%20+%20y_2%5E2)%0A"> You might recognize that this is equivalent to an inner product: <img src="https://latex.codecogs.com/png.latex?%20x%5ET%20y%20%E2%89%A4%20%5Cfrac%7B1%7D%7B2%7D%20(x%5ET%20x%20+%20y%5ET%20y)"> where <img src="https://latex.codecogs.com/png.latex?x%20=%20%5Bx_1,%20%5Cdots,%20x_n%5D"> and <img src="https://latex.codecogs.com/png.latex?y%20=%20%5By_1,%20%5Cdots,%20y_n%5D">.</p>
<p>The inequality is asserting that the vector product <img src="https://latex.codecogs.com/png.latex?x%5ETy"> of <em>any</em> two vectors <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y"> has an upper bound given by the average of <img src="https://latex.codecogs.com/png.latex?x%5ETx"> and <img src="https://latex.codecogs.com/png.latex?y%5ETy">.</p>
<p>Once again, I’m not intuitively convinced until I see code running. Notice how we import <code>numpy</code>, which calls compiled C routines under the hood (<em>but our runtime is the browser now</em>).</p>
<div>
<div id="pyodide-2">

</div>
<script type="pyodide-2-contents">
eyJjb2RlIjoiaW1wb3J0IG51bXB5IGFzIG5wXG5cbnggPSBucC5hcnJheShbMSwgMiwgM10pXG55ID0gbnAuYXJyYXkoWzQsIDUsIDZdKVxuXG5wcmludChmXCJ7eCBAIHl9IOKJpCB7MC41ICogKHggQCB4ICsgeSBAIHkpfVwiKSIsImF0dHIiOnsicnVuYnV0dG9uIjpmYWxzZSwiZXZhbCI6dHJ1ZSwiZWRpdCI6dHJ1ZX19
</script>
</div>
</section>
<section id="generalizing-to-functions" class="level2">
<h2 class="anchored" data-anchor-id="generalizing-to-functions">Generalizing to Functions</h2>
<p>You might have heard that functions are infinite-dimensional vectors (🤯). In that case, the inequality also applies! But how does the inner product <img src="https://latex.codecogs.com/png.latex?x%5ETy"> for two functions look like?</p>
<p>The convention is to use the bracket notation <img src="https://latex.codecogs.com/png.latex?%E2%9F%A8%20x,%20y%20%E2%9F%A9"> rather than <img src="https://latex.codecogs.com/png.latex?x%5ETy">. To sum over the infinite individual entries of the (function) vector, we use the integral:</p>
<p><img src="https://latex.codecogs.com/png.latex?%E2%9F%A8%20f,%20g%20%E2%9F%A9%20=%20%5Cint%20f(x)%20g(x)%20dx"> Using this definition, the inequality holds for functions as well: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%E2%9F%A8%20f,%20g%20%E2%9F%A9%20&amp;%E2%89%A4%20%5Cfrac%7B1%7D%7B2%7D%20(%E2%9F%A8%20f,%20f%20%E2%9F%A9%20+%20%E2%9F%A8%20g,%20g%20%E2%9F%A9)%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20%5Cleft(%20%5Cint%20f(x)%5E2%20dx%20+%20%5Cint%20g(x)%5E2%20dx%20%5Cright)%0A%5Cend%7Balign%7D%0A"></p>
<p>Let’s take two concrete functions <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Ccos(x)"> and <img src="https://latex.codecogs.com/png.latex?g(x)%20=%20%5Csin(4x)">. I choose these arbitrarily because plotting them looks nice. Feel free to use different functions <code>f</code> and <code>g</code> in the code.</p>
<script type="exercise-setup-ex_funcs-contents">
eyJjb2RlIjoiaW1wb3J0IG1hdHBsb3RsaWIucHlwbG90IGFzIHBsdFxuXG5kZWYgcGxvdCh4LCBmLCBnKTpcbiAgICBwbHQucGxvdCh4LCBmKmcsIGxhYmVsPVwiJGYgXFxcXGNkb3QgZyRcIiwgY29sb3I9XCJncmVlblwiKVxuICAgIHBsdC5wbG90KHgsIGYqZiwgbGFiZWw9XCIkZl4yJFwiLCBjb2xvcj1cImxpZ2h0Ymx1ZVwiKVxuICAgIHBsdC5wbG90KHgsIGcqZywgbGFiZWw9XCIkZ14yJFwiLCBjb2xvcj1cImxpZ2h0Ymx1ZVwiKVxuICAgIHBsdC5wbG90KHgsIDAuNSAqIChmKmYgKyBnKmcpLCBsYWJlbD1cIiQwLjUgKGZeMiArIGdeMikkXCIsIGNvbG9yPVwicmVkXCIpXG4gICAgcGx0LnRpdGxlKFwiVXBwZXIgYm91bmQgZm9yICRcXFxcbGFuZ2xlIGYsIGcgXFxcXHJhbmdsZSRcIilcbiAgICBwbHQubGVnZW5kKClcbiAgICBwbHQuZ3JpZCgpXG4gICAgcGx0LnNob3coKSIsImF0dHIiOnsiZXhlcmNpc2UiOiJleF9mdW5jcyIsImV2YWwiOnRydWUsInNldHVwIjp0cnVlfX0=
</script>
<div>
<div id="pyodide-4">

</div>
<script type="pyodide-4-contents">
eyJjb2RlIjoiaW1wb3J0IG51bXB5IGFzIG5wXG5cbnggPSBucC5saW5zcGFjZSgwLCAyLCAxMDApXG5mID0gbnAuY29zKHgpXG5nID0gbnAuc2luKDQqeClcbnBsb3QoeCwgZiwgZykiLCJhdHRyIjp7ImVkaXQiOnRydWUsImV4ZXJjaXNlIjoiZXhfZnVuY3MiLCJldmFsIjp0cnVlLCJydW5idXR0b24iOmZhbHNlLCJhdXRvcnVuIjp0cnVlfX0=
</script>
</div>
<p>In the plot above, the individual functions <img src="https://latex.codecogs.com/png.latex?f%5E2"> and <img src="https://latex.codecogs.com/png.latex?g%5E2"> are plotted with light-blue lines. Their average is the red line, and the product <img src="https://latex.codecogs.com/png.latex?f%20%E2%8B%85%20g"> is the green line. The red line is an upper bound for the green one. We see that the green line crosses over the two blue lines at different points but never crosses over the red line.</p>
<p><strong>About the integral:</strong> Perhaps you noticed that I formulated the inequality on inner-products, but I’m plotting the functions pointwise. The missing step is the integral, which is evaluated in Python using the numpy function <code>np.trapz()</code>. As we can confirm below, the inequality holds:</p>
<div>
<div id="pyodide-5">

</div>
<script type="pyodide-5-contents">
eyJjb2RlIjoiaW1wb3J0IG51bXB5IGFzIG5wXG5cbnggPSBucC5saW5zcGFjZSgwLCAyLCAxMDApXG5mID0gbnAuY29zKHgpXG5nID0gbnAuc2luKDQqeClcblxucHJpbnQoZlwie25wLnRyYXB6KGYqZywgeCl9IOKJpCB7MC41ICogKG5wLnRyYXB6KGYqZiwgeCkgKyBucC50cmFweihnKmcsIHgpKX1cIikiLCJhdHRyIjp7InJ1bmJ1dHRvbiI6ZmFsc2UsImV2YWwiOnRydWUsImVkaXQiOnRydWV9fQ==
</script>
</div>
</section>
<section id="generalizing-to-matrices" class="level2">
<h2 class="anchored" data-anchor-id="generalizing-to-matrices">Generalizing to Matrices</h2>
<p>Will the inequality also apply to matrices? The inner product of two matrices <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> (also called <a href="https://en.wikipedia.org/wiki/Frobenius_inner_product">Frobenius inner product</a>) is defined as: <img src="https://latex.codecogs.com/png.latex?%E2%9F%A8A,%20B%E2%9F%A9%20=%20%5Ctext%7Btr%7D(A%5ET%20B)"> where <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Btr%7D"> is the trace operator.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Beware that this inner product is different from matrix multiplication <img src="https://latex.codecogs.com/png.latex?%E2%9F%A8A,%20B%E2%9F%A9%20=%20tr(A%5ET%20B)%20%E2%89%A0%20AB"></p>
</div>
</div>
<p>The inequality for matrices then reads:</p>
<p><img src="https://latex.codecogs.com/png.latex?tr(A%5ET%20B)%20%E2%89%A4%20%5Cfrac%7B1%7D%7B2%7D%20(tr(A%5ET%20A)%20+%20tr(B%5ET%20B))"></p>
<p>It’s easy to convince yourself that the inequality holds for matrices by writing down the trace as a sum of scalars. As we verified before, inequality holds for each scalar in the sum:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Atr(A%5ET%20B)%0A&amp;=%20%5Csum_%7Bi,j%7D%20A_%7Bij%7D%20B_%7Bij%7D%20&amp;%20%5Ctext%7B(definition)%7D%5C%5C%0A&amp;%E2%89%A4%20%5Cfrac%7B1%7D%7B2%7D%20%5Cleft(%20%5Csum_%7Bi,j%7D%20A_%7Bij%7D%5E2%20+%20%5Csum_%7Bi,j%7D%20B_%7Bij%7D%5E2%20%5Cright)%20&amp;%5Ctext%7B(applied%20by%20scalar)%7D%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20(tr(A%5ET%20A)%20+%20tr(B%5ET%20B))%0A%5Cend%7Balign%7D%0A"></p>
<p>Let’s check the inequality with random matrices. You can use the <code>"Start Over"</code> button to re-run the code with new matrices.</p>
<div>
<div id="pyodide-6">

</div>
<script type="pyodide-6-contents">
eyJjb2RlIjoiaW1wb3J0IG51bXB5IGFzIG5wXG5cbmRpbSA9IDJcbkEgPSBucC5yYW5kb20ucmFuZG4oZGltLCBkaW0pXG5CID0gbnAucmFuZG9tLnJhbmRuKGRpbSwgZGltKVxuXG5kZWYgaXAoWCwgWSk6ICAjIGlubmVyIHByb2R1Y3RcbiAgcmV0dXJuIG5wLnRyYWNlKFguVCBAIFkpXG5cbnByaW50KGZcIntpcChBLCBCKX0g4omkIHswLjUgKiAoaXAoQSwgQSkgKyBpcChCLCBCKSl9XCIpIiwiYXR0ciI6eyJydW5idXR0b24iOmZhbHNlLCJldmFsIjp0cnVlLCJlZGl0Ijp0cnVlfX0=
</script>
</div>
<p>The inequality holds, but I have no geometric intuition about the trace of a matrix, or how this inequality could be visualized for matrices. If you have an idea, please let me know! 🙏</p>
</section>
<section id="further-sources" class="level2">
<h2 class="anchored" data-anchor-id="further-sources">Further Sources</h2>
<p>If you found the mathematics interesting, particularly the generalization of inner products, I recommend MIT’s course <a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/">Matrix Calculus for Machine Learning and Beyond</a>, which covers this topic in more detail, and goes <em>much</em> further 😄.</p>


<script type="pyodide-data">
eyJwYWNrYWdlcyI6eyJwa2dzIjpbInB5b2RpZGVfaHR0cCIsIm1pY3JvcGlwIiwiaXB5dGhvbiJdfSwib3B0aW9ucyI6eyJpbmRleFVSTCI6Imh0dHBzOi8vY2RuLmpzZGVsaXZyLm5ldC9weW9kaWRlL3YwLjI2LjEvZnVsbC8ifX0=
</script>
<script type="ojs-module-contents">
{"contents":[{"inline":false,"source":"viewof _pyodide_editor_6 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-6-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-6-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_6 = pyodideOjs.process(_pyodide_editor_6, {});\n","methodName":"interpret","cellName":"pyodide-6"},{"inline":false,"source":"viewof _pyodide_editor_5 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-5-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-5-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_5 = pyodideOjs.process(_pyodide_editor_5, {});\n","methodName":"interpret","cellName":"pyodide-5"},{"inline":false,"source":"viewof _pyodide_editor_4 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-4-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  // Default exercise configuration\n  const options = Object.assign(\n    {\n      id: \"pyodide-4-contents\",\n      envir: `exercise-env-${block.attr.exercise}`,\n      error: false,\n      autorun: false,\n      caption: 'Exercise',\n    },\n    block.attr\n  );\n\n  const editor = new PyodideExerciseEditor(pyodideOjs.pyodidePromise, block.code, options);\n  return editor.container;\n}\nviewof _pyodide_value_4 = pyodideOjs.process(_pyodide_editor_4, {});\n_pyodide_feedback_4 = {\n  const { PyodideGrader } = window._exercise_ojs_runtime;\n  const emptyFeedback = document.createElement('div');\n\n  const grader = new PyodideGrader(_pyodide_value_4.evaluator);\n  const feedback = await grader.gradeExercise();\n  if (!feedback) return emptyFeedback;\n  return feedback;\n}\n","methodName":"interpret","cellName":"pyodide-4"},{"inline":false,"source":"viewof _pyodide_editor_2 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-2-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_2 = pyodideOjs.process(_pyodide_editor_2, {});\n","methodName":"interpret","cellName":"pyodide-2"},{"inline":false,"source":"viewof _pyodide_editor_1 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-1-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_1 = pyodideOjs.process(_pyodide_editor_1, {});\n","methodName":"interpret","cellName":"pyodide-1"},{"inline":false,"source":"pyodideOjs = {\n  const {\n    PyodideEvaluator,\n    PyodideEnvironmentManager,\n    setupPython,\n    startPyodideWorker,\n    b64Decode,\n    collapsePath,\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // Pyodide supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"pyodide-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  let pyodidePromise = (async () => {\n    statusText.textContent = `Downloading Pyodide`;\n    const pyodide = await startPyodideWorker(data.options);\n\n    statusText.textContent = `Downloading package: micropip`;\n    await pyodide.loadPackage(\"micropip\");\n    const micropip = await pyodide.pyimport(\"micropip\");\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return micropip.install(pkg);\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n    await micropip.destroy();\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await pyodide.FS.mkdir(path);\n        } catch (e) {\n          if (e.name !== \"ErrnoError\") throw e;\n          if (e.errno !== 20) {\n            const errorTextPtr = await pyodide._module._strerror(e.errno);\n            const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n            throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      try {\n        return await pyodide.FS.writeFile(file, new Uint8Array(data));\n      } catch (e) {\n        if (e.name !== \"ErrnoError\") throw e;\n        const errorTextPtr = await pyodide._module._strerror(e.errno);\n        const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n        throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n      }\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Pyodide environment setup`;\n    await setupPython(pyodide);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return pyodide;\n  })().catch((err) => {\n    statusText.style.color = \"var(--exercise-editor-hl-er, #AD0000)\";\n    statusText.textContent = err.message;\n    //indicatorContainer.querySelector(\".spinner-grow\").classList.add(\"d-none\");\n    throw err;\n  });\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const pyodide = await pyodidePromise;\n    const evaluator = new PyodideEvaluator(pyodide, context);\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    pyodidePromise,\n    renderedOjs,\n    process,\n  };\n}\n","methodName":"interpretQuiet","cellName":"pyodide-prelude"}]}
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>“The Cauchy-Schwarz Master Class: An Introduction to the Art of Mathematical Inequalities” by J. Michael Steele.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Mathematics</category>
  <category>Python</category>
  <guid>https://tomasruizt.github.io/posts/viz-inequalities-inner-prod-wasm/</guid>
  <pubDate>Wed, 11 Sep 2024 22:00:00 GMT</pubDate>
  <media:content url="https://tomasruizt.github.io/posts/viz-inequalities-inner-prod-wasm/upper-bound-img.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>A Closed-Form Solution to Linearly Fine-Tune LLMs for Binary Classification</title>
  <dc:creator>Tomas Ruiz</dc:creator>
  <link>https://tomasruizt.github.io/posts/linear-adaptation/</link>
  <description><![CDATA[ 





<section id="summary" class="level1">
<h1>Summary</h1>
<p>In this post I show how to linearly fine-tune a large language model (LLM) using a closed-form solution, based on the Moore-Penrose Inverse. I will focus on the special case of binary classification because the changes in output are easier to interpret. The new linear transformation <img src="https://latex.codecogs.com/png.latex?W"> is shown in figure 1 (green).</p>
<div id="fig-llm" class="lightbox quarto-float quarto-figure quarto-figure-center anchored" data-text-align="center" alt="Learned Linear Transformation">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-llm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="linear-adaptation/linear-adaptation.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: The learned transformation W (green) is applied in parallel to the existing linear layer (blue), preserving the existing knowledge of the model. Both are the summed to make the logits, which are passed to the softmax function."><img src="https://tomasruizt.github.io/posts/linear-adaptation/linear-adaptation/linear-adaptation.png" class="img-fluid figure-img" style="width:75.0%" data-text-align="center" alt="Learned Linear Transformation"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-llm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The learned transformation W (green) is applied in parallel to the existing linear layer (blue), preserving the existing knowledge of the model. Both are the summed to make the logits, which are passed to the softmax function.
</figcaption>
</figure>
</div>
</section>
<section id="about-fine-tuning" class="level1">
<h1>About Fine-Tuning</h1>
<p>Large Language Models (LLMs) are great baseline models for <strong>zero-shot</strong> classification, i.e.&nbsp;without any labeled examples. However, one often has a small labeled dataset <img src="https://latex.codecogs.com/png.latex?D"> and is interested in improving the performance over this baseline. In the <strong>few-shot</strong> setting, some labeled examples are provided in the prompt for the model to learn in context. To improve upon this setting, the next step is to <strong>fine-tune</strong> the model on the labeled dataset.</p>
<p>There are different ways to fine-tune LLMs. For example: optimizing all network parameters, optimizing only the parameters of the final layer, or freezing all parameters but introduce a new smaller set of tunable parameters <span class="citation" data-cites="hu2021lora">(Hu et al. 2021)</span>. In this post, I focus on the simple case of fine-tuning the last linear transformation, because I’m interested in interpreting the changes to individual logits and probabilities.</p>
</section>
<section id="binary-classification" class="level1">
<h1>Binary Classification</h1>
<p>I also focus on binary classification specifically. This means that the model must only answer <code>yes/no</code> or <code>0/1</code> to the prompt. This setting is easier to interpret with metrics like precision, recall or the <img src="https://latex.codecogs.com/png.latex?F1"> score. Furthermore, in the binary case we can interpret how the fine-tuning procedure affected the model by inspecting the answers that flipped between <code>yes/no</code> and vice-versa <span class="citation" data-cites="dutta2024accuracy">(Dutta et al. 2024)</span>.</p>
<p>In terms of computation, we will see that the problem structure of binary classification can be leveraged to compute a closed-form solution efficiently. As shown in Figure&nbsp;1, I add an additional linear transformation <img src="https://latex.codecogs.com/png.latex?W"> before the softmax, and solve for it using the Moore-Penrose Inverse. This is mathematically equivalent to training <img src="https://latex.codecogs.com/png.latex?W"> with gradient descent, but without all the iteration.</p>
</section>
<section id="closed-form-solution" class="level1">
<h1>Closed-Form Solution</h1>
<p>In underbraces, I’ve written the dimension of the matrices and vectors. In the original language model, the probability vector <img src="https://latex.codecogs.com/png.latex?y"> has the same size of the vocabulary <img src="https://latex.codecogs.com/png.latex?V">, and is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cunderbrace%7Bp(y%20%7C%20x_%7B1:t%7D)%7D_%7B(1,V)%7D%0A&amp;=%20%5Ctext%7Bsoftmax%7D(%5Cunderbrace%7B%5Ctext%7Blogits%7D_t%7D_%7B(1,V)%7D)%20%5C%5C%0A&amp;=%20%5Ctext%7Bsoftmax%7D(%5Cunderbrace%7Bz_t%7D_%7B(1,d)%7D%20%5Cunderbrace%7BA%7D_%7B(d,V)%7D)%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?z_t"> is the hidden state for the last token, and <img src="https://latex.codecogs.com/png.latex?A"> is the weights the last linear layer (no bias is included as in <span class="citation" data-cites="chowdhery2023palm">Chowdhery et al. (2023)</span>). The loss for this model is defined as the distance between these probabilites and our true labels, which is a set of binary labels <img src="https://latex.codecogs.com/png.latex?D%20=%20%5Cbegin%7Bbmatrix%7D%20d_1%20%5C%5C%20%5Cvdots%20%5C%5C%20d_N%20%5Cend%7Bbmatrix%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B(N,%202)%7D"></p>
<p>With fine-tuning, we modify the probabilities that the LLM assigns to the tokens for <code>yes/no</code> from <img src="https://latex.codecogs.com/png.latex?p"> to <img src="https://latex.codecogs.com/png.latex?p_a"> (<em>adapted probabilities</em>). The role of <img src="https://latex.codecogs.com/png.latex?W"> is change the logits that are passed to the softmax function. We tweak <img src="https://latex.codecogs.com/png.latex?W"> to approximate the dataset <img src="https://latex.codecogs.com/png.latex?D"> with the adapted probabilities <img src="https://latex.codecogs.com/png.latex?p_a">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Ap_a(y%20%7C%20x_%7B1:t%7D)%20&amp;=%20%5Ctext%7Bsoftmax%7D(%5Ctext%7Blogits%7D_t%20+%20z_t%20W)%20%5C%5C%0A%5Cimplies%20%5Cunderbrace%7B%5Clog%20p_a%7D_%7B(1,%20V)%7D%0A&amp;=%20%5Cunderbrace%7B%5Ctext%7Blogits%7D_t%7D_%7B(1,V)%7D%20+%20%5Cunderbrace%7Bz_t%7D_%7B(1,d)%7D%20%5Cunderbrace%7BW%7D_%7B(d,V)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>In vectorized form (one row per datapoint <img src="https://latex.codecogs.com/png.latex?N">) this can be written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cunderbrace%7B%5Clog%20P_a%7D_%7B(N,V)%7D%0A&amp;=%20%5Cunderbrace%7BL_t%7D_%7B(N,V)%7D%20+%20%5Cunderbrace%7BZ_t%7D_%7B(N,d)%7D%20%5Cunderbrace%7BW%7D_%7B(d,V)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>Solving for <img src="https://latex.codecogs.com/png.latex?W"> exactly is only possible for squared invertible matrices <img src="https://latex.codecogs.com/png.latex?Z_t">. However, <img src="https://latex.codecogs.com/png.latex?W"> is rectangular (size <img src="https://latex.codecogs.com/png.latex?(d,%20V)">), so this problem is solved approximately by minimizing the squared distance:</p>
<p><img src="https://latex.codecogs.com/png.latex?W%20=%20%5Carg%20%5Cmin_W%20%7C%7C%20(%5Clog%20P_a%20-%20L_t)%20-%20Z_t%20W%20%7C%7C%5E2_2%20%5Cqquad%20(1)%20"></p>
<p>This is a least squares problem, whose solution is given by the <strong>Moore-Penrose Inverse</strong>.</p>
<p><img src="https://latex.codecogs.com/png.latex?W%20=%20(Z_t%5ET%20Z_t)%5E%7B-1%7D%20Z_t%5ET%20(%5Clog%20P_a%20-%20L_t)"></p>
<p>Or equivalently, by solving the following linear system of equations with <img src="https://latex.codecogs.com/png.latex?V"> columns (<em>But see note on numerical stability <sup>1</sup></em>).</p>
<p><img src="https://latex.codecogs.com/png.latex?W%20=%20%5Ctext%7Blinsolve%7D(%5Cunderbrace%7BZ_t%5ET%20Z_t%7D_%7B(d,d)%7D,%20%5Cspace%20%5Cunderbrace%7BZ_t%5ET%20(%5Clog%20P_a%20-%20L_t)%7D_%7B(d,V)%7D)%20%5Cqquad%20(2)%20"></p>
</section>
<section id="runtime" class="level1">
<h1>Runtime</h1>
<p>Each linear system takes <img src="https://latex.codecogs.com/png.latex?O(d%5E3)"> to solve, so solving <img src="https://latex.codecogs.com/png.latex?V"> of these systems is prohibitively expensive (<img src="https://latex.codecogs.com/png.latex?V=128k,%20d=4k"> in LLama3 8B). (<em>But see note on repeated linear solves<sup>2</sup></em>). However, we can exploit the structure of the binary classification problem, by only evaluating the logits <img src="https://latex.codecogs.com/png.latex?L_t"> and probabilities <img src="https://latex.codecogs.com/png.latex?P_a"> for the <code>yes/no</code> tokens. This reduces the size of the probability matrix <img src="https://latex.codecogs.com/png.latex?P_a"> by <em>4 to 5 orders of magnitude</em>, from <img src="https://latex.codecogs.com/png.latex?(N,V)"> to <img src="https://latex.codecogs.com/png.latex?(N,2)">. Similarly, the learned matrix <img src="https://latex.codecogs.com/png.latex?W"> shrinks from size <img src="https://latex.codecogs.com/png.latex?(d,V)"> to <img src="https://latex.codecogs.com/png.latex?(d,2)">.</p>
<p>As a result, we need to solve only 2 linear systems, each with runtime constant in the vocabulary size <img src="https://latex.codecogs.com/png.latex?V"> and in the number of datapoints in our dataset <img src="https://latex.codecogs.com/png.latex?N">, but proportional to <img src="https://latex.codecogs.com/png.latex?O(d%5E3)">. As an added benefit of evaluating only the <code>yes/no</code> logits, the output of the fine-tuned model is compliant by design, as it cannot output any other logits other than for <code>yes/no</code>.</p>
<p>To solve for <img src="https://latex.codecogs.com/png.latex?W"> either using eq (1) or eq (2), we plug in our dataset <img src="https://latex.codecogs.com/png.latex?D"> for <img src="https://latex.codecogs.com/png.latex?P_a">, since both matrices have the same size.</p>
</section>
<section id="inference" class="level1">
<h1>Inference</h1>
<p>At inference time, the matrix <img src="https://latex.codecogs.com/png.latex?W"> stays constant, while the logits change for each input.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Ap_a(y%7Cx_%7B1:t%7D)%0A&amp;=%20%5Ctext%7Bsoftmax%7D%20%5C%7B%20%5Ctext%7Blogits%7D_t%20+%20z_t%20W%20%5C%7D%20%5C%5C%0A&amp;=%20%5Ctext%7Bsoftmax%7D%20%5C%7B%20z_t%20A%20+%20z_t%20W%20%5C%7D%0A%5Cend%7Baligned%7D%0A"></p>
</section>
<section id="next-steps" class="level1">
<h1>Next Steps</h1>
<p>In the next post, I will show an implementation of this method in PyTorch, and interpret how linear fine-tuning changes the outputs of the original LLM. I am interested in the flips between <code>yes/no</code> outside of the small fine-tuning dataset <img src="https://latex.codecogs.com/png.latex?D">, and particularly on the boundaries of the dataset, and how this pertains to generalization. Stay tuned! :)</p>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-chowdhery2023palm" class="csl-entry">
Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al. 2023. <span>“Palm: Scaling Language Modeling with Pathways.”</span> <em>Journal of Machine Learning Research</em> 24 (240): 1–113.
</div>
<div id="ref-dutta2024accuracy" class="csl-entry">
Dutta, Abhinav, Sanjeev Krishnan, Nipun Kwatra, and Ramachandran Ramjee. 2024. <span>“Accuracy Is Not All You Need.”</span> <em>arXiv Preprint arXiv:2407.09141</em>.
</div>
<div id="ref-hu2021lora" class="csl-entry">
Hu, Edward J, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. <span>“Lora: Low-Rank Adaptation of Large Language Models.”</span> <em>arXiv Preprint arXiv:2106.09685</em>.
</div>
<div id="ref-watkins2004fundamentals" class="csl-entry">
Watkins, David S. 2004. <em>Fundamentals of Matrix Computations</em>. John Wiley &amp; Sons.
</div>
</div>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>2024-09: The matrix <img src="https://latex.codecogs.com/png.latex?Z%5ET%20Z"> is positive definite, so it is in theory efficiently invertible using the Cholesky decomposition. However, its condition number <img src="https://latex.codecogs.com/png.latex?%5Ckappa%20(Z%5ET%20Z)"> is squarely proportional to the condition number of <img src="https://latex.codecogs.com/png.latex?%5Ckappa(Z)">. This can lead to numerical instability when solving the linear system. In fact, I stumbled upon numerical instability while implementing this in linear system in PyTorch, which lead me to use an <img src="https://latex.codecogs.com/png.latex?L_2"> regularization term. See <a href="https://tobydriscoll.net/fnc-julia/leastsq/normaleqns.html#conditioning-and-stability">Source</a>.↩︎</p></li>
<li id="fn2"><p>2024-09: It turns out that solving a linear system with <img src="https://latex.codecogs.com/png.latex?V"> columns on the right-hand side can be done cheaper than in <img src="https://latex.codecogs.com/png.latex?V%20%5Ccdot%20%5Cfrac%7B2%7D%7B3%7D%20d%5E3"> flops. To solve <img src="https://latex.codecogs.com/png.latex?A%20X%20=%20B"> (with <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd,d%7D"> and <img src="https://latex.codecogs.com/png.latex?X,%20B%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd,V%7D">) the factorization of <img src="https://latex.codecogs.com/png.latex?A"> requires <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B2%7D%7B3%7D%20d%5E3"> flops, but it only has to be done once. After that, solving for each of the <img src="https://latex.codecogs.com/png.latex?V"> columns of <img src="https://latex.codecogs.com/png.latex?B"> costs <img src="https://latex.codecogs.com/png.latex?2%20d%5E2"> flops each. So the total flop count is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B2%7D%7B3%7D%20d%5E3%20+%20V%20%5Ccdot%202%20d%5E2">. This is a significant improvement over the naive approach. See <span class="citation" data-cites="watkins2004fundamentals">(Watkins 2004, 77–78)</span>.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Machine Learning</category>
  <guid>https://tomasruizt.github.io/posts/linear-adaptation/</guid>
  <pubDate>Thu, 01 Aug 2024 22:00:00 GMT</pubDate>
  <media:content url="https://tomasruizt.github.io/posts/linear-adaptation/linear-adaptation/linear-adaptation.png" medium="image" type="image/png" height="128" width="144"/>
</item>
</channel>
</rss>
